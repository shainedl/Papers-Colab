{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Surprisingly_Effective_Fix_Latent.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shainedl/Papers-Colab/blob/master/Surprisingly_Effective_Fix_Latent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdHitQGWqsTy",
        "colab_type": "text"
      },
      "source": [
        "Based on *A Surprisingly Effective Fix for Deep Latent Variable Modeling of Text* (Li et al, Carnegie Mellon University)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1KfwBE_quKv",
        "colab_type": "code",
        "outputId": "faf04331-e360-4922-c431-8c50d7dfef73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from google.colab import files\n",
        "from collections import defaultdict\n",
        "from itertools import count, chain\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "import pdb"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUSwBG_LNz8o",
        "colab_type": "code",
        "outputId": "271683f3-45f7-4a21-bd26-4746379533e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9rIRtZ8qwlM",
        "colab_type": "code",
        "outputId": "19d307d2-5136-4d75-dedd-68c1832a52b6",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "uploaded_training = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8918077a-f63b-40e6-9edf-ea0715731446\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-8918077a-f63b-40e6-9edf-ea0715731446\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving sample_train.txt to sample_train.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seN2020gq1jI",
        "colab_type": "code",
        "outputId": "0e63ef4e-ab5b-4263-e4c1-68f94c06fe80",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "uploaded_val = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2e108e82-7092-47ff-8d18-4017aa7af90e\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-2e108e82-7092-47ff-8d18-4017aa7af90e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving sample_dev.txt to sample_dev.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P91t6QSzrHRU",
        "colab_type": "code",
        "outputId": "95fc5660-b039-4ada-fe8a-d7b8377a0142",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "uploaded_test = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f7cd8632-11f4-4bd2-b3d8-e8c2aee6533b\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-f7cd8632-11f4-4bd2-b3d8-e8c2aee6533b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving sample_test.txt to sample_test.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8p2XL6irNO-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(file):\n",
        "  \"\"\"\n",
        "  Load training data and output vocabulary dictionaries\n",
        "  \"\"\"\n",
        "  w2i = defaultdict(lambda x=count(0): next(x))\n",
        "  w2i[\"<s>\"] \n",
        "  w2i[\"</s>\"] \n",
        "  w2i[\"<unk>\"] \n",
        "  data = []\n",
        "  file = file.decode('utf-8')\n",
        "  sentences = file.splitlines()\n",
        "\n",
        "  for sentence in sentences:\n",
        "    tokens = word_tokenize(sentence)\n",
        "    for token in tokens:\n",
        "      w2i[token]\n",
        "    data.append(tokens)\n",
        "\n",
        "  freq_dist = nltk.FreqDist([item for sublist in data for item in sublist])\n",
        "  freq1 = set(list(freq_dist.keys())[-4000:])\n",
        "\n",
        "  w2i = dict(w2i)\n",
        "  for key in list(w2i.keys()):\n",
        "    if key in freq1:\n",
        "      w2i.pop(key)\n",
        "  i2w = {i:w for w,i in w2i.items()}\n",
        "\n",
        "  return data, w2i, i2w, freq_dist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWXItkUMrRkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_data, w2i, i2w, freq_dist = load_data(uploaded_training['sample_train.txt'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEM68Sh5rS6J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data_test(file):\n",
        "  \"\"\"\n",
        "  Load test and validation data\n",
        "  \"\"\"\n",
        "  data = []\n",
        "  file = file.decode('utf-8')\n",
        "  sentences = file.splitlines()\n",
        "\n",
        "  for sentence in sentences:\n",
        "    tokens = word_tokenize(sentence)\n",
        "    data.append(tokens)\n",
        "    \n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "py39K3QErT-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_data = load_data_test(uploaded_val['sample_dev.txt'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAWj_cf5rVTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = load_data_test(uploaded_test['sample_test.txt'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIMNYuk7rWij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_cuda = True\n",
        "cuda = torch.device('cuda')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQatzwBOrX_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_sequence(seq, to_ix):\n",
        "  idxs = [to_ix[\"<s>\"]]\n",
        "  for w in seq:\n",
        "    if w in to_ix:\n",
        "      idxs.append(to_ix[w])\n",
        "    else:\n",
        "      idxs.append(to_ix[\"<unk>\"])\n",
        "  idxs.append(to_ix[\"</s>\"])\n",
        "  return torch.tensor(idxs, dtype=torch.long)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCqyDFGRrZFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_data(batch_size, data, w2i):\n",
        "  \"\"\"\n",
        "  Batches data with sequences of the same length\n",
        "  \"\"\"\n",
        "  sentence_lengths = np.array([len(sentence) for sentence in data])\n",
        "  sorted_idx = np.argsort(sentence_lengths)\n",
        "  sorted_lengths = sentence_lengths[sorted_idx]\n",
        "\n",
        "  len_increase_idx = []\n",
        "  for i in range(1, len(sorted_lengths)):\n",
        "    if sorted_lengths[i] > sorted_lengths[i-1]:\n",
        "      len_increase_idx.append(i)\n",
        "  len_increase_idx.append(len(sorted_lengths))\n",
        "\n",
        "  batch_data = []\n",
        "  curr_idx = 0\n",
        "  for i, idx in enumerate(len_increase_idx):\n",
        "    while curr_idx < idx:\n",
        "      batch_sentences = []\n",
        "      new_idx = min(curr_idx + batch_size, idx)\n",
        "      for i in range(curr_idx, new_idx):\n",
        "        sent_to_vec = prepare_sequence(data[sorted_idx[i]], w2i)\n",
        "        batch_sentences.append(sent_to_vec)\n",
        "      curr_idx = new_idx\n",
        "      batch_sentences = torch.stack(batch_sentences).to(device=cuda)\n",
        "      batch_data.append(batch_sentences)\n",
        "\n",
        "  i = 0\n",
        "  j = len(batch_data)\n",
        "  while i < j:\n",
        "    if i != 0 and len(batch_data[i]) <= 2 and len(batch_data[i][0]) == len(batch_data[i-1][0]):\n",
        "      batch_data.append(torch.cat((batch_data[i], batch_data[i-1])))\n",
        "      batch_data.pop(i)\n",
        "      batch_data.pop(i-1)\n",
        "      i -= 1\n",
        "      j = len(batch_data)\n",
        "    elif len(batch_data[i]) == 1:\n",
        "      batch_data.pop(i)\n",
        "      j = len(batch_data)\n",
        "    else:\n",
        "      i += 1\n",
        "\n",
        "  return batch_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK7jnw4ZraWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtzNU6jhrbii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_training = batch_data(batch_size, training_data, w2i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1w7Eh6mrcgn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_val = batch_data(batch_size, val_data, w2i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn266gK4rdZr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_test = batch_data(batch_size, test_data, w2i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3QnlHyKreWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_size, hidden_size, latent_size):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.embeddings = nn.Embedding(num_embeddings=vocab_size,\n",
        "                                   embedding_dim=embedding_size,\n",
        "                                   padding_idx=-1)\n",
        "    \n",
        "    self.rnn = nn.LSTM(input_size=embedding_size,\n",
        "                        hidden_size=hidden_size,\n",
        "                        batch_first=True)\n",
        "    \n",
        "    self.fc_mu = nn.Linear(hidden_size, latent_size)\n",
        "    self.fc_var = nn.Linear(hidden_size, latent_size)\n",
        "\n",
        "    self._initialize_parameters([-0.01, 0.01], [-0.1, 0.1]) \n",
        "\n",
        "  def encode(self, x):\n",
        "    \"\"\"\n",
        "    Produces a Gaussian distribution over the possible values of the code z \n",
        "    from which x could have been generated\n",
        "        \n",
        "    Parameters\n",
        "    ----------\n",
        "      x: batch size x sequence length Tensor\n",
        "        observed data\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "      mu: batch size x latent size Tensor\n",
        "        mean of Gaussian distribution\n",
        "        \n",
        "      logvar: batch size x latent size Tensor\n",
        "        log of variance of Gaussian distribution     \n",
        "    \"\"\"\n",
        "    x = self.embeddings(x)\n",
        "    outputs, (hidden, cell) = self.rnn(x)\n",
        "    mu = self.fc_mu(hidden)\n",
        "    logvar = self.fc_var(hidden)\n",
        "    mu = mu.squeeze()\n",
        "    logvar = logvar.squeeze()\n",
        "    return mu, logvar \n",
        "\n",
        "  def _initialize_parameters(self, lstm_init, embed_init):\n",
        "    \"\"\"\n",
        "    Initializes the LSTM parameters and embeddings with uniform distributions\n",
        "    \"\"\"\n",
        "    for param in self.parameters():\n",
        "      nn.init.uniform_(param, a=lstm_init[0], b=lstm_init[1])\n",
        "    nn.init.uniform_(self.embeddings.weight, a=embed_init[0], b=embed_init[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-wDLqSlrfl8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_size, hidden_size, latent_size):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.embeddings = nn.Embedding(num_embeddings=vocab_size,\n",
        "                                   embedding_dim=embedding_size,\n",
        "                                   padding_idx=-1)\n",
        "    \n",
        "    self.rnn = nn.LSTM(input_size=embedding_size + latent_size,\n",
        "                        hidden_size=hidden_size,\n",
        "                        batch_first=True)    \n",
        "\n",
        "    self.fc_hid = nn.Linear(latent_size, hidden_size)\n",
        "    self.fc_voc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    self.dropout = nn.Dropout()\n",
        "    self._initialize_parameters([-0.01, 0.01], [-0.1, 0.1])\n",
        "\n",
        "  def decode(self, z, inputs):\n",
        "    \"\"\"\n",
        "    Given a code z it produces unscaled output corresponding to the vocabulary\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "      z: batch size x latent size Tensor\n",
        "        latent variables\n",
        "      \n",
        "      inputs: batch size x sequence length Tensor\n",
        "        source sequence\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "      output_logits: batch size x sequence length x vocab size Tensor\n",
        "        unscaled output\n",
        "    \"\"\"\n",
        "    cell = self.fc_hid(z)\n",
        "    cell = cell.unsqueeze(0)\n",
        "    hidden = torch.tanh(cell)\n",
        "\n",
        "    embed = self.embeddings(inputs)\n",
        "    embed = self.dropout(embed)\n",
        "    z = z.expand(embed.size(1), z.size(0), z.size(1))\n",
        "    z = z.transpose(1,0)\n",
        "    embed_lat = torch.cat((embed, z), 2)\n",
        "\n",
        "    outputs, (hidden, cell) = self.rnn(embed_lat, (hidden, cell))\n",
        "    outputs = self.dropout(outputs)\n",
        "    output_logits = self.fc_voc(outputs)\n",
        "    return output_logits\n",
        "\n",
        "  def decode_greedy(self, z, inputs, interpolation=False):\n",
        "    \"\"\"\n",
        "    Given a code z it produces unscaled output corresponding to the vocabulary\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "      z: batch size x latent size Tensor\n",
        "        latent variables\n",
        "      \n",
        "      inputs: batch size x sequence length Tensor\n",
        "        source sequence\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "      output_logits: batch size x sequence length x vocab size Tensor\n",
        "        unscaled output\n",
        "      \n",
        "      batch_decoded: batch size x output sequence length list\n",
        "        decoded output sequence\n",
        "    \"\"\"\n",
        "    cell = self.fc_hid(z)\n",
        "    cell = cell.unsqueeze(0)\n",
        "    hidden = torch.tanh(cell)\n",
        "\n",
        "    batch_size = inputs.size(0)\n",
        "    input_d = inputs[:,0]\n",
        "    output_logit_prev = None\n",
        "    seq_len = inputs.size(1)\n",
        "    batch_decoded = [[] for j in range(batch_size)]\n",
        "\n",
        "    end_mask = torch.ones(batch_size)\n",
        "    counter = 0\n",
        "    while end_mask.sum() != 0 and counter < seq_len:\n",
        "      embed = self.embeddings(input_d)\n",
        "      embed_lat = torch.cat((embed, z), 1)\n",
        "      embed_lat = embed_lat.unsqueeze(1)\n",
        "      outputs, (hidden, cell) = self.rnn(embed_lat, (hidden, cell))  \n",
        "      output_logit = self.fc_voc(outputs)\n",
        "      if output_logit_prev is not None:\n",
        "        output_logits = torch.cat((output_logit_prev, output_logit), dim=1)\n",
        "        output_logit_prev = output_logits\n",
        "      else:\n",
        "        output_logit_prev = output_logit\n",
        "      input_d = torch.argmax(output_logit, dim=2).flatten()\n",
        "\n",
        "      for k in range(batch_size):\n",
        "        if end_mask[k] != 0:\n",
        "          if interpolation and input_d[k].item() == w2i[\"</s>\"] :\n",
        "            end_mask[k] = 0\n",
        "          else:\n",
        "            token = i2w[input_d[k].item()]\n",
        "            batch_decoded[k].append(token)\n",
        "      counter += 1\n",
        "    \n",
        "    return output_logits, batch_decoded\n",
        "\n",
        "  def _initialize_parameters(self, lstm_init, embed_init):\n",
        "    \"\"\"\n",
        "    Initializes the LSTM parameters and embeddings with uniform distributions\n",
        "    \"\"\"\n",
        "    for param in self.parameters():\n",
        "      nn.init.uniform_(param, a=lstm_init[0], b=lstm_init[1])\n",
        "    nn.init.uniform_(self.embeddings.weight, a=embed_init[0], b=embed_init[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjIMk_a1rg9R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VAE(nn.Module):\n",
        "\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super(VAE, self).__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.re_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "  def forward(self, x, greedy=False):\n",
        "    \"\"\"\n",
        "    Forward pass of the model \n",
        "    \"\"\"\n",
        "    mu, logvar = self.encoder.encode(x)\n",
        "    kl = self.get_kl(mu, logvar)\n",
        "    z = self._reparameterize(mu, logvar)\n",
        "\n",
        "    source = x[:,:-1]\n",
        "    target = x[:, 1:]\n",
        "    if greedy:\n",
        "      output_logits, batch_decoded = self.decoder.decode_greedy(z, source)\n",
        "      re = self.get_reconstruction_error(output_logits, target)\n",
        "      return kl, re, batch_decoded\n",
        "    else: \n",
        "      output_logits = self.decoder.decode(z, source)\n",
        "      re = self.get_reconstruction_error(output_logits, target)\n",
        "      return kl, re\n",
        "\n",
        "  def _reparameterize(self, mu, logvar):\n",
        "    \"\"\"\n",
        "    Reparameterize the random variable z to express as a deterministic variable\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "      mu: batch size x latent size Tensor\n",
        "        mean of Gaussian distribution\n",
        "        \n",
        "      logvar: batch size x latent size Tensor\n",
        "        log of variance of Gaussian distribution     \n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "      z: batch size x latent size Tensor\n",
        "        reparameterization of latent variables\n",
        "    \"\"\"\n",
        "    std = torch.exp(logvar / 2)\n",
        "    eps = torch.randn_like(std)\n",
        "    return mu + std * eps  \n",
        "\n",
        "  def get_kl(self, mu, logvar):\n",
        "    \"\"\"\n",
        "    Returns the KLD between posterior and prior\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "      mu: batch size x latent size Tensor\n",
        "        mean of Gaussian distribution\n",
        "        \n",
        "      logvar: batch size x latent size Tensor\n",
        "        log of variance of Gaussian distribution\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "      kl: batch size x latent size Tensor\n",
        "        kl divergence\n",
        "    \"\"\"\n",
        "    return (mu**2 + logvar.exp() - 1 - logvar) / 2\n",
        "\n",
        "  def get_reconstruction_error(self, output_logits, target):\n",
        "    \"\"\"\n",
        "    Returns the reconstruction error\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "      output_logits: batch size x sequence length x vocab size Tensor\n",
        "        unscaled output\n",
        "      \n",
        "      target: batch size x sequence length Tensor\n",
        "        target sequence\n",
        "    \"\"\"\n",
        "    target = target.contiguous().view(-1)\n",
        "    output_logits = output_logits.view(-1, output_logits.size(2))\n",
        "    return self.re_loss(output_logits, target)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW6vyn9kriZ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(w2i)\n",
        "embedding_size = 128\n",
        "hidden_size = 512 \n",
        "latent_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngpF9vFvPHeE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print_every = round(len(batch_training) / 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7s2VHLqCrkXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epoch, pretraining=False, target_rate=4.0):\n",
        "  \"\"\"\n",
        "  Trains the model\n",
        "  Helpful link for free bits: https://stats.stackexchange.com/questions/267924/explanation-of-the-free-bits-technique-for-variational-autoencoders\n",
        "  \"\"\"\n",
        "  model.train()\n",
        "  anneal = 0.1 * epoch if epoch < 10 else 1.0\n",
        "\n",
        "  running_loss = 0.0\n",
        "  running_kl = 0.0\n",
        "  running_re = 0.0\n",
        "  for batch_idx, data in enumerate(batch_training):\n",
        "    optimizer_e.zero_grad()\n",
        "    optimizer_d.zero_grad()\n",
        "    kl, re = model(data)\n",
        "    if pretraining:\n",
        "      loss = re\n",
        "    else:\n",
        "      kl_mean = kl.mean(dim=0)\n",
        "      kl_mask = (kl_mean > target_rate).float()\n",
        "      fb_mask = (kl_mean <= target_rate).float()\n",
        "      free_b = kl_mask + target_rate\n",
        "      kl = (kl_mean * kl_mask + free_b * fb_mask).sum()\n",
        "      loss = kl * anneal + re\n",
        "      running_kl += kl * anneal\n",
        "      running_re += re\n",
        "    loss.backward()\n",
        "    optimizer_e.step()\n",
        "    optimizer_d.step()\n",
        "\n",
        "    running_loss += loss\n",
        "  \n",
        "    if (epoch == 0 or epoch % 10 == 9) and batch_idx % print_every == print_every-1:    \n",
        "      print('[%d, %5d] Train loss: %.3f' % (epoch + 1, batch_idx + 1, running_loss / print_every))\n",
        "      running_loss = 0.0 \n",
        "      if not pretraining:\n",
        "        print('[%d, %5d] Train KL: %.3f' % (epoch + 1, batch_idx + 1, running_kl / print_every))\n",
        "        print('[%d, %5d] Train RE: %.3f' % (epoch + 1, batch_idx + 1, running_re / print_every))\n",
        "        running_kl = 0.0\n",
        "        running_re = 0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL_TDNPMrs3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(epoch, validation=False, pretraining=False, target_rate=4.0):\n",
        "  \"\"\"\n",
        "  Run the model on validation or test dataset\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  anneal = 0.1 * epoch if epoch < 10 else 1.0\n",
        "\n",
        "  data = batch_val if validation else batch_test\n",
        "  running_loss = 0.0\n",
        "  running_kl = 0.0\n",
        "  running_re = 0.0\n",
        "  for batch_idx, data in enumerate(data):\n",
        "    if validation:  \n",
        "      kl, re = model(data)\n",
        "    else:\n",
        "      kl, re, batch_decoded = model(data, True)\n",
        "    if pretraining:\n",
        "      loss = re\n",
        "    else:\n",
        "      kl_mean = kl.mean(dim=0)\n",
        "      kl_mask = (kl_mean > target_rate).float()\n",
        "      fb_mask = (kl_mean <= target_rate).float()\n",
        "      fb = kl_mask + target_rate\n",
        "      kl = (kl_mean * kl_mask + fb * fb_mask).sum()\n",
        "      loss = kl * anneal + re\n",
        "      running_kl += kl * anneal\n",
        "      running_re += re\n",
        "\n",
        "    running_loss += loss\n",
        "\n",
        "  avg_loss = running_loss / (batch_idx + 1)\n",
        "  avg_kl = running_kl / (batch_idx + 1)\n",
        "  avg_re = running_re / (batch_idx + 1)\n",
        "  if epoch == 0 or epoch % 10 == 9:\n",
        "    if validation:\n",
        "      print('[%d, %5d] Validation loss: %.3f' % (epoch + 1, batch_idx + 1, avg_loss))\n",
        "      if not pretraining:\n",
        "        print('[%d, %5d] Validation KL: %.3f' % (epoch + 1, batch_idx + 1, avg_kl))\n",
        "        print('[%d, %5d] Validation RE: %.3f' % (epoch + 1, batch_idx + 1, avg_re))\n",
        "    else:\n",
        "      print('[%d, %5d] Test loss: %.3f' % (epoch + 1, batch_idx + 1, avg_loss))\n",
        "      if not pretraining:\n",
        "        print('[%d, %5d] Test KL: %.3f' % (epoch + 1, batch_idx + 1, avg_kl))\n",
        "        print('[%d, %5d] Test RE: %.3f' % (epoch + 1, batch_idx + 1, avg_re))\n",
        "      print(*batch_decoded[0])\n",
        "\n",
        "\n",
        "  return avg_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8DKERdQruwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecayLearning:\n",
        "  \"\"\"\n",
        "  Class updated from https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
        "  \"\"\"\n",
        "  def __init__(self, patience=2):\n",
        "    self.patience = patience\n",
        "    self.counter = 0\n",
        "    self.best_score = None\n",
        "    self.update_lr = False\n",
        "\n",
        "  def __call__(self, val_loss):\n",
        "    score = -val_loss\n",
        "\n",
        "    if self.best_score is None:\n",
        "      self.best_score = score\n",
        "    elif score < self.best_score:\n",
        "      self.counter += 1\n",
        "      if self.counter >= self.patience:\n",
        "        self.update_lr = True\n",
        "    else:\n",
        "      self.best_score = score\n",
        "      self.counter = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GHj1kvBrzsf",
        "colab_type": "code",
        "outputId": "9e9b5c49-cf83-4002-e58b-3d24a43050a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        }
      },
      "source": [
        "encoder = Encoder(vocab_size, embedding_size, hidden_size, latent_size)\n",
        "decoder = Decoder(vocab_size, embedding_size, hidden_size, latent_size)\n",
        "model = VAE(encoder, decoder)\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "  \n",
        "lr = 0.5\n",
        "optimizer_e = optim.SGD(encoder.parameters(), lr=lr)\n",
        "optimizer_d = optim.SGD(decoder.parameters(), lr=lr)\n",
        "\n",
        "decay_learning = DecayLearning()\n",
        "num_decays = 0\n",
        "early_stop = 5\n",
        "\n",
        "for epoch in range(100):\n",
        "  train(epoch, True)\n",
        "  val_loss = test(epoch, True, True)\n",
        "  if epoch == 0 or epoch % 10 == 9: \n",
        "    test(epoch, False, True)\n",
        "  decay_learning(val_loss)\n",
        "  if decay_learning.update_lr:\n",
        "    lr *= 0.5\n",
        "    num_decays += 1\n",
        "    if num_decays == early_stop + 1:\n",
        "      print(\"Stopping early at epoch\", epoch+1)\n",
        "      break\n",
        "    print(\"Learning rate has been decayed to\", lr, \"at epoch\", epoch+1)\n",
        "    optimizer_e = optim.SGD(encoder.parameters(), lr=lr)\n",
        "    optimizer_d = optim.SGD(decoder.parameters(), lr=lr)\n",
        "    decay_learning = DecayLearning()\n",
        "\n",
        "save_name = 'surprising.pt'\n",
        "path = F\"/content/gdrive/My Drive/{save_name}\" \n",
        "\n",
        "torch.save({\n",
        "          'model_state_dict': model.state_dict()\n",
        "          }, path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  1600] Train loss: 4.169\n",
            "[1,  3200] Train loss: 3.755\n",
            "[1,  4800] Train loss: 3.759\n",
            "[1,  6400] Train loss: 3.889\n",
            "[1,  8000] Train loss: 3.872\n",
            "[1,   802] Validation loss: 4.563\n",
            "[1,   801] Test loss: 6.728\n",
            "A man is sitting on a <unk> , <unk> , and <unk> , are playing\n",
            "Learning rate has been decayed to 0.25 at epoch 5\n",
            "[10,  1600] Train loss: 1.508\n",
            "[10,  3200] Train loss: 1.553\n",
            "[10,  4800] Train loss: 1.722\n",
            "[10,  6400] Train loss: 1.955\n",
            "[10,  8000] Train loss: 2.148\n",
            "[10,   802] Validation loss: 3.175\n",
            "[10,   801] Test loss: 5.391\n",
            "Two men are sitting </s> </s> . </s> </s> are sitting next to the man\n",
            "Learning rate has been decayed to 0.125 at epoch 15\n",
            "Learning rate has been decayed to 0.0625 at epoch 18\n",
            "[20,  1600] Train loss: 0.517\n",
            "[20,  3200] Train loss: 0.600\n",
            "[20,  4800] Train loss: 0.759\n",
            "[20,  6400] Train loss: 0.990\n",
            "[20,  8000] Train loss: 1.221\n",
            "[20,   802] Validation loss: 2.194\n",
            "[20,   801] Test loss: 4.139\n",
            "Two men are sweeping , one is sitting and one is sitting down . </s>\n",
            "Learning rate has been decayed to 0.03125 at epoch 24\n",
            "[30,  1600] Train loss: 0.347\n",
            "[30,  3200] Train loss: 0.436\n",
            "[30,  4800] Train loss: 0.579\n",
            "[30,  6400] Train loss: 0.793\n",
            "[30,  8000] Train loss: 1.026\n",
            "[30,   802] Validation loss: 1.025\n",
            "[30,   801] Test loss: 1.933\n",
            "Two men are removing , one is sitting and eating while sitting down . </s>\n",
            "Learning rate has been decayed to 0.015625 at epoch 33\n",
            "[40,  1600] Train loss: 0.282\n",
            "[40,  3200] Train loss: 0.373\n",
            "[40,  4800] Train loss: 0.505\n",
            "[40,  6400] Train loss: 0.706\n",
            "[40,  8000] Train loss: 0.934\n",
            "[40,   802] Validation loss: 0.860\n",
            "[40,   801] Test loss: 1.445\n",
            "Two men are pretending while one is one . </s> is sitting while smoking .\n",
            "Stopping early at epoch 41\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cmDuj-yr2LS",
        "colab_type": "code",
        "outputId": "861907fe-2bd9-4991-afdf-d48a212ac851",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 798
        }
      },
      "source": [
        "decoder = Decoder(vocab_size, embedding_size, hidden_size, latent_size)\n",
        "model = VAE(encoder, decoder)\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "\n",
        "lr = 0.5\n",
        "optimizer_e = optim.SGD(encoder.parameters(), lr=lr)\n",
        "optimizer_d = optim.SGD(decoder.parameters(), lr=lr)\n",
        "num_decays = 0\n",
        "decay_learning = DecayLearning()\n",
        "\n",
        "for epoch in range(10):\n",
        "  train(epoch, False)\n",
        "  val_loss = test(epoch, True, False)\n",
        "  if epoch == 0 or epoch % 10 == 9: \n",
        "    test(epoch, False, False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  1600] Train loss: 2.669\n",
            "[1,  1600] Train KL: 0.000\n",
            "[1,  1600] Train RE: 2.669\n",
            "[1,  3200] Train loss: 1.883\n",
            "[1,  3200] Train KL: 0.000\n",
            "[1,  3200] Train RE: 1.883\n",
            "[1,  4800] Train loss: 1.834\n",
            "[1,  4800] Train KL: 0.000\n",
            "[1,  4800] Train RE: 1.834\n",
            "[1,  6400] Train loss: 1.949\n",
            "[1,  6400] Train KL: 0.000\n",
            "[1,  6400] Train RE: 1.949\n",
            "[1,  8000] Train loss: 2.108\n",
            "[1,  8000] Train KL: 0.000\n",
            "[1,  8000] Train RE: 2.108\n",
            "[1,   802] Validation loss: 3.404\n",
            "[1,   802] Validation KL: 0.000\n",
            "[1,   802] Validation RE: 3.404\n",
            "[1,   801] Test loss: 4.372\n",
            "[1,   801] Test KL: 0.000\n",
            "[1,   801] Test RE: 4.372\n",
            "Two men are one , , , one is sitting sitting sitting sitting down .\n",
            "[10,  1600] Train loss: 115.571\n",
            "[10,  1600] Train KL: 115.210\n",
            "[10,  1600] Train RE: 0.363\n",
            "[10,  3200] Train loss: 115.581\n",
            "[10,  3200] Train KL: 115.205\n",
            "[10,  3200] Train RE: 0.378\n",
            "[10,  4800] Train loss: 115.742\n",
            "[10,  4800] Train KL: 115.204\n",
            "[10,  4800] Train RE: 0.540\n",
            "[10,  6400] Train loss: 115.999\n",
            "[10,  6400] Train KL: 115.204\n",
            "[10,  6400] Train RE: 0.797\n",
            "[10,  8000] Train loss: 116.331\n",
            "[10,  8000] Train KL: 115.204\n",
            "[10,  8000] Train RE: 1.129\n",
            "[10,   802] Validation loss: 117.540\n",
            "[10,   802] Validation KL: 115.316\n",
            "[10,   802] Validation RE: 2.224\n",
            "[10,   801] Test loss: 119.647\n",
            "[10,   801] Test KL: 115.330\n",
            "[10,   801] Test RE: 4.317\n",
            "Two men are , , . </s> , , is sitting their person standing on\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oewjtCpM5Qmb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "21c7f204-fd7e-41a6-e1b7-346dbe309b8d"
      },
      "source": [
        "for epoch in range(10,100):\n",
        "  train(epoch, False)\n",
        "  val_loss = test(epoch, True, False)\n",
        "  if epoch == 0 or epoch % 10 == 9: \n",
        "    test(epoch, False, False)\n",
        "  decay_learning(val_loss)\n",
        "  if decay_learning.update_lr:\n",
        "    lr *= 0.5\n",
        "    num_decays += 1\n",
        "    if num_decays == early_stop + 1:\n",
        "      print(\"Stopping early at epoch\", epoch+1)\n",
        "      break\n",
        "    print(\"Learning rate has been decayed to\", lr, \"at epoch\", epoch+1)\n",
        "    optimizer_e = optim.SGD(encoder.parameters(), lr=lr)\n",
        "    optimizer_d = optim.SGD(decoder.parameters(), lr=lr)\n",
        "    decay_learning = DecayLearning()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate has been decayed to 0.25 at epoch 13\n",
            "Learning rate has been decayed to 0.125 at epoch 16\n",
            "[20,  1600] Train loss: 128.193\n",
            "[20,  1600] Train KL: 128.001\n",
            "[20,  1600] Train RE: 0.192\n",
            "[20,  3200] Train loss: 128.245\n",
            "[20,  3200] Train KL: 128.001\n",
            "[20,  3200] Train RE: 0.245\n",
            "[20,  4800] Train loss: 128.370\n",
            "[20,  4800] Train KL: 128.000\n",
            "[20,  4800] Train RE: 0.369\n",
            "[20,  6400] Train loss: 128.581\n",
            "[20,  6400] Train KL: 128.001\n",
            "[20,  6400] Train RE: 0.581\n",
            "[20,  8000] Train loss: 128.855\n",
            "[20,  8000] Train KL: 128.000\n",
            "[20,  8000] Train RE: 0.855\n",
            "[20,   802] Validation loss: 130.209\n",
            "[20,   802] Validation KL: 128.031\n",
            "[20,   802] Validation RE: 2.178\n",
            "[20,   801] Test loss: 132.171\n",
            "[20,   801] Test KL: 128.038\n",
            "[20,   801] Test RE: 4.133\n",
            "Two men are keeping , . </s> </s> , is sitting , is on .\n",
            "Learning rate has been decayed to 0.0625 at epoch 22\n",
            "[30,  1600] Train loss: 128.104\n",
            "[30,  1600] Train KL: 128.000\n",
            "[30,  1600] Train RE: 0.103\n",
            "[30,  3200] Train loss: 128.153\n",
            "[30,  3200] Train KL: 128.000\n",
            "[30,  3200] Train RE: 0.153\n",
            "[30,  4800] Train loss: 128.253\n",
            "[30,  4800] Train KL: 128.000\n",
            "[30,  4800] Train RE: 0.253\n",
            "[30,  6400] Train loss: 128.418\n",
            "[30,  6400] Train KL: 128.000\n",
            "[30,  6400] Train RE: 0.417\n",
            "[30,  8000] Train loss: 128.654\n",
            "[30,  8000] Train KL: 128.000\n",
            "[30,  8000] Train RE: 0.654\n",
            "[30,   802] Validation loss: 130.039\n",
            "[30,   802] Validation KL: 128.032\n",
            "[30,   802] Validation RE: 2.007\n",
            "[30,   801] Test loss: 132.341\n",
            "[30,   801] Test KL: 128.037\n",
            "[30,   801] Test RE: 4.304\n",
            "Two men are keeping , , </s> </s> , </s> is sitting and is sitting\n",
            "Learning rate has been decayed to 0.03125 at epoch 31\n",
            "[40,  1600] Train loss: 128.077\n",
            "[40,  1600] Train KL: 128.000\n",
            "[40,  1600] Train RE: 0.077\n",
            "[40,  3200] Train loss: 128.124\n",
            "[40,  3200] Train KL: 128.000\n",
            "[40,  3200] Train RE: 0.124\n",
            "[40,  4800] Train loss: 128.215\n",
            "[40,  4800] Train KL: 128.000\n",
            "[40,  4800] Train RE: 0.215\n",
            "[40,  6400] Train loss: 128.364\n",
            "[40,  6400] Train KL: 128.000\n",
            "[40,  6400] Train RE: 0.364\n",
            "[40,  8000] Train loss: 128.570\n",
            "[40,  8000] Train KL: 128.000\n",
            "[40,  8000] Train RE: 0.569\n",
            "[40,   802] Validation loss: 129.423\n",
            "[40,   802] Validation KL: 128.031\n",
            "[40,   802] Validation RE: 1.391\n",
            "[40,   801] Test loss: 131.090\n",
            "[40,   801] Test KL: 128.036\n",
            "[40,   801] Test RE: 3.054\n",
            "Two men are keeping , , one person , , , on man is sitting\n",
            "Learning rate has been decayed to 0.015625 at epoch 49\n",
            "[50,  1600] Train loss: 128.067\n",
            "[50,  1600] Train KL: 128.000\n",
            "[50,  1600] Train RE: 0.067\n",
            "[50,  3200] Train loss: 128.110\n",
            "[50,  3200] Train KL: 128.000\n",
            "[50,  3200] Train RE: 0.110\n",
            "[50,  4800] Train loss: 128.192\n",
            "[50,  4800] Train KL: 128.000\n",
            "[50,  4800] Train RE: 0.192\n",
            "[50,  6400] Train loss: 128.326\n",
            "[50,  6400] Train KL: 128.000\n",
            "[50,  6400] Train RE: 0.326\n",
            "[50,  8000] Train loss: 128.513\n",
            "[50,  8000] Train KL: 128.000\n",
            "[50,  8000] Train RE: 0.513\n",
            "[50,   802] Validation loss: 129.175\n",
            "[50,   802] Validation KL: 128.031\n",
            "[50,   802] Validation RE: 1.144\n",
            "[50,   801] Test loss: 130.484\n",
            "[50,   801] Test KL: 128.037\n",
            "[50,   801] Test RE: 2.446\n",
            "Two men are keeping , , one person , , on is , sitting sitting\n",
            "[60,  1600] Train loss: 128.057\n",
            "[60,  1600] Train KL: 128.000\n",
            "[60,  1600] Train RE: 0.057\n",
            "[60,  3200] Train loss: 128.096\n",
            "[60,  3200] Train KL: 128.000\n",
            "[60,  3200] Train RE: 0.096\n",
            "[60,  4800] Train loss: 128.173\n",
            "[60,  4800] Train KL: 128.000\n",
            "[60,  4800] Train RE: 0.173\n",
            "[60,  6400] Train loss: 128.300\n",
            "[60,  6400] Train KL: 128.000\n",
            "[60,  6400] Train RE: 0.299\n",
            "[60,  8000] Train loss: 128.488\n",
            "[60,  8000] Train KL: 128.000\n",
            "[60,  8000] Train RE: 0.488\n",
            "[60,   802] Validation loss: 129.007\n",
            "[60,   802] Validation KL: 128.030\n",
            "[60,   802] Validation RE: 0.976\n",
            "[60,   801] Test loss: 130.066\n",
            "[60,   801] Test KL: 128.036\n",
            "[60,   801] Test RE: 2.031\n",
            "Two men are keeping , , one person , , on is , sitting it\n",
            "Stopping early at epoch 61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv8OMjgWRM9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def interpolate(latent_size):\n",
        "  sample1 = torch.randn(1, latent_size, device=cuda)\n",
        "  sample2 = torch.randn(1, latent_size, device=cuda)\n",
        "  for w in range(11):\n",
        "    weight = w * 0.1\n",
        "    sample = weight * sample2 + (1-weight) * sample1\n",
        "    _, batch_decoded = decoder.decode_greedy(sample, torch.zeros(12, dtype=torch.long, device=cuda).unsqueeze(0), True)\n",
        "    print(*batch_decoded[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqNbXz7ERNpy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "88a01872-57dd-4c77-d918-96b6e7e11890"
      },
      "source": [
        "interpolate(latent_size)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The man standing is standing by a hotel .\n",
            "The male standing is standing by a hotel .\n",
            "The male man is standing by the garden .\n",
            "The male man standing standing by the his log .\n",
            "The bald man sitting standing up the the bed .\n",
            "The bald man sitting standing up the the bed .\n",
            "The male man sitting sitting up from the bed .\n",
            "The blue man sitting sitting down from the bed .\n",
            "The middle <unk> sitting sitting down from the ground the bed .\n",
            "The middle <unk> sitting sitting down from the ground the ground .\n",
            "The five <unk> sitting sitting down from the ground to the ground\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHlMR_cNXMiu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "4af13790-ef8a-4ee1-d75b-7d9bef57c79a"
      },
      "source": [
        "interpolate(latent_size)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "An elderly watches drums drums to cross the street\n",
            "An elderly watches two drums to cross the race a street .\n",
            "An elderly watches two drums to cross the a race .\n",
            "A elderly and uses equipment drums to cross a race .\n",
            "A older woman with old equipment to cross a hole into .\n",
            "A older woman and an equipment to cross a hole against .\n",
            "A barefoot woman and an <unk> gather to a turn up .\n",
            "A small boy , two <unk> stand next a card behind .\n",
            "A small boy , only old man after a card behind beans\n",
            "A small boy is holding an men squatting behind a disc set\n",
            "A small boy is holding a black cart next to cigarettes .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozEIcAyNXPLP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "e801a990-4ab2-4f29-ddd1-56022484921e"
      },
      "source": [
        "interpolate(latent_size)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "An infant up in the laundry garden river walks by by naked\n",
            "An infant baby in the blue cliff ring walks by by himself\n",
            "Male infant kid in the blue cliff ring walks by himself by\n",
            "One long-haired kid in a white mat ring while by himself himself\n",
            "Two long-haired boy in a white slide walks while by himself walks\n",
            "Two long-haired boy in a white subway bench while himself himself himself\n",
            "Two dark-haired boy in a white crowd stands while himself himself himself\n",
            "Two shirtless boy sitting a white crowd while standing behind himself himself\n",
            "Two white man sitting a crowd crowd while a boy up smiles\n",
            "Two white man sitting a crowd crowd while standing posing posing .\n",
            "Two man people sitting a crowd boy standing behind posing standing .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjLdDaEfXU9V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "bcd18e6f-6e92-41f4-b575-9564f92b67c4"
      },
      "source": [
        "interpolate(latent_size)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "white men are running holding a <unk> and smile a smile .\n",
            "Two men are swimming holding a <unk> and smile a smile .\n",
            "Two men are swimming holding a <unk> and like a smile .\n",
            "Two people are swimming holding a <unk> and like a joke set\n",
            "Two people are swimming up a <unk> and like on a joke\n",
            "Two people are swimming up and white friend to a hula joke\n",
            "Two people are swimming up something toy like to a hula joke\n",
            "Two children are up up something toy to put a hula hoop\n",
            "Two children are up up something plastic to hula a hoop hoop\n",
            "Two girls jumping up to blue plastic to dive .\n",
            "Two girls jumping up to blue rocking to hula hula .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYTgYIUNXWcG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "18d7abc7-4d94-4d86-ebf0-34a87bf48dae"
      },
      "source": [
        "interpolate(latent_size)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A female red-haired talks on a woman and another person standing to\n",
            "A female soldier talks on a woman and another woman eating to\n",
            "A female soldier talks on a phone and another woman fixing her\n",
            "A female soldier sits on a bench and people stop to a\n",
            "A female teenager sits on a bench and people are on the\n",
            "A female teenager sits on a bench and they heading the train\n",
            "A female family sitting on a ledge that 's heading during the\n",
            "A female family sitting on a ledge where leaving work during display\n",
            "A family sits sitting on a snowy track or work during work\n",
            "A family sits around on a snowy lined lined during work site\n",
            "Three family sits around at a snowy forest work during work .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FS90E7tXZHQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "8461138f-098e-47d2-fe15-a91c4a658f98"
      },
      "source": [
        "interpolate(latent_size)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Two white dog in a crowd walks up <unk> up <unk> .\n",
            "Two white dog making a white stands up <unk> up <unk> .\n",
            "Two white dog making a white jumps up <unk> for his hands\n",
            "Two white dog making a <unk> jumps up with his <unk> jumps\n",
            "The white rider has a <unk> up while <unk> in his mouth\n",
            "The dirt rider has white <unk> in a ball jumps for his\n",
            "The dirt rider racing white football in a <unk> jumps in her\n",
            "The dirt rider racing white football at a team 's in midair\n",
            "The dirt rider <unk> flat with the air in midair\n",
            "The ladder car flat <unk>\n",
            "The ladder car flat <unk>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhcyjFFEXkfu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "0a9ce2d9-6fdd-43fa-fa38-99fd32387de6"
      },
      "source": [
        "interpolate(latent_size)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Three group men are in a blue canoe on the concrete dock\n",
            "Three group men are in a blue canoe on the concrete railing\n",
            "Three young men are in a blue next to some trees .\n",
            "Three young men are in a canoe next to some trees .\n",
            "Three young men are using a canoe out behind behind fishing .\n",
            "Three Three adults men wearing a trash back from behind behind .\n",
            "Three Three two men are holding two luggage to their back behind\n",
            "Three Three three men men holding two mom behind behind fish behind\n",
            "three Three three adults men holding his mom behind from behind behind\n",
            "three three three three men holding two trash from behind behind toy\n",
            "three three three three adults men holding two boxes as from behind\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lmUQED3XoGU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "c6684d8f-9a6a-4e7f-b319-60fba9dcbcb4"
      },
      "source": [
        "interpolate(latent_size)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Men are in a green uniform in a big middle\n",
            "Men are in a green uniform in a big big gym\n",
            "Men men in a professional uniform through a big gym .\n",
            "Two men riding a uniform performing the gym .\n",
            "Two men riding in a uniform won a dark gym .\n",
            "Two men riding in a uniform won a dark gym .\n",
            "Four men riding in a hallway won a evening alley .\n",
            "Four men standing in a sidewalk during a indoor country .\n",
            "Six kids in standing enjoying a country lit .\n",
            "Six of men standing across a courtyard .\n",
            "Six of kids standing standing a quiet .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHPSowXOXpjK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "d586be39-601f-4de1-df9f-f66b96d658a7"
      },
      "source": [
        "interpolate(latent_size)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "three men men up up and down outside talking outside .\n",
            "two men men up up and working outside another man .\n",
            "two adults men up up <unk> outside while a man outside .\n",
            "two young people up up <unk> standing outside a man outside\n",
            "two young people up up <unk> standing beside a outside outside\n",
            "two young people up blue wall standing a outside man near a\n",
            "two girls people in white bicycle next a man near a building\n",
            "two girls people in a car parked outside a building building\n",
            "The girls , in a truck steps beside the <unk> beside\n",
            "The little boy in a truck truck beside the <unk> beside a\n",
            "The small boy in a truck truck beside a city building\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04IXz4aTXq7O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "23f30249-99be-475a-fa2f-d0fda4e68228"
      },
      "source": [
        "interpolate(latent_size)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some woman are fighting with rope and a backpack sitting\n",
            "Some women are fighting over with another and running\n",
            "Five women are fighting over a helmet sitting with a backpack on\n",
            "Five women are fighting over a helmet sitting on a pole with\n",
            "2 men are fighting over another motorcycle on a pole with a\n",
            "2 men are <unk> over another while riding a pole .\n",
            "Two men men jumping a tree while interacting on a small pole\n",
            "two three men fixing a bike while next on a small bridge\n",
            "two two men <unk> a tree next to another a bridge .\n",
            "The two men <unk> a tree next to a bridge .\n",
            "The two men <unk> the tree next to a bridge .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOEpfyhzRDxs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Downsampling\n",
        "\"\"\"\n",
        "\n",
        "import random\n",
        "\n",
        "# Training\n",
        "f = open('./snli_1.0/snli_1.0_train.txt')\n",
        "contents = f.read()\n",
        "file_as_list = contents.splitlines()\n",
        "sentences = []\n",
        "for example in file_as_list[1:]:\n",
        "    sentence1 = example.split('\\t')[5]\n",
        "    sentence2 = example.split('\\t')[6]\n",
        "    sent1_len = len(sentence1.split())\n",
        "    sent2_len = len(sentence2.split())\n",
        "    if sent1_len <= 12: sentences.append(sentence1)\n",
        "    if sent2_len <= 12: sentences.append(sentence2)\n",
        "\n",
        "sample = random.sample(sentences, 40000)\n",
        "\n",
        "f = open('./snli_1.0/sample_train.txt', \"w\")\n",
        "for sentence in sample:\n",
        "    f.write(sentence)\n",
        "    f.write(\"\\n\")\n",
        "f.close()\n",
        "\n",
        "# Development\n",
        "f = open('./snli_1.0/snli_1.0_dev.txt')\n",
        "contents = f.read()\n",
        "file_as_list = contents.splitlines()\n",
        "sentences = []\n",
        "for example in file_as_list[1:]:\n",
        "    sentence1 = example.split('\\t')[5]\n",
        "    sentence2 = example.split('\\t')[6]\n",
        "    sent1_len = len(sentence1.split())\n",
        "    sent2_len = len(sentence2.split())\n",
        "    if sent1_len <= 12: sentences.append(sentence1)\n",
        "    if sent2_len <= 12: sentences.append(sentence2)\n",
        "\n",
        "sample = random.sample(sentences, 4000)\n",
        "\n",
        "f = open('./snli_1.0/sample_dev.txt', \"w\")\n",
        "for sentence in sample:\n",
        "    f.write(sentence)\n",
        "    f.write(\"\\n\")\n",
        "f.close()\n",
        "\n",
        "# Test\n",
        "f = open('./snli_1.0/snli_1.0_test.txt')\n",
        "contents = f.read()\n",
        "file_as_list = contents.splitlines()\n",
        "sentences = []\n",
        "for example in file_as_list[1:]:\n",
        "    sentence1 = example.split('\\t')[5]\n",
        "    sentence2 = example.split('\\t')[6]\n",
        "    sent1_len = len(sentence1.split())\n",
        "    sent2_len = len(sentence2.split())\n",
        "    if sent1_len <= 12: sentences.append(sentence1)\n",
        "    if sent2_len <= 12: sentences.append(sentence2)\n",
        "\n",
        "sample = random.sample(sentences, 4000)\n",
        "\n",
        "f = open('./snli_1.0/sample_test.txt', \"w\")\n",
        "for sentence in sample:\n",
        "    f.write(sentence)\n",
        "    f.write(\"\\n\")\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}