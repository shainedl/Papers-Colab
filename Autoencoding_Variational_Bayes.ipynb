{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autoencoding_Variational_Bayes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shainedl/Papers-Colab/blob/master/Autoencoding_Variational_Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XP8kT-JxlTMG",
        "colab_type": "text"
      },
      "source": [
        "Based on *Auto-Encoding Variational Bayes* by Diederick P Kigma and Max Welling (Machine Learning Group, Universiteit van Amsterdam)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_2RvUvdlChL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "import torchvision \n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f_QS2nnlPND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VAE(nn.Module):\n",
        "  def __init__(self, input_size, hidden_units, N_z):\n",
        "    super(VAE, self).__init__()\n",
        "    \n",
        "    self.fc1 = nn.Linear(input_size, hidden_units)\n",
        "    self.fc21 = nn.Linear(hidden_units, N_z)\n",
        "    self.fc22 = nn.Linear(hidden_units, N_z)\n",
        "    self.fc3 = nn.Linear(N_z, hidden_units)\n",
        "    self.fc4 = nn.Linear(hidden_units, input_size)\n",
        "    \n",
        "    self.input_size = input_size\n",
        "  \n",
        "  def encode(self, x):\n",
        "    \"\"\"\n",
        "    Produces a Gaussian distribution over the possible values of the code z \n",
        "    from which x could have been generated\n",
        "    According to Appendix C.2\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "      x: batch size x input size (after reshape) Tensor\n",
        "        observed data\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "      mu: batch size x N_z (z dimensions) Tensor\n",
        "        mean of Gaussian distribution\n",
        "        \n",
        "      logvar: batch size x N_z (z dimensions) Tensor \n",
        "        log of variance of Gaussian distribution\n",
        "    \"\"\"\n",
        "    h_e  = torch.tanh(self.fc1(x.view(-1,self.input_size)))\n",
        "    mu = self.fc21(h_e)\n",
        "    logvar = self.fc22(h_e)\n",
        "    \n",
        "    return mu, logvar\n",
        "  \n",
        "  def decode(self, z):\n",
        "    \"\"\"\n",
        "    Given a code z it produces a Bernoulli distribution over the possible\n",
        "    corresponding values of x\n",
        "    According to Appendix C.1\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "      z: batch size x N_z (z dimensions) Tensor\n",
        "        latent variables\n",
        "      \n",
        "    Returns\n",
        "    -------\n",
        "      batch size x input size Tensor\n",
        "        reconstructed x\n",
        "    \"\"\"\n",
        "    h_d = torch.tanh(self.fc3(z))\n",
        "    \n",
        "    return torch.sigmoid(self.fc4(h_d))\n",
        "  \n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Forward pass of the model \n",
        "    \"\"\"\n",
        "    mu, logvar = self.encode(x)\n",
        "    z = self.__reparameterize(mu, logvar)\n",
        "    \n",
        "    return self.decode(z), mu, logvar\n",
        "  \n",
        "  def __reparameterize(self, mu, logvar):\n",
        "    \"\"\"\n",
        "    Reparameterize the random variable z to express as a deterministic variable\n",
        "    \"\"\"\n",
        "    std = torch.exp(logvar / 2)\n",
        "    eps = torch.randn_like(std)\n",
        "    \n",
        "    return mu + std * eps\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oI2droyQsv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(mu, logvar, y, x):\n",
        "  \"\"\"\n",
        "  KL according to Appendix B\n",
        "  \"\"\"\n",
        "  KL = torch.sum(1 + logvar - mu**2 - torch.exp(logvar)) / 2\n",
        "  \n",
        "  RE = F.binary_cross_entropy(y, x.view(-1,784), reduction = 'sum')\n",
        "  \n",
        "  elbo = KL - RE\n",
        "  loss = -1 * elbo\n",
        "  \n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCJfBpMrzwhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epoch):\n",
        "  \n",
        "  model.train()\n",
        "  \n",
        "  running_loss = 0.0\n",
        "  for batch_idx, data in enumerate(train_loader):\n",
        "    inputs, labels = data\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "    # forward + backward + optimize\n",
        "    y, mu, logvar = model(inputs)\n",
        "    loss = loss_function(mu, logvar, y, inputs)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "    if batch_idx % 100 == 99:    # print every 100 mini-batches\n",
        "      print('[%d, %5d] Train loss: %.3f' % (epoch + 1, batch_idx + 1, running_loss / 100))\n",
        "      running_loss = 0.0      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeoN3eGt9bA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(epoch):\n",
        "  \n",
        "  model.eval()\n",
        "  \n",
        "  running_loss = 0.0\n",
        "  for batch_idx, data in enumerate(test_loader):\n",
        "    inputs, labels = data\n",
        "\n",
        "    y, mu, logvar = model(inputs)\n",
        "    running_loss += loss_function(mu, logvar, y, inputs).item()\n",
        "\n",
        "  print('[%d, %5d] Test loss: %.3f' % (epoch + 1, batch_idx + 1, running_loss / 100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQyY9Sc9dxxP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "f7cd766f-6e0a-46fc-b51c-011bf4cb75f3"
      },
      "source": [
        "batch_size = 100\n",
        "\"\"\"\n",
        "https://nextjournal.com/gkoehler/pytorch-mnist\n",
        "Remove the normalization to create Bernoulli data\n",
        "\"\"\"\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor()\n",
        "                             ])),\n",
        "  batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor()\n",
        "                             ])),\n",
        "  batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /files/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:02, 4291061.58it/s]                             \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /files/MNIST/raw/train-images-idx3-ubyte.gz to /files/MNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /files/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 57455.55it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /files/MNIST/raw/train-labels-idx1-ubyte.gz to /files/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /files/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:01, 969717.43it/s]                             \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /files/MNIST/raw/t10k-images-idx3-ubyte.gz to /files/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /files/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 21767.07it/s]            "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /files/MNIST/raw/t10k-labels-idx1-ubyte.gz to /files/MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhNEp6PSwD-0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b881d59d-f99d-4a22-890e-352a2e31d8b6"
      },
      "source": [
        "len(train_loader)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "600"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cbQc8lZ76kiJ",
        "colab": {}
      },
      "source": [
        "examples = enumerate(train_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNhkkGqH6mE3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dc8995d0-61aa-4d31-a5a1-3a62abbc4e9f"
      },
      "source": [
        "example_data[0].size()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L95RklBI3Ed",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "8ac45b91-855c-49ae-d67f-367a2b86fbb7"
      },
      "source": [
        "plt.imshow(example_data[0].view(28,28).detach().numpy())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f4f5d2549e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngrADQRB2IAjCDgRB2IEgCDsQBGEHgvg/9f0vhFhTP14AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_H3B7YF3zkR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "16eb09ad-c097-4657-d936-8718e7ed1f35"
      },
      "source": [
        "# input_size = 28 * 28 = 784\n",
        "model = VAE(784, 500, 20)\n",
        "optimizer = optim.Adagrad(model.parameters(), lr=1e-2)\n",
        "\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "  train(epoch)\n",
        "  test(epoch)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   100] Train loss: 20986.326\n",
            "[1,   200] Train loss: 16105.736\n",
            "[1,   300] Train loss: 15070.960\n",
            "[1,   400] Train loss: 14477.962\n",
            "[1,   500] Train loss: 14030.065\n",
            "[1,   600] Train loss: 13689.535\n",
            "[1,   100] Test loss: 13393.386\n",
            "[2,   100] Train loss: 13421.904\n",
            "[2,   200] Train loss: 13290.443\n",
            "[2,   300] Train loss: 13054.070\n",
            "[2,   400] Train loss: 13033.963\n",
            "[2,   500] Train loss: 12879.618\n",
            "[2,   600] Train loss: 12742.299\n",
            "[2,   100] Test loss: 12656.682\n",
            "[3,   100] Train loss: 12762.029\n",
            "[3,   200] Train loss: 12716.090\n",
            "[3,   300] Train loss: 12661.805\n",
            "[3,   400] Train loss: 12595.447\n",
            "[3,   500] Train loss: 12559.858\n",
            "[3,   600] Train loss: 12556.954\n",
            "[3,   100] Test loss: 12395.341\n",
            "[4,   100] Train loss: 12538.357\n",
            "[4,   200] Train loss: 12453.994\n",
            "[4,   300] Train loss: 12406.548\n",
            "[4,   400] Train loss: 12434.434\n",
            "[4,   500] Train loss: 12423.593\n",
            "[4,   600] Train loss: 12450.919\n",
            "[4,   100] Test loss: 12265.799\n",
            "[5,   100] Train loss: 12386.373\n",
            "[5,   200] Train loss: 12313.010\n",
            "[5,   300] Train loss: 12285.700\n",
            "[5,   400] Train loss: 12262.037\n",
            "[5,   500] Train loss: 12289.667\n",
            "[5,   600] Train loss: 12321.609\n",
            "[5,   100] Test loss: 12140.807\n",
            "[6,   100] Train loss: 12248.786\n",
            "[6,   200] Train loss: 12176.686\n",
            "[6,   300] Train loss: 12249.062\n",
            "[6,   400] Train loss: 12222.997\n",
            "[6,   500] Train loss: 12169.439\n",
            "[6,   600] Train loss: 12132.332\n",
            "[6,   100] Test loss: 12036.154\n",
            "[7,   100] Train loss: 12161.298\n",
            "[7,   200] Train loss: 12117.327\n",
            "[7,   300] Train loss: 12103.301\n",
            "[7,   400] Train loss: 12062.363\n",
            "[7,   500] Train loss: 12097.358\n",
            "[7,   600] Train loss: 12065.308\n",
            "[7,   100] Test loss: 11947.842\n",
            "[8,   100] Train loss: 12067.657\n",
            "[8,   200] Train loss: 12063.980\n",
            "[8,   300] Train loss: 11965.753\n",
            "[8,   400] Train loss: 12013.599\n",
            "[8,   500] Train loss: 11997.757\n",
            "[8,   600] Train loss: 12014.675\n",
            "[8,   100] Test loss: 11875.542\n",
            "[9,   100] Train loss: 11974.300\n",
            "[9,   200] Train loss: 11938.069\n",
            "[9,   300] Train loss: 11953.380\n",
            "[9,   400] Train loss: 11983.840\n",
            "[9,   500] Train loss: 11916.324\n",
            "[9,   600] Train loss: 11941.974\n",
            "[9,   100] Test loss: 11808.227\n",
            "[10,   100] Train loss: 11883.447\n",
            "[10,   200] Train loss: 11906.097\n",
            "[10,   300] Train loss: 11912.823\n",
            "[10,   400] Train loss: 11873.909\n",
            "[10,   500] Train loss: 11908.955\n",
            "[10,   600] Train loss: 11838.957\n",
            "[10,   100] Test loss: 11748.378\n",
            "[11,   100] Train loss: 11840.750\n",
            "[11,   200] Train loss: 11852.375\n",
            "[11,   300] Train loss: 11844.158\n",
            "[11,   400] Train loss: 11847.509\n",
            "[11,   500] Train loss: 11787.761\n",
            "[11,   600] Train loss: 11787.373\n",
            "[11,   100] Test loss: 11696.841\n",
            "[12,   100] Train loss: 11810.715\n",
            "[12,   200] Train loss: 11770.579\n",
            "[12,   300] Train loss: 11758.309\n",
            "[12,   400] Train loss: 11764.435\n",
            "[12,   500] Train loss: 11750.558\n",
            "[12,   600] Train loss: 11745.789\n",
            "[12,   100] Test loss: 11653.739\n",
            "[13,   100] Train loss: 11781.905\n",
            "[13,   200] Train loss: 11745.872\n",
            "[13,   300] Train loss: 11716.076\n",
            "[13,   400] Train loss: 11673.082\n",
            "[13,   500] Train loss: 11719.787\n",
            "[13,   600] Train loss: 11654.900\n",
            "[13,   100] Test loss: 11603.430\n",
            "[14,   100] Train loss: 11685.807\n",
            "[14,   200] Train loss: 11647.317\n",
            "[14,   300] Train loss: 11668.007\n",
            "[14,   400] Train loss: 11646.814\n",
            "[14,   500] Train loss: 11661.374\n",
            "[14,   600] Train loss: 11695.734\n",
            "[14,   100] Test loss: 11538.622\n",
            "[15,   100] Train loss: 11638.694\n",
            "[15,   200] Train loss: 11631.247\n",
            "[15,   300] Train loss: 11593.331\n",
            "[15,   400] Train loss: 11631.996\n",
            "[15,   500] Train loss: 11604.699\n",
            "[15,   600] Train loss: 11636.649\n",
            "[15,   100] Test loss: 11498.719\n",
            "[16,   100] Train loss: 11609.800\n",
            "[16,   200] Train loss: 11549.352\n",
            "[16,   300] Train loss: 11561.484\n",
            "[16,   400] Train loss: 11595.271\n",
            "[16,   500] Train loss: 11585.126\n",
            "[16,   600] Train loss: 11589.596\n",
            "[16,   100] Test loss: 11464.377\n",
            "[17,   100] Train loss: 11553.538\n",
            "[17,   200] Train loss: 11567.998\n",
            "[17,   300] Train loss: 11546.498\n",
            "[17,   400] Train loss: 11522.156\n",
            "[17,   500] Train loss: 11542.744\n",
            "[17,   600] Train loss: 11502.558\n",
            "[17,   100] Test loss: 11418.099\n",
            "[18,   100] Train loss: 11484.794\n",
            "[18,   200] Train loss: 11522.019\n",
            "[18,   300] Train loss: 11534.337\n",
            "[18,   400] Train loss: 11486.112\n",
            "[18,   500] Train loss: 11486.875\n",
            "[18,   600] Train loss: 11480.182\n",
            "[18,   100] Test loss: 11388.735\n",
            "[19,   100] Train loss: 11477.140\n",
            "[19,   200] Train loss: 11536.297\n",
            "[19,   300] Train loss: 11438.601\n",
            "[19,   400] Train loss: 11472.657\n",
            "[19,   500] Train loss: 11416.448\n",
            "[19,   600] Train loss: 11435.928\n",
            "[19,   100] Test loss: 11353.687\n",
            "[20,   100] Train loss: 11442.925\n",
            "[20,   200] Train loss: 11411.715\n",
            "[20,   300] Train loss: 11385.804\n",
            "[20,   400] Train loss: 11478.259\n",
            "[20,   500] Train loss: 11418.795\n",
            "[20,   600] Train loss: 11438.253\n",
            "[20,   100] Test loss: 11331.033\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGztHz_V7mu3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample = torch.randn(1, 20)\n",
        "sample = model.decode(sample)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rqc-d-gCH52B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "45907336-bb34-4d59-f6e5-da71daf7f978"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(sample.view(28,28).detach().numpy())"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f4f5be4cba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAQiElEQVR4nO3df2xd9XnH8c9jx7/ixCEmJKQQKGEB\nGpgaJo+uhTFathaYKug/qGiqmIqWaipSkfrHENNUpP3DprWs0qpKYbCGqaXq1iL4g7ZA1jZjkxgB\npSRAS4AFkmDyg0Acx3FsXz/7wzedCz7PMb4/k+f9kizb5/HxfXKST86993vO92vuLgCnv45WNwCg\nOQg7kARhB5Ig7EAShB1IYlEzH6zberxX/c18SCCVcR3ThJ+wuWo1hd3MrpP0TUmdkv7Z3e+Jfr5X\n/fqYXVvLQwIIPO1bCmsLfhpvZp2SviXpeknrJd1iZusX+vsANFYtr9mvkPSKu7/m7hOSvi/pxvq0\nBaDeagn7OZL2zPp+b3XbbzGzjWa2zcy2TepEDQ8HoBYNfzfe3Te5+5C7D3Wpp9EPB6BALWHfJ2nN\nrO/PrW4D0IZqCfszktaZ2QVm1i3p85IerU9bAOptwUNv7j5lZrdL+qlmht4ecPcX6tYZgLqqaZzd\n3R+T9FidegHQQFwuCyRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxA\nEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASTV2yGQU6OuNyX29cH1haWJs+84xwX5ueDus6\n+E5Ynh4ZCes+MREUPX5s1BVndiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgnH2JrCu7rDeuWIwrI99\ndE1Y3/dHxX+NPZccCfcdPx6P4XfvXBHW1/w4/v2dr+4prFVGj4X7ykuuAWCc/gOpKexmtlvSUUkV\nSVPuPlSPpgDUXz3O7J9090N1+D0AGojX7EAStYbdJT1uZs+a2ca5fsDMNprZNjPbNqkTNT4cgIWq\n9Wn8Ve6+z8xWSnrCzH7l7ltn/4C7b5K0SZIGbJB3VIAWqenM7u77qp8PSHpY0hX1aApA/S047GbW\nb2ZLT34t6dOSdtarMQD1VcvT+FWSHjazk7/ne+7+k7p0daqZOQbF5c74/9TKh84M63s/Ff81XX31\njsLa7y7dG+773Mh5Yf2/DqwP65Ul8TUEHd1dC6pJkldKxtkbyCuVkh+osbcWXCOw4LC7+2uSPlrH\nXgA0EENvQBKEHUiCsANJEHYgCcIOJMEtrk1gvT1h/Z2PFE8FLUnLL43vM7pkyXBhrVPxEM/IRF9Y\nX3QsHlac7i45X1hQ74j37Sg5btYb356rnuJhQR8bD3f1sbG4fiK+9Lt06K4FOLMDSRB2IAnCDiRB\n2IEkCDuQBGEHkiDsQBKMszdB2XjwOxfHY9mfWbU7rA92Fk/J/OLYh8J9XxxeFdb734x76zpSMtVY\ndCtoZ7xUddlxm1x7dlwfKB5n7zl0PNy343/fDOtl4+ztiDM7kARhB5Ig7EAShB1IgrADSRB2IAnC\nDiTBOHszlNyXPbk8npZ4YFF87/X+yWWFtf/Ye1G4b9cvl4T1Zbsnw7qNT8X1YKzceuLjUlm5PKwf\nXr84rHtwKjurZClSH6/xfvU2XE6aMzuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJME4ezOUzI+uJfFY\n9dLOeJz9yFTx3O9j4/GSyn2jYVk2FY8Xe1+87HKl54zC2nRP/M9vZG08p32lJ77XfuCN4uPasbt4\nrn1JqhyPj3k7jqOXKT2zm9kDZnbAzHbO2jZoZk+Y2a7q5/jqBwAtN5+n8d+RdN17tt0paYu7r5O0\npfo9gDZWGnZ33yrp8Hs23yhpc/XrzZJuqnNfAOpsoa/ZV7n7yRc9b0kqnMjMzDZK2ihJvYqvZQbQ\nODW/G+/uLhWvHujum9x9yN2HuhTf+ACgcRYa9v1mtlqSqp8P1K8lAI2w0LA/KunW6te3SnqkPu0A\naJTS1+xm9pCkayStMLO9kr4m6R5JPzCz2yS9LunmRjZ5qvOu+DB3dsf3Rh+e6g/rr46uKKxNHIvH\n2bvjoWwdXxH3PjEQz/0+1Vt8Puk6Ht/HP90Zj6MvPhAft/6XDhb/7iNHw3013X7rq9eqNOzufktB\n6do69wKggbhcFkiCsANJEHYgCcIOJEHYgSS4xbUJyobepkbj20TfnYzHx3o7i2/lXLJ8LNz32GXx\n8Nfo75QsqzwRny969xcPny0ejvftfyse/lr8xkhY9+Hia718Kp4i+3TEmR1IgrADSRB2IAnCDiRB\n2IEkCDuQBGEHkmCcvR4s/j/TSpb3LRurPjYVz/BzXt97pwj8f+svjKdMLpumusxPD64P6786cX5h\nbfmv49/dN3wsrHccfDesVyaCsfRTcCroWnFmB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkGGdvBoun\nRPbOeMx3XX+8BsfH+3cV1s5eFE+Z3GvxNQBvTi0N68/3nRvWX54s/rNPLypZDrorvpe+dCnsjvi4\nZ8OZHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSYJy9CSpL4vvRFy2bCOuXL94d1td1vVNYK1n1WIcr\n8Zz1vxi9JKz/5+tr4wcIpqUfOys+10z1xktVL+uL//l2B/ezVw4dCvc9He93Lz2zm9kDZnbAzHbO\n2na3me0zs+3Vjxsa2yaAWs3nafx3JF03x/Z73X1D9eOx+rYFoN5Kw+7uWyUVz3sE4JRQyxt0t5vZ\n89Wn+cuLfsjMNprZNjPbNqkTNTwcgFosNOzflnShpA2ShiV9vegH3X2Tuw+5+1CX4jeqADTOgsLu\n7vvdveLu05Luk3RFfdsCUG8LCruZrZ717eck7Sz6WQDtoXSc3cweknSNpBVmtlfS1yRdY2YbJLmk\n3ZK+1MAe2551xvddnxjsDesrB98O6/0d8Xsdh6e7C2tPjl4a7nvfjivD+rKfx2vDDx6Jx6PHVhYP\n9I9cEt9L331WvLb8oUNxb2s7ziusdW09Eu7rk/G1D6ei0rC7+y1zbL6/Ab0AaCAulwWSIOxAEoQd\nSIKwA0kQdiAJbnGth5IpixcdnwrrB47FQ0hbS24z3TJ8cWFt/N9Xhfte/JM3wrqPxFNRqzu+Rbbr\n4xcW1s74bDxF9h0XPBnW90ycGdb/6XDxzZhr/ye+mvN0HHrjzA4kQdiBJAg7kARhB5Ig7EAShB1I\ngrADSTDOXgdWsiSzTQTzKUua3DUQ1h889ImwvvKp4r/GlY+/Gu5beTueXtAr8W2oHT0l49XB6eQv\nz/9FuO8n+w6G9SM9b4X1b110dWHNOvOd5/L9iYGkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcbZ68BL\nlvftPBpPBd03HC9N3Lc/vmd8YPfx4uKJGpfcsvh8YEvi3vd8tvgag+v73wz3XdYR3+cvBX9uSe7F\n1z94sJzz6YozO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwTh7PZTd833onbA+sGdZWJ9YUvJ/cjDO\nb4PLw107ahxHH/7TNWH9kT++t7BWPo4eO1xy3Pt/Xtz79HiN1x+cgkrP7Ga2xsx+ZmYvmtkLZvaV\n6vZBM3vCzHZVP8f/qgC01Hyexk9J+qq7r5f0B5K+bGbrJd0paYu7r5O0pfo9gDZVGnZ3H3b356pf\nH5X0kqRzJN0oaXP1xzZLuqlRTQKo3Qd6zW5mH5Z0uaSnJa1y9+Fq6S1Jcy4qZmYbJW2UpF4tXmif\nAGo073fjzWyJpB9KusPdR2bXfOZOkDnfJXL3Te4+5O5DXYonJwTQOPMKu5l1aSbo33X3H1U37zez\n1dX6aknxkpwAWqr0abzNzJN8v6SX3P0bs0qPSrpV0j3Vz480pMNTQNl0y9Mlyx4v3RFPmTx2Ubw0\n8eh5vcWPvXZ1uO/0orj+9oZ4Gux/vH5zWL+0qzusR054fBvqF1/+s7B+9r+9XFirTMd/Z6ej+bxm\nv1LSFyTtMLPt1W13aSbkPzCz2yS9LunmxrQIoB5Kw+7uT0kqmgXg2vq2A6BRuFwWSIKwA0kQdiAJ\nwg4kQdiBJLjFtR5KppKePh5PeWxv7Avri0dGw/qij5xbWHvzD4vH4CVpfN14WP/ihv8O65/oja8R\nkIpvYx2bngj3/Lu3L49/8+3xFNtly1Fnw5kdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5JgnL0ZSsbh\nvWRZ5cqBeCx70btHCmsXvH52uO+7vx/fz/4vh64J6+Ofise6L+vbW1j72503hPue/zfx/eyVXa+G\n9bLjng1ndiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IwryJY5EDNugfMyakbSsdnXG5Ox5Ht2UDcb2j\n+HxSNp/+9NhYWGcc/f2e9i0a8cNzzgbNmR1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkpjP+uxrJD0o\naZUkl7TJ3b9pZndL+gtJJ2+2vsvdH2tUo2iQknXKp8dL1jEfj+edR/uYz+QVU5K+6u7PmdlSSc+a\n2RPV2r3u/g+Naw9AvcxnffZhScPVr4+a2UuSzml0YwDq6wO9ZjezD0u6XNLT1U23m9nzZvaAmS0v\n2GejmW0zs22TiqdfAtA48w67mS2R9ENJd7j7iKRvS7pQ0gbNnPm/Ptd+7r7J3YfcfahLPXVoGcBC\nzCvsZtalmaB/191/JEnuvt/dK+4+Lek+SVc0rk0AtSoNu5mZpPslveTu35i1ffa0pJ+TtLP+7QGo\nl/m8G3+lpC9I2mFm26vb7pJ0i5lt0Mxw3G5JX2pIhwDqYj7vxj8laa77YxlTB04hXEEHJEHYgSQI\nO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IoqlLNpvZQUmvz9q0QtKh\npjXwwbRrb+3al0RvC1XP3s5397PmKjQ17O97cLNt7j7UsgYC7dpbu/Yl0dtCNas3nsYDSRB2IIlW\nh31Tix8/0q69tWtfEr0tVFN6a+lrdgDN0+ozO4AmIexAEi0Ju5ldZ2a/NrNXzOzOVvRQxMx2m9kO\nM9tuZtta3MsDZnbAzHbO2jZoZk+Y2a7q5znX2GtRb3eb2b7qsdtuZje0qLc1ZvYzM3vRzF4ws69U\nt7f02AV9NeW4Nf01u5l1SnpZ0p9I2ivpGUm3uPuLTW2kgJntljTk7i2/AMPMrpY0KulBd7+suu3v\nJR1293uq/1Eud/e/apPe7pY02uplvKurFa2evcy4pJsk/blaeOyCvm5WE45bK87sV0h6xd1fc/cJ\nSd+XdGML+mh77r5V0uH3bL5R0ubq15s184+l6Qp6awvuPuzuz1W/Pirp5DLjLT12QV9N0YqwnyNp\nz6zv96q91nt3SY+b2bNmtrHVzcxhlbsPV79+S9KqVjYzh9JlvJvpPcuMt82xW8jy57XiDbr3u8rd\nf0/S9ZK+XH262pZ85jVYO42dzmsZ72aZY5nx32jlsVvo8ue1akXY90laM+v7c6vb2oK776t+PiDp\nYbXfUtT7T66gW/18oMX9/EY7LeM91zLjaoNj18rlz1sR9mckrTOzC8ysW9LnJT3agj7ex8z6q2+c\nyMz6JX1a7bcU9aOSbq1+faukR1rYy29pl2W8i5YZV4uPXcuXP3f3pn9IukEz78i/KumvW9FDQV9r\nJf2y+vFCq3uT9JBmntZNaua9jdsknSlpi6Rdkp6UNNhGvf2rpB2SntdMsFa3qLerNPMU/XlJ26sf\nN7T62AV9NeW4cbkskARv0AFJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEv8H1qL7c8+9WusAAAAASUVO\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9i7mOmMUNEq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}