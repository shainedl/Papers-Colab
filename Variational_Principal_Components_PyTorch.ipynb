{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Variational_Principal_Components-PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shainedl/Papers-Colab/blob/master/Variational_Principal_Components_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWamupqWQXfW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.distributions as tdist\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal as multivariate_normal\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L50MQamPPB_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BayesianPCA():\n",
        "  \n",
        "  def __init__(self, a_alpha=10e-3, b_alpha=10e-3, a_tau=10e-3, b_tau=10e-3, beta=10e-3):\n",
        "    \n",
        "    # hyperparameters\n",
        "    self.a_alpha = a_alpha\n",
        "    self.b_alpha = b_alpha\n",
        "    self.a_tau = a_tau\n",
        "    self.b_tau = b_tau\n",
        "    self.beta = beta \n",
        "     \n",
        "  def __reestimate(self):\n",
        "    \"\"\"\n",
        "    Cycle through the groups of variables in turn to re-estimate each distribution \n",
        "    \"\"\"\n",
        "    \n",
        "    # observation parameter\n",
        "    self.tau = self.a_tau_tilde / self.b_tau_tilde\n",
        "    \n",
        "    # latent variables\n",
        "    self.sigma_x = torch.inverse(torch.eye(self.q, dtype=torch.float64) + self.tau *\n",
        "                   (torch.trace(self.sigma_w) + torch.mm(self.mean_w.t(), self.mean_w)))\n",
        "    self.mean_x = self.tau * torch.mm(torch.mm(self.sigma_x, self.mean_w.t()),(self.t_n - self.mean_mu))\n",
        "    \n",
        "    # observation parameter                                \n",
        "    self.sigma_mu = torch.eye(self.d, dtype=torch.float64) / (self.beta + self.N * self.tau)\n",
        "    w_x = torch.mm(self.mean_w, self.mean_x)\n",
        "    sum = 0\n",
        "    for n in range(self.N):\n",
        "      sum += torch.sub(self.t_n[:,n], w_x[:,n])\n",
        "    self.mean_mu = self.tau * torch.mm(self.sigma_mu, torch.reshape(sum, (-1,1)))\n",
        "        \n",
        "    # hyperparameter controlling the columns of W\n",
        "    self.alpha = self.a_alpha_tilde / self.b_alpha_tilde\n",
        "                                     \n",
        "    # weight                                 \n",
        "    self.sigma_w = torch.inverse(torch.diag(self.alpha) + self.tau * \n",
        "                   (self.N * self.sigma_x + torch.mm(self.mean_x, self.mean_x.t())))\n",
        "    self.mean_w = (self.tau * torch.mm(self.sigma_w, torch.mm(self.mean_x, (torch.sub(self.t_n.t(), self.mean_mu.t()))))).t()\n",
        "\n",
        "    # alpha's gamma distribution parameter                            \n",
        "    self.b_alpha_tilde = self.b_alpha + 0.5 * (torch.trace(self.sigma_w) + torch.diag(torch.mm(self.mean_w.t(), self.mean_w)))                                                     \n",
        "    # tau's gamma distribution parameter     \n",
        "    self.b_tau_tilde = torch.tensor([self.b_tau + 0.5 * torch.sum(torch.mm(self.t_n.t(), self.t_n)) + \\\n",
        "                       0.5 * self.N * (torch.trace(self.sigma_mu) + torch.dot(self.mean_mu.flatten(), self.mean_mu.flatten()))+ \\\n",
        "                       0.5 * torch.trace(torch.mm(torch.trace(self.sigma_w) + \\\n",
        "                       torch.mm(self.mean_w.t(), self.mean_w), self.N * self.sigma_x + \\\n",
        "                       torch.mm(self.mean_x, self.mean_x.t()))) + \\\n",
        "                       torch.sum(torch.mm(torch.mm(self.mean_mu.t(), self.mean_w), self.mean_x)) - \\\n",
        "                       torch.sum(torch.mm(torch.mm(self.t_n.t(), self.mean_w), self.mean_x)) - \\\n",
        "                       torch.sum(torch.mm(self.t_n.t(), self.mean_mu))])      \n",
        "    \n",
        "  def __get_elbo(self):\n",
        "    \"\"\"\n",
        "    Computes the rigorous lower bound on the true log marginal likelihood \n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "      float\n",
        "        the lower bound = prior + likelihood - entropy \n",
        "    \n",
        "    \"\"\"                             \n",
        "    # random sample\n",
        "    x = torch.stack([multivariate_normal(self.mean_x[:,n], self.sigma_x).sample() for n in range(self.N)]).t()\n",
        "    mu = multivariate_normal(self.mean_mu.flatten(), self.sigma_mu).sample()\n",
        "    w = torch.stack([multivariate_normal(self.mean_w[i], self.sigma_w).sample() for i in range(self.d)])\n",
        "    alpha = tdist.gamma.Gamma(self.a_alpha_tilde, self.b_alpha_tilde).sample()     \n",
        "    tau = tdist.gamma.Gamma(self.a_tau_tilde, self.b_tau_tilde).sample()\n",
        "\n",
        "    # priors\n",
        "    # p(x) = N(x|0,I_q)\n",
        "    prior = torch.sum(torch.stack([multivariate_normal(torch.zeros(self.q, dtype=torch.float64), \\\n",
        "              torch.eye(self.q, dtype=torch.float64)).log_prob(x[:,i]) for i in range(self.N)]))\n",
        "    # p(w|alpha) = conditional distribution                   \n",
        "    prior += torch.sum(torch.stack([(self.d / 2) * torch.log(alpha[i] / \\\n",
        "              (2 * math.pi)) - 0.5 * alpha[i] * torch.sum(w[:,i]**2) \\\n",
        "              for i in range(self.q)]))                                      \n",
        "    # p(alpha) = Gamma(a, b)                             \n",
        "    prior += torch.sum((tdist.gamma.Gamma(self.a_alpha, self.b_alpha)).log_prob(alpha))                                  \n",
        "    # p(mu) = N(mu|0,Beta^-1I)       \n",
        "    prior += multivariate_normal(torch.zeros(self.d, dtype=torch.float64), torch.eye(self.d, dtype=torch.float64)/self.beta).log_prob(mu) \n",
        "    # p(tau) = Gamma(c, d)      \n",
        "    prior += torch.sum((tdist.gamma.Gamma(self.a_tau, self.b_tau)).log_prob(tau))            \n",
        "    \n",
        "    # log likelihood of the conditional distribution \n",
        "    # p(t_n | x_n, W, mu, tau)\n",
        "    w_x = torch.mm(w, x)\n",
        "    list_t = []\n",
        "    for n in range(self.N):\n",
        "      list_t.append(w_x[:,n] + mu)\n",
        "    likelihood = torch.sum(torch.stack([multivariate_normal(torch.stack(list_t).t()[:,n], \\\n",
        "                  torch.eye(self.d, dtype=torch.float64) / tau).log_prob(self.t_n[:,n]) for n in range(self.N)]))   \n",
        "    \n",
        "    # entropy\n",
        "    # q(x) \n",
        "    entropy = self.N * (0.5 * torch.log((torch.cholesky(self.sigma_x).diag().prod()**2) \\\n",
        "                + (self.d / 2) * (1 + torch.log(torch.DoubleTensor([2 * math.pi])))))   \n",
        "                  \n",
        "    # q(mu)\n",
        "    entropy += 0.5 * torch.log((torch.cholesky(self.sigma_mu).diag().prod()**2) \\\n",
        "                + (self.d / 2) * torch.log(torch.DoubleTensor([2 * math.pi])))\n",
        "                \n",
        "    # q(W)   \n",
        "    entropy += self.d * (0.5 * torch.log((torch.cholesky(self.sigma_w).diag().prod()**2) \\\n",
        "                + (self.d / 2) * torch.log(torch.DoubleTensor([2 * math.pi]))))  \n",
        "                  \n",
        "    # q(alpha)\n",
        "    entropy += self.q * (torch.log(torch.lgamma(torch.DoubleTensor([self.a_alpha_tilde])).exp()) \\\n",
        "                        - (self.a_alpha_tilde - 1) \\\n",
        "                        * torch.digamma(torch.DoubleTensor([self.a_alpha_tilde])) + self.a_alpha_tilde)\n",
        "    for i in range(self.q):\n",
        "      entropy -= torch.log(self.b_alpha_tilde[i])\n",
        "                 \n",
        "    # q(tau)  \n",
        "    entropy += -1*(torch.DoubleTensor([self.a_tau_tilde - 1]) * torch.digamma(torch.DoubleTensor([self.a_tau_tilde])) \\\n",
        "               - torch.log(torch.DoubleTensor([self.b_tau_tilde])) + torch.DoubleTensor([self.a_tau_tilde]))\n",
        "    # will ignore torch.log(torch.lgamma(torch.Tensor([self.a_tau_tilde])).exp()) since = inf\n",
        "\n",
        "    return prior + likelihood - entropy \n",
        "  \n",
        "  def fit(self, t_n, iterations = 1000, threshold = 0.05):\n",
        "    \"\"\"\n",
        "    Fits the data\n",
        "    \n",
        "    Parameters \n",
        "    ----------\n",
        "    t_n : d x N matrix\n",
        "      observed data to be fit\n",
        "      \n",
        "    iterations: int\n",
        "      number of iterations to re-estimate the lower bound\n",
        "    \n",
        "    threshold: float\n",
        "      determines convergence\n",
        "      \n",
        "    \"\"\"\n",
        "    self.t_n = t_n\n",
        "    self.d = self.t_n.shape[0]                     \n",
        "    self.q = self.d - 1\n",
        "    self.N = self.t_n.shape[1]   \n",
        "    \n",
        "    # variational parameters\n",
        "    self.mean_x = torch.randn(self.q, self.N, dtype=torch.float64)\n",
        "    self.sigma_x = torch.eye(self.q, dtype=torch.float64)\n",
        "    self.mean_mu = torch.randn(self.d, 1, dtype=torch.float64)\n",
        "    self.sigma_mu = torch.eye(self.d, dtype=torch.float64)\n",
        "    self.mean_w = torch.randn(self.d, self.q, dtype=torch.float64)\n",
        "    self.sigma_w = torch.eye(self.q, dtype=torch.float64)\n",
        "    self.a_alpha_tilde = self.a_alpha + self.d / 2\n",
        "    self.b_alpha_tilde = torch.abs(torch.randn(self.q, dtype=torch.float64))\n",
        "    self.a_tau_tilde = (self.a_tau + self.N * self.d / 2)\n",
        "    self.b_tau_tilde = torch.abs(torch.randn(1, dtype=torch.float64))\n",
        "             \n",
        "    self.elbos = [self.__get_elbo()]  \n",
        "    for i in range(iterations):\n",
        "      self.__reestimate()\n",
        "      self.elbos.append(self.__get_elbo())\n",
        "      if torch.abs(self.elbos[-2] - self.elbos[-1]) <= threshold:\n",
        "        print('ELBO converged.')\n",
        "        print(\"Iterations: \", i+1)                   \n",
        "        print(\"ELBO: \", int(self.elbos[-1])) \n",
        "        break\n",
        "              \n",
        "      if (i+1) % 100 == 0:\n",
        "        print(\"Iterations: \", i+1)                   \n",
        "        print(\"ELBO: \", int(self.elbos[-1])) \n",
        "        \n",
        "        if i == iterations:\n",
        "          print('Ended without convergence.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJr_9k_-q4bm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def hinton(matrix, max_weight=None, ax=None):\n",
        "    \"\"\"\n",
        "    Draw Hinton diagram for visualizing a weight matrix.\n",
        "    From https://matplotlib.org/3.1.1/gallery/specialty_plots/hinton_demo.html\n",
        "    \n",
        "    \"\"\"\n",
        "    ax = ax if ax is not None else plt.gca()\n",
        "\n",
        "    if not max_weight:\n",
        "        max_weight = 2 ** np.ceil(np.log(np.abs(matrix).max()) / np.log(2))\n",
        "\n",
        "    ax.patch.set_facecolor('gray')\n",
        "    ax.set_aspect('equal', 'box')\n",
        "    ax.xaxis.set_major_locator(plt.NullLocator())\n",
        "    ax.yaxis.set_major_locator(plt.NullLocator())\n",
        "\n",
        "    for (x, y), w in np.ndenumerate(matrix):\n",
        "        color = 'white' if w > 0 else 'black'\n",
        "        size = np.sqrt(np.abs(w) / max_weight)\n",
        "        rect = plt.Rectangle([x - size / 2, y - size / 2], size, size,\n",
        "                             facecolor=color, edgecolor=color)\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "    ax.autoscale_view()\n",
        "    ax.invert_yaxis()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLdhtVGJWuKK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "14ba5eeb-a65f-41af-c485-cfa8e68d0294"
      },
      "source": [
        "\"\"\"\n",
        "We generate 100 data points in d = 10 dimensions from a Gaussian distribution \n",
        "having standard deviations of (5, 4, 3, 2) along four orthogonal directions \n",
        "and a standard deviation of 1 in the remaining five directions\n",
        "\"\"\"\n",
        "m = tdist.multivariate_normal.MultivariateNormal(torch.zeros(10, dtype=torch.float64), torch.diag(torch.DoubleTensor([5,4,3,2,1,1,1,1,1,1])))\n",
        "X = m.sample(sample_shape=torch.Size([100])).t()\n",
        "\n",
        "\"\"\"\n",
        "Hinton diagram of <W> from variational Bayesian PCA \n",
        "\"\"\"\n",
        "test = BayesianPCA()\n",
        "test.fit(X) \n",
        "hinton(np.asarray(test.mean_w.t()))"
      ],
      "execution_count": 497,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iterations:  100\n",
            "ELBO:  595\n",
            "Iterations:  200\n",
            "ELBO:  572\n",
            "Iterations:  300\n",
            "ELBO:  566\n",
            "Iterations:  400\n",
            "ELBO:  610\n",
            "Iterations:  500\n",
            "ELBO:  545\n",
            "Iterations:  600\n",
            "ELBO:  537\n",
            "Iterations:  700\n",
            "ELBO:  542\n",
            "Iterations:  800\n",
            "ELBO:  496\n",
            "Iterations:  900\n",
            "ELBO:  573\n",
            "Iterations:  1000\n",
            "ELBO:  577\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAANMAAADuCAYAAACj4zKqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIf8FOmj+hX7JjlYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}