{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Variational_Principal_Components-PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shainedl/Papers-Colab/blob/master/Variational_Principal_Components_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWamupqWQXfW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.distributions as tdist\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal as multivariate_normal\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L50MQamPPB_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BayesianPCA():\n",
        "  \n",
        "  def __init__(self, a_alpha=10e-3, b_alpha=10e-3, a_tau=10e-3, b_tau=10e-3, beta=10e-3):\n",
        "    \n",
        "    # hyperparameters\n",
        "    self.a_alpha = a_alpha\n",
        "    self.b_alpha = b_alpha\n",
        "    self.a_tau = a_tau\n",
        "    self.b_tau = b_tau\n",
        "    self.beta = beta \n",
        "     \n",
        "  def __reestimate(self):\n",
        "    \"\"\"\n",
        "    Cycle through the groups of variables in turn to re-estimate each distribution \n",
        "    \"\"\"\n",
        "    \n",
        "    # observation parameter\n",
        "    self.tau = self.a_tau_tilde / self.b_tau_tilde\n",
        "\n",
        "    # latent variables\n",
        "    self.sigma_x = torch.inverse(torch.eye(self.q) + self.tau *\n",
        "                   (torch.trace(self.sigma_w) + torch.mm(self.mean_w.t(), self.mean_w)))\n",
        "    self.mean_x = self.tau * torch.mm(torch.mm(self.sigma_x, self.mean_w.t()),(self.t_n - self.mean_mu))\n",
        "    \n",
        "    # observation parameter                                \n",
        "    self.sigma_mu = torch.eye(self.d) / (self.beta + self.N * self.tau)\n",
        "    w_x = torch.mm(self.mean_w, self.mean_x)\n",
        "    sum = 0\n",
        "    for n in range(self.N):\n",
        "      sum += torch.sub(self.t_n[:,n], w_x[:,n])\n",
        "    self.mean_mu = self.tau * torch.mm(self.sigma_mu, torch.reshape(sum, (-1,1)))\n",
        "        \n",
        "    # hyperparameter controlling the columns of W\n",
        "    self.alpha = self.a_alpha_tilde / self.b_alpha_tilde\n",
        "                                     \n",
        "    # weight                                 \n",
        "    self.sigma_w = torch.inverse(torch.diag(self.alpha) + self.tau * \n",
        "                   (self.N * self.sigma_x + torch.mm(self.mean_x, self.mean_x.t())))\n",
        "    self.mean_w = (self.tau * torch.mm(self.mean_x, (torch.sub(self.t_n.t(), self.mean_mu.t())))).t()\n",
        "   \n",
        "    # alpha's gamma distribution parameter                            \n",
        "    self.b_alpha_tilde = self.b_alpha + 0.5 * (torch.trace(self.sigma_w) + torch.diag(torch.mm(self.mean_w.t(), self.mean_w)))                                                     \n",
        "    # tau's gamma distribution parameter     \n",
        "    self.b_tau_tilde = torch.tensor([self.b_tau + 0.5 * torch.sum(torch.mm(self.t_n.t(), self.t_n)) + \\\n",
        "                       0.5 * self.N * (torch.trace(self.sigma_mu) + torch.dot(self.mean_mu.flatten(), self.mean_mu.flatten()))+ \\\n",
        "                       0.5 * torch.trace(torch.mm(torch.trace(self.sigma_w) + \\\n",
        "                       torch.mm(self.mean_w.t(), self.mean_w), self.N * self.sigma_x + \\\n",
        "                       torch.mm(self.mean_x, self.mean_x.t()))) + \\\n",
        "                       torch.sum(torch.mm(torch.mm(self.mean_mu.t(), self.mean_w), self.mean_x)) - \\\n",
        "                       torch.sum(torch.mm(torch.mm(self.t_n.t(), self.mean_w), self.mean_x)) - \\\n",
        "                       torch.sum(torch.mm(self.t_n.t(), self.mean_mu))])      \n",
        "    \n",
        "  def __get_elbo(self):\n",
        "    \"\"\"\n",
        "    Computes the rigorous lower bound on the true log marginal likelihood \n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "      float\n",
        "        the lower bound = prior + likelihood - entropy \n",
        "    \n",
        "    \"\"\"                             \n",
        "    # random sample\n",
        "    x = torch.stack([multivariate_normal(self.mean_x[:,n], self.sigma_x).sample() for n in range(self.N)]).t()\n",
        "    mu = multivariate_normal(self.mean_mu.flatten(), self.sigma_mu).sample()\n",
        "    w = torch.stack([multivariate_normal(self.mean_w[i], self.sigma_w).sample() for i in range(self.d)])\n",
        "    alpha = tdist.gamma.Gamma(self.a_alpha_tilde, 1 / self.b_alpha_tilde).sample()     \n",
        "    tau = tdist.gamma.Gamma(self.a_tau_tilde, 1 / self.b_tau_tilde).sample()\n",
        "    \n",
        "    \n",
        "    # priors\n",
        "    # p(x) = N(x|0,I_q)\n",
        "    prior = torch.sum(torch.stack([multivariate_normal(torch.zeros(self.q), \\\n",
        "              torch.eye(self.q)).log_prob(x[:,i]) for i in range(self.N)]))\n",
        "    \n",
        "    # p(w|alpha) = conditional distribution                   \n",
        "    prior += torch.sum(torch.stack([(self.d / 2) * torch.log(alpha[i] / \\\n",
        "              (2 * math.pi)) - 0.5 * alpha[i] * torch.sum(w[:,i]**2) \\\n",
        "              for i in range(self.q)]))                \n",
        "                                 \n",
        "    # p(alpha) = Gamma(a, b)                             \n",
        "    prior += torch.sum((tdist.gamma.Gamma(self.a_alpha, 1/self.b_alpha)).log_prob(alpha))                   \n",
        "                                 \n",
        "    # p(mu) = N(mu|0,Beta^-1I)       \n",
        "    prior += multivariate_normal(torch.zeros(self.d), torch.eye(self.d)/self.beta).log_prob(mu)\n",
        "                    \n",
        "    # p(tau) = Gamma(c, d)      \n",
        "    prior += torch.sum((tdist.gamma.Gamma(self.a_tau, 1/self.b_tau)).log_prob(tau))            \n",
        "    \n",
        "    \n",
        "    # log likelihood of the conditional distribution \n",
        "    # p(t_n | x_n, W, mu, tau)\n",
        "    w_x = torch.mm(w, x)\n",
        "    list_t = []\n",
        "    for n in range(self.N):\n",
        "      list_t.append(w_x[:,n] + mu)\n",
        "    likelihood = torch.sum(torch.stack([multivariate_normal(torch.stack(list_t).t()[:,n], \\\n",
        "                  torch.eye(self.d) / tau).log_prob(self.t_n[:,n]) for n in range(self.N)]))                 \n",
        "\n",
        "    # entropy\n",
        "    # q(x) \n",
        "    entropy = self.N * (0.5 * torch.log(torch.cholesky(self.sigma_x).diag().prod()**2 \\\n",
        "                + (self.d / 2) * (1 + torch.log(torch.Tensor([2 * math.pi])))))   \n",
        "                       \n",
        "    # q(mu)\n",
        "    entropy += 0.5 * torch.log(torch.cholesky(self.sigma_mu).diag().prod()**2 \\\n",
        "                + (self.d / 2) * torch.log(torch.Tensor([2 * math.pi])))\n",
        "                            \n",
        "    # q(W)          \n",
        "    entropy += self.d * (0.5 * torch.log(torch.cholesky(self.sigma_w).diag().prod()**2 \\\n",
        "                + (self.d / 2) * torch.log(torch.Tensor([2 * math.pi]))))  \n",
        "                         \n",
        "    # q(alpha)\n",
        "    entropy += self.q * (torch.log(torch.lgamma(torch.Tensor([self.a_alpha_tilde])).exp()) \\\n",
        "                        - (self.a_alpha_tilde - 1) \\\n",
        "                        * torch.digamma(torch.Tensor([self.a_alpha_tilde])) + self.a_alpha_tilde)\n",
        "    for i in range(self.q):\n",
        "      entropy -= torch.log(self.b_alpha_tilde[i])\n",
        "                         \n",
        "    # q(tau)   \n",
        "    entropy += - (self.a_tau_tilde - 1) * torch.digamma(torch.Tensor([self.a_tau_tilde])) - \\\n",
        "               torch.log(self.b_tau_tilde) + self.a_tau_tilde  \n",
        "    # will ignore torch.log(torch.lgamma(torch.Tensor([self.a_tau_tilde])).exp()) since = inf\n",
        "    \n",
        "    return prior + likelihood - entropy \n",
        "  \n",
        "  def fit(self, t_n, iterations = 1000, threshold = 1.0):\n",
        "    \"\"\"\n",
        "    Fits the data\n",
        "    \n",
        "    Parameters \n",
        "    ----------\n",
        "    t_n : d x N matrix\n",
        "      observed data to be fit\n",
        "      \n",
        "    iterations: int\n",
        "      number of iterations to re-estimate the lower bound\n",
        "    \n",
        "    threshold: float\n",
        "      determines convergence\n",
        "      \n",
        "    \"\"\"\n",
        "    self.t_n = t_n\n",
        "    self.d = self.t_n.shape[0]                     \n",
        "    self.q = self.d - 1\n",
        "    self.N = self.t_n.shape[1]   \n",
        "    \n",
        "    # variational parameters\n",
        "    self.mean_x = torch.randn(self.q, self.N)\n",
        "    self.sigma_x = torch.eye(self.q)\n",
        "    self.mean_mu = torch.randn(self.d, 1)\n",
        "    self.sigma_mu = torch.eye(self.d)\n",
        "    self.mean_w = torch.randn(self.d, self.q)\n",
        "    self.sigma_w = torch.eye(self.q)\n",
        "    self.a_alpha_tilde = self.a_alpha + self.d / 2\n",
        "    self.b_alpha_tilde = torch.abs(torch.randn(self.q))  \n",
        "    self.a_tau_tilde = self.a_tau + self.N * self.d / 2\n",
        "    self.b_tau_tilde = torch.abs(torch.randn(1))\n",
        "             \n",
        "    self.elbos = [self.__get_elbo()]  \n",
        "    for i in range(iterations):\n",
        "      self.__reestimate()\n",
        "      self.elbos.append(self.__get_elbo())\n",
        "      if torch.abs(self.elbos[-2] - self.elbos[-1]) <= threshold:\n",
        "        print('ELBO converged.')\n",
        "        print(\"Iterations: \", i+1)                   \n",
        "        print(\"ELBO: \", int(self.elbos[-1])) \n",
        "        break\n",
        "              \n",
        "      if (i+1) % 100 == 0:\n",
        "        print(\"Iterations: \", i+1)                   \n",
        "        print(\"ELBO: \", int(self.elbos[-1])) \n",
        "        \n",
        "        if i == iterations:\n",
        "          print('Ended without convergence.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJr_9k_-q4bm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def hinton(matrix, max_weight=None, ax=None):\n",
        "    \"\"\"\n",
        "    Draw Hinton diagram for visualizing a weight matrix.\n",
        "    From https://matplotlib.org/3.1.1/gallery/specialty_plots/hinton_demo.html\n",
        "    \n",
        "    \"\"\"\n",
        "    ax = ax if ax is not None else plt.gca()\n",
        "\n",
        "    if not max_weight:\n",
        "        max_weight = 2 ** np.ceil(np.log(np.abs(matrix).max()) / np.log(2))\n",
        "\n",
        "    ax.patch.set_facecolor('gray')\n",
        "    ax.set_aspect('equal', 'box')\n",
        "    ax.xaxis.set_major_locator(plt.NullLocator())\n",
        "    ax.yaxis.set_major_locator(plt.NullLocator())\n",
        "\n",
        "    for (x, y), w in np.ndenumerate(matrix):\n",
        "        color = 'white' if w > 0 else 'black'\n",
        "        size = np.sqrt(np.abs(w) / max_weight)\n",
        "        rect = plt.Rectangle([x - size / 2, y - size / 2], size, size,\n",
        "                             facecolor=color, edgecolor=color)\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "    ax.autoscale_view()\n",
        "    ax.invert_yaxis()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLdhtVGJWuKK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "79d54656-c313-4732-c98d-56d9ee85f336"
      },
      "source": [
        "\"\"\"\n",
        "We generate 100 data points in d = 10 dimensions from a Gaussian distribution \n",
        "having standard deviations of (5, 4, 3, 2) along four orthogonal directions \n",
        "and a standard deviation of 1 in the remaining five directions\n",
        "\"\"\"\n",
        "m = tdist.multivariate_normal.MultivariateNormal(torch.zeros(10), torch.diag(torch.Tensor([5,4,3,2,1,1,1,1,1,1])))\n",
        "X = m.sample(sample_shape=torch.Size([100])).t()\n",
        "\n",
        "\"\"\"\n",
        "Hinton diagram of <W> from variational Bayesian PCA \n",
        "\"\"\"\n",
        "test = BayesianPCA()\n",
        "test.fit(X,1) \n",
        "hinton(test.mean_w)"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-2.1353e+00, -7.0041e-01, -1.8042e+00, -4.6449e+00, -2.0855e+00,\n",
            "          2.4061e-01,  2.3531e+00,  1.0547e+00, -2.5082e+00,  2.8340e-01,\n",
            "         -1.4685e+00,  6.1246e-01, -7.8829e-01, -2.7179e+00, -1.8902e+00,\n",
            "          3.6274e-01, -3.3849e+00,  5.0054e-01,  3.6157e+00, -2.1100e+00,\n",
            "         -2.1670e-01,  1.7288e+00,  3.4352e+00,  1.0332e+00, -7.9964e-02,\n",
            "          2.5562e+00,  2.4328e+00, -1.1508e+00,  1.1279e+00, -1.8129e-01,\n",
            "          6.0997e-01,  9.7176e-01,  7.9352e-01, -7.9286e-01, -1.8781e+00,\n",
            "         -4.5199e-01,  7.3514e-01, -4.5349e+00,  5.4989e-01,  1.0770e+00,\n",
            "         -2.5894e+00,  3.7157e+00,  3.9264e+00,  3.2652e-01, -2.2121e+00,\n",
            "         -1.6374e+00, -2.2648e+00, -7.0610e-01,  1.0082e+00, -6.1526e-01,\n",
            "         -1.8178e+00, -5.3216e-01,  1.3734e+00,  3.2996e-01,  1.1243e+00,\n",
            "          1.2932e+00,  1.8312e+00,  3.9696e+00, -5.1162e+00,  2.0104e+00,\n",
            "          5.6794e-01,  5.7152e-01, -3.3005e+00, -2.6957e+00, -6.1891e-01,\n",
            "          6.4333e-01, -1.9379e+00,  3.1625e-01, -8.3742e-01, -1.8909e+00,\n",
            "         -6.6346e-01, -2.7906e+00, -2.1550e+00, -3.8620e+00, -3.2714e+00,\n",
            "          1.8834e+00,  7.0330e-01, -1.8342e-01, -2.1179e-01, -3.7644e+00,\n",
            "          3.1758e-01,  3.6157e+00, -2.7851e+00, -4.4445e+00, -1.0894e+00,\n",
            "          4.1807e+00,  1.9109e+00,  3.8994e+00,  5.2977e-01, -3.5585e+00,\n",
            "         -3.3064e+00,  5.0107e+00,  1.4191e+00,  1.8577e+00,  5.5527e-01,\n",
            "          2.8173e-01,  1.5678e+00,  1.5121e+00, -1.8429e-01,  2.4662e+00],\n",
            "        [-3.1094e+00, -4.9389e-01,  1.7849e+00, -5.1096e+00,  6.6788e-01,\n",
            "          5.1277e+00, -1.6930e+00,  4.1830e-01,  1.0868e+00,  1.6074e+00,\n",
            "          1.2394e-01,  2.7128e+00, -4.3730e-01, -1.0734e-01, -3.0270e-02,\n",
            "         -1.1554e+00, -2.5841e+00,  1.5383e+00, -1.3243e+00, -1.3486e+00,\n",
            "          8.2777e-01, -5.0340e-01, -2.2283e+00,  1.8834e+00, -1.3757e+00,\n",
            "         -2.3270e+00, -1.7394e+00,  1.0625e+00, -6.9453e-01, -2.0586e+00,\n",
            "          1.4568e+00,  3.0523e-01,  4.8131e+00, -9.3642e-01,  8.1536e-01,\n",
            "         -1.0074e+00,  1.4278e+00, -1.0481e+00,  7.3346e-01,  2.0452e+00,\n",
            "          1.8834e+00, -3.2153e+00,  1.8907e+00, -3.0015e+00, -2.4171e+00,\n",
            "          1.0930e+00, -6.6297e-01,  3.0967e+00,  3.6824e-01,  3.2573e+00,\n",
            "          1.6054e+00,  1.7265e+00,  2.4911e+00, -2.4765e+00,  1.4754e+00,\n",
            "         -1.3092e+00, -1.8614e+00,  2.0922e+00,  9.4538e-02,  2.2247e+00,\n",
            "         -7.7541e-01,  3.5640e+00, -1.1603e+00, -1.6233e+00,  9.5978e-01,\n",
            "          1.8051e+00, -1.1528e+00, -8.0404e-01,  8.6721e-01,  2.9592e+00,\n",
            "         -2.8288e+00, -7.8545e-01, -4.7571e+00,  3.9931e+00, -8.2610e-01,\n",
            "         -8.6137e-01,  2.5711e+00,  1.7840e+00,  3.1772e+00,  4.1625e+00,\n",
            "         -1.7566e-01, -2.4899e+00, -5.1180e+00,  1.7455e+00,  2.4204e+00,\n",
            "          5.8098e-01, -2.7154e+00,  1.4825e-01, -5.2038e-03, -2.1593e+00,\n",
            "          1.3807e+00,  1.4729e+00, -4.5097e+00, -1.9985e+00, -1.4269e+00,\n",
            "          1.2856e+00, -4.7177e-01,  7.4003e-02,  8.7641e-01, -1.9861e+00],\n",
            "        [-1.6631e+00, -6.5523e-01, -1.1910e+00, -2.4270e+00,  1.6510e+00,\n",
            "         -5.2552e-01,  1.1919e+00, -1.3874e+00, -1.1460e+00, -1.3704e+00,\n",
            "          5.0099e-01, -1.1429e+00,  2.3272e+00,  2.2252e+00,  1.8766e-01,\n",
            "         -3.2427e+00,  3.2240e-01, -1.5380e+00,  1.1187e+00, -5.0386e-01,\n",
            "         -4.9174e+00,  7.7127e-01, -8.5998e-01,  2.1539e+00,  4.6167e-01,\n",
            "          1.4609e+00, -1.5071e+00,  2.8796e+00, -6.4312e-01, -1.8248e+00,\n",
            "         -2.0333e-01,  3.5942e+00, -1.2662e+00,  4.1008e-01,  9.8263e-01,\n",
            "         -1.2456e+00,  1.6939e+00, -2.3636e+00, -3.5889e+00,  1.5408e+00,\n",
            "          4.8520e-01, -6.9073e-01, -2.6073e-01,  3.4757e-01,  1.4968e+00,\n",
            "         -3.0528e+00,  9.6468e-02,  2.1152e+00, -4.6763e+00,  2.3019e+00,\n",
            "          2.8012e+00,  5.4465e-01, -6.7228e-01,  4.9034e-01, -8.4501e-01,\n",
            "         -6.7032e-01,  1.7729e+00, -8.7921e-01,  1.9732e-01,  1.6714e+00,\n",
            "          1.0719e+00, -3.5434e-01,  8.1301e-01, -2.1418e+00, -7.2294e-01,\n",
            "         -7.1425e-01, -2.2115e+00,  4.9584e+00, -3.6412e-01,  4.6339e-01,\n",
            "          4.5081e-01, -2.6294e+00, -2.9097e-01,  1.9832e+00,  3.5377e+00,\n",
            "         -1.1124e-01,  7.0716e-01,  2.3171e+00, -1.3288e+00, -1.0322e+00,\n",
            "         -1.1459e+00,  2.4478e+00, -7.3492e-01, -1.2632e+00, -7.7597e-01,\n",
            "          1.9101e+00, -1.4174e-01, -1.5365e+00,  1.4600e+00, -3.4480e+00,\n",
            "         -1.4204e-01, -4.5407e-01, -9.3046e-02,  2.8501e-01,  8.3169e-01,\n",
            "          1.4466e+00, -7.1273e-01, -1.1131e+00, -1.5104e+00, -3.1554e-01],\n",
            "        [ 1.1948e+00, -2.4220e-01,  1.0978e+00, -2.2345e-01,  1.1281e+00,\n",
            "          8.2354e-01, -1.5029e+00, -1.3578e+00, -7.5742e-01, -2.0278e-01,\n",
            "         -1.0543e+00,  6.4595e-01,  3.1333e-01, -1.0474e+00,  7.7749e-01,\n",
            "         -1.0869e+00, -2.1452e+00,  1.0312e+00, -7.0556e-01,  4.5237e-01,\n",
            "          1.5624e+00,  1.1760e-01, -6.1864e-01, -5.5030e-02,  8.8980e-01,\n",
            "         -8.0498e-02,  8.7102e-01, -1.8493e+00,  9.6650e-01, -2.4044e+00,\n",
            "          1.3221e-01, -1.7694e-01, -1.8790e+00,  1.4072e+00,  6.6312e-01,\n",
            "          2.6285e+00, -9.6843e-01, -7.7557e-01, -1.3477e+00,  8.7230e-01,\n",
            "          1.2014e+00, -1.5281e+00,  1.6513e-02,  1.3581e+00, -1.3501e+00,\n",
            "         -7.0292e-01,  1.8751e+00, -2.2902e+00,  2.2466e+00,  1.0422e+00,\n",
            "          1.4163e+00, -1.9870e+00,  1.2115e-01, -1.3184e+00,  8.3220e-01,\n",
            "         -2.2100e+00,  3.1757e+00,  1.6867e+00, -6.4671e-01, -8.8855e-01,\n",
            "          6.7326e-02,  2.6653e-01,  4.5877e-01,  3.1380e-01, -3.9226e-01,\n",
            "         -6.7938e-01, -1.3313e+00,  6.8720e-01,  1.1897e+00,  2.7450e-01,\n",
            "         -1.9285e-01,  9.1050e-01,  1.3669e+00,  1.2723e+00, -1.0143e-01,\n",
            "         -1.1192e+00,  2.5847e-01,  8.2413e-01,  5.0613e-01, -8.2165e-01,\n",
            "         -1.1775e+00, -1.9445e-01,  2.3177e-01, -2.4155e+00,  1.9427e+00,\n",
            "          1.7630e+00, -1.0375e-01, -2.1904e+00,  5.7278e-02,  2.2565e-01,\n",
            "          1.0837e+00,  3.1220e-02, -1.0559e+00, -2.5177e+00, -2.1494e-01,\n",
            "         -5.6658e-01,  1.9677e+00, -5.3200e-02, -6.5192e-01, -4.7626e-01],\n",
            "        [-9.2230e-01, -1.4027e+00, -1.0734e+00, -5.6503e-01, -8.2208e-01,\n",
            "         -2.1412e-01,  7.7101e-01, -1.0378e+00, -1.8750e-01, -2.0609e+00,\n",
            "         -1.2685e+00,  2.8441e-01,  2.8492e-01, -4.7218e-01,  6.9677e-01,\n",
            "         -2.6225e-01, -1.2211e+00, -8.5410e-01, -6.9203e-01,  6.9518e-01,\n",
            "         -1.5904e-01, -1.0296e-01,  4.1672e-01, -2.7516e-01, -1.4947e+00,\n",
            "         -6.2255e-02,  2.3378e-02, -3.7244e-01, -5.9798e-01,  1.0592e-02,\n",
            "         -5.0316e-01,  1.2263e+00,  6.6068e-02, -2.2869e-01, -5.4195e-01,\n",
            "          4.6025e-01, -2.6820e-01,  1.9925e-01,  5.0636e-01, -8.3792e-01,\n",
            "          1.8404e+00, -1.2784e+00,  1.4430e+00,  1.3092e-01,  1.6043e+00,\n",
            "          1.2209e+00, -1.9122e+00,  2.3588e-01, -3.9109e-01, -3.2528e-01,\n",
            "          9.9530e-01,  1.5914e-01, -5.6784e-01,  1.1386e+00,  2.1305e-01,\n",
            "         -3.6938e-02,  1.4042e+00, -1.9212e+00, -1.8472e-01, -8.4832e-01,\n",
            "          2.8886e-01, -1.3874e-01, -2.2349e+00, -4.5186e-02,  1.5022e+00,\n",
            "          1.0423e+00,  1.8051e+00, -8.4686e-01, -2.1469e+00,  9.1120e-01,\n",
            "          5.3141e-02, -1.9494e+00, -9.4629e-01,  6.2731e-01, -8.9695e-01,\n",
            "          9.8312e-01,  3.1024e-01,  9.5963e-01,  5.6041e-01, -1.1977e+00,\n",
            "         -2.7433e-01,  4.5286e-01,  5.9666e-01,  1.2946e+00, -2.2921e-01,\n",
            "         -2.9095e+00,  5.2288e-01, -1.1630e+00,  1.7372e-01, -6.8110e-01,\n",
            "          7.4900e-02,  1.0491e+00, -2.9461e-01, -5.5767e-01,  8.5914e-01,\n",
            "         -1.4967e+00,  5.4626e-01,  8.0326e-02, -8.1862e-02, -1.7852e+00],\n",
            "        [-1.3350e-01,  2.3097e-01, -2.0831e-01,  3.0250e-01, -1.3977e+00,\n",
            "         -1.0961e+00, -7.8443e-01,  2.9463e-01,  5.1084e-01,  1.0151e-01,\n",
            "         -1.4661e+00, -5.3229e-01,  7.2765e-01, -6.3975e-01,  9.2357e-01,\n",
            "         -1.1181e+00, -7.8831e-01,  3.5364e-01,  1.1408e+00, -2.1867e+00,\n",
            "          7.8531e-01, -1.0991e+00, -5.6642e-01,  1.0065e+00, -8.3578e-02,\n",
            "          6.2910e-01,  2.5559e+00,  5.4234e-01,  6.1603e-01,  3.0391e-01,\n",
            "          1.4646e-01,  1.0083e+00,  4.8295e-01, -6.0935e-01,  1.3806e+00,\n",
            "          8.8558e-01,  2.0504e+00,  9.6541e-01, -7.6930e-01, -5.7037e-01,\n",
            "         -6.1208e-04, -1.5079e+00, -5.5643e-01, -2.9449e-01,  1.2342e+00,\n",
            "          1.5951e+00,  5.2634e-01,  1.7130e+00, -1.5401e+00, -2.6840e-01,\n",
            "         -2.5769e-01, -6.5319e-01,  9.5660e-02,  8.3024e-01,  7.8505e-01,\n",
            "          1.4447e+00, -8.9452e-01, -1.7991e+00,  9.8653e-01, -1.0319e+00,\n",
            "          9.4493e-02,  3.0970e-02,  2.7498e-01,  8.3998e-01,  2.1696e+00,\n",
            "          2.5344e+00,  3.2212e-01,  1.4235e+00, -1.3620e-01,  8.2267e-01,\n",
            "          3.2744e-01, -9.1898e-01, -1.3923e+00, -5.5640e-01, -1.5292e+00,\n",
            "         -1.1846e+00,  6.1301e-02,  8.6458e-01,  3.2279e-01, -2.6033e+00,\n",
            "          5.5766e-02, -1.0809e+00, -1.0164e+00, -4.5516e-01, -5.9130e-01,\n",
            "          2.8588e-01,  3.9773e-01,  8.7681e-01, -7.1835e-02, -5.9287e-01,\n",
            "         -9.3737e-01,  1.7328e+00, -2.7579e-01,  8.9934e-01, -2.9793e-01,\n",
            "          4.1888e-01, -8.2564e-01,  8.8366e-01, -6.9456e-01, -2.9593e-01],\n",
            "        [ 6.5850e-01, -2.4706e-01, -6.4021e-01,  1.7178e+00, -7.6164e-02,\n",
            "          1.3863e+00, -1.9333e+00,  1.2709e+00, -7.9069e-01, -5.0638e-01,\n",
            "          4.5145e-01, -4.5834e-01,  8.0394e-01,  2.1539e-01, -9.0358e-01,\n",
            "          7.9194e-01,  9.2065e-01, -3.7719e-01, -1.8495e-01, -1.0782e+00,\n",
            "          2.1120e+00, -1.5639e+00,  9.3506e-02, -5.3270e-01, -1.4192e-01,\n",
            "         -1.7528e+00,  2.0809e+00, -1.2435e+00, -6.3760e-01,  7.9489e-02,\n",
            "          2.2083e-01, -3.4632e-01, -2.5154e+00, -7.4192e-01, -5.5952e-01,\n",
            "         -2.0761e+00,  3.6234e-01,  2.4631e+00,  4.7058e-02, -1.7612e-01,\n",
            "          1.6905e+00, -1.0068e+00, -9.9058e-01, -1.1571e+00, -4.5403e-01,\n",
            "         -2.6102e-01,  1.0791e+00, -6.7229e-01,  9.6479e-01,  1.5484e-01,\n",
            "         -1.1818e+00,  9.5259e-01, -6.6874e-01, -2.2485e-01, -8.4889e-01,\n",
            "         -2.0307e-01, -4.2702e-01, -3.9975e-01, -2.2284e+00,  1.6894e+00,\n",
            "         -5.0824e-01,  1.0058e+00,  1.2656e+00,  1.6046e+00, -4.7181e-01,\n",
            "          1.0957e+00,  4.2050e-01,  4.9364e-01,  3.0054e-01, -7.0128e-01,\n",
            "         -9.8643e-01, -8.7780e-01, -2.6768e+00,  6.4218e-01, -2.1648e+00,\n",
            "          4.8719e-01, -1.6502e+00, -1.2212e+00,  1.2750e+00, -1.0049e-01,\n",
            "          1.7297e+00,  6.6873e-02,  8.6938e-01, -3.6050e-01,  5.8146e-01,\n",
            "         -7.7825e-01,  1.2324e+00, -1.1031e+00,  5.9506e-01,  7.3515e-01,\n",
            "          4.0140e-02, -7.5998e-03,  8.6333e-01, -5.5535e-01,  2.9288e-01,\n",
            "         -6.6857e-01,  1.3951e+00,  8.9457e-01,  9.2692e-02, -1.3053e+00],\n",
            "        [-9.4761e-01, -4.7284e-01,  8.0681e-01,  6.2671e-01,  3.2029e-01,\n",
            "          2.9374e-01,  1.3340e-01,  1.4436e+00,  1.2804e+00, -2.2140e+00,\n",
            "         -1.0314e+00,  1.4752e+00, -2.5296e-01, -2.3324e-01,  1.9357e-01,\n",
            "          7.8161e-02, -1.5222e+00, -7.3477e-01,  1.8006e+00, -8.7115e-01,\n",
            "         -8.0812e-01,  2.4629e+00, -7.1760e-01, -1.4808e+00, -1.8312e+00,\n",
            "          1.0395e+00,  1.8635e-01,  1.4406e+00,  1.0526e+00, -1.2608e-01,\n",
            "          1.0621e+00, -2.1086e+00, -1.1855e-01,  7.1999e-01, -6.9562e-01,\n",
            "          2.4152e-01,  5.6957e-01, -1.9636e-01, -7.9154e-01, -1.8825e+00,\n",
            "         -1.4181e+00,  2.9035e-01, -1.9716e+00,  1.5803e+00, -1.5497e+00,\n",
            "          7.0982e-01,  5.1382e-01,  5.8089e-01, -4.1093e-01,  3.7754e-01,\n",
            "         -1.5289e-01, -7.1758e-01, -2.6279e-01, -1.7905e-01,  1.9230e-01,\n",
            "          3.5152e-01,  4.3574e-01, -1.0777e+00,  1.9470e-01, -8.2819e-01,\n",
            "          1.6508e-01,  4.5652e-01,  1.5272e+00, -1.0961e-01,  2.7450e-01,\n",
            "          2.4875e+00, -6.5545e-01, -1.3533e+00,  8.6465e-01,  4.2166e-01,\n",
            "         -2.9254e-01, -1.2921e-01,  6.9748e-01,  6.5372e-02,  6.7220e-01,\n",
            "         -8.9107e-01,  9.9862e-01,  1.9048e-01, -6.1523e-02, -6.9586e-01,\n",
            "         -2.1543e-01, -3.3161e-01, -1.7089e-01,  1.7563e+00,  1.0633e-01,\n",
            "          3.0485e-01,  1.1975e+00,  6.6825e-01,  1.0417e+00, -2.9881e-01,\n",
            "          3.1548e-02,  7.1806e-02,  1.5938e-01,  5.1362e-01, -3.0352e-01,\n",
            "          2.2581e-01, -6.2610e-01,  1.2811e+00, -1.3330e-01, -2.1217e+00],\n",
            "        [ 9.6005e-01,  1.8424e+00,  1.9576e+00, -1.5692e+00, -5.0315e-01,\n",
            "         -9.3990e-01, -4.1541e-01,  2.7675e-01, -1.0298e+00,  1.3751e+00,\n",
            "         -1.1032e+00,  2.4506e-01,  1.2422e+00,  2.6614e-01, -7.2406e-01,\n",
            "          1.1186e+00, -2.6855e-01,  1.8161e+00, -2.0090e+00, -1.4604e+00,\n",
            "          5.6375e-01, -4.6800e-01,  1.1329e+00, -1.1008e+00,  8.4299e-02,\n",
            "         -1.4177e+00, -6.2525e-01, -1.5704e+00, -2.4072e-01, -1.8516e+00,\n",
            "          6.0565e-01, -6.0329e-01, -5.4151e-01,  2.2250e-01, -1.2990e+00,\n",
            "          1.4095e+00, -8.6155e-01,  9.5514e-01,  2.0579e+00,  5.3696e-01,\n",
            "         -1.2171e+00, -1.3244e+00, -4.3961e-01,  3.6518e-01, -2.2955e-02,\n",
            "         -4.2024e-01,  1.2860e+00,  3.5220e-01, -9.3993e-01,  6.3999e-01,\n",
            "          1.1775e+00, -9.9774e-01,  6.7476e-01, -1.3235e+00,  6.0607e-01,\n",
            "         -2.6728e-01, -3.8164e-01, -7.1004e-01,  7.2373e-01,  1.3577e+00,\n",
            "         -1.5344e+00,  6.0884e-01, -7.1317e-01,  1.0366e+00, -9.7084e-02,\n",
            "          2.6497e-01, -5.0814e-01,  8.4044e-01, -1.3026e+00,  7.1321e-01,\n",
            "          1.2105e+00, -8.7011e-01,  3.5890e-01, -4.9345e-01,  5.4849e-01,\n",
            "         -1.1315e+00,  3.7548e-01, -5.0528e-01, -1.3676e+00, -1.2057e+00,\n",
            "          7.5231e-01, -1.6964e+00,  1.2002e+00, -2.2241e+00,  5.8870e-01,\n",
            "         -1.6014e-01,  2.4625e-01,  3.2551e-01,  2.6026e+00,  1.0503e+00,\n",
            "         -6.1698e-01, -8.4360e-01,  6.2836e-01, -3.2291e-01,  1.8653e-01,\n",
            "         -1.1659e+00, -9.0815e-01, -1.5452e-02, -2.1752e-01,  2.9425e-01],\n",
            "        [ 2.6864e-01,  5.4101e-01,  1.6969e-01, -7.2337e-02,  1.6100e-01,\n",
            "          1.5761e+00,  6.6360e-01,  3.1923e-01,  6.8950e-04,  1.7933e+00,\n",
            "         -1.1512e+00,  1.6302e+00,  1.6044e-01,  9.3291e-01, -6.0210e-01,\n",
            "         -1.4854e+00,  1.6274e-02,  9.2057e-01, -1.5073e+00, -1.6232e-01,\n",
            "          7.9982e-01, -7.7731e-01,  7.7364e-01, -3.6370e-01, -7.4039e-01,\n",
            "         -7.6303e-01,  1.1574e-01, -5.4919e-01,  7.5476e-01,  4.3102e-01,\n",
            "         -6.8204e-01, -2.8255e+00, -7.6269e-01,  1.3828e-01, -1.1131e+00,\n",
            "         -1.0325e+00,  7.1339e-01,  4.1690e-01, -5.6367e-01, -1.7335e-01,\n",
            "         -8.7557e-01, -8.7555e-03, -1.3080e+00, -6.7253e-01, -4.3311e-02,\n",
            "         -2.2667e-01,  1.1886e+00, -9.5956e-01,  8.1653e-01, -6.7524e-01,\n",
            "         -1.2485e+00, -1.4585e+00,  2.9873e-01, -6.7984e-01,  1.6740e+00,\n",
            "          6.3217e-02, -6.1410e-01,  1.7523e+00,  1.0345e-01, -2.8998e-01,\n",
            "          9.8193e-01,  1.8321e+00, -7.7999e-02,  5.0758e-01,  7.2518e-01,\n",
            "          1.6215e-01, -1.5573e+00, -1.0290e+00,  7.9839e-01,  2.3912e-01,\n",
            "          8.1571e-01, -2.2025e-01, -7.3276e-01, -1.5467e+00, -2.0795e-02,\n",
            "         -2.8686e+00,  3.1522e-01,  1.5367e+00,  9.5384e-01,  1.2010e+00,\n",
            "          3.8523e-03,  1.8245e-01,  1.2803e+00, -1.1261e+00,  5.8601e-01,\n",
            "         -2.7868e-01, -1.0637e+00, -4.1532e-02,  9.6815e-01,  9.3465e-01,\n",
            "         -4.8379e-01,  9.1708e-01,  1.3013e+00,  7.5802e-01,  3.7459e-01,\n",
            "         -1.4870e+00,  6.4245e-01,  3.3003e-01, -1.0707e-01, -8.0273e-01]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAADuCAYAAAAwYhJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngVAAAqEABEIBCIQCEPwX6SMdTGtszjgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECVElK21aTXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}