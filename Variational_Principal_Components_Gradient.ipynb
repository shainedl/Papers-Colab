{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Variational_Principal_Components-Gradient.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shainedl/Papers-Colab/blob/master/Variational_Principal_Components_Gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EcQh6hjN224",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch, math\n",
        "import torch.distributions as tdist\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal as multivariate_normal"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbPYGX67HLZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BayesianPCA():\n",
        "  \n",
        "  def __init__(self, a_alpha=10e-3, b_alpha=10e-3, a_tau=10e-3, b_tau=10e-3, beta=10e-3):\n",
        "    \n",
        "    # hyperparameters\n",
        "    self.a_alpha = a_alpha\n",
        "    self.b_alpha = b_alpha\n",
        "    self.a_tau = a_tau\n",
        "    self.b_tau = b_tau\n",
        "    self.beta = beta \n",
        "    \n",
        "  def __get_elbo(self):\n",
        "    \"\"\"\n",
        "    Computes the rigorous lower bound on the true log marginal likelihood \n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "      float\n",
        "        the lower bound = prior + likelihood - entropy \n",
        "    \n",
        "    \"\"\"                             \n",
        "    # random sample\n",
        "    x = torch.stack([self.__reparameterize_n(self.mean_x[:,n], self.sigma_x) for n in range(self.N)]).t()\n",
        "    mu = self.__reparameterize_n(self.mean_mu.flatten(), self.sigma_mu)\n",
        "    w = torch.stack([self.__reparameterize_n(self.mean_w[i,:], self.sigma_w) for i in range(self.d)])\n",
        "    alpha = self.__reparameterize_g(self.a_alpha_tilde, self.b_alpha_tilde)\n",
        "    tau = self.__reparameterize_g(self.a_tau_tilde, self.b_tau_tilde)\n",
        "\n",
        "    # priors\n",
        "    # p(x) = N(x|0,I_q)\n",
        "    prior = torch.sum(torch.stack([multivariate_normal(torch.zeros(self.q, dtype=torch.float64), \\\n",
        "              torch.eye(self.q, dtype=torch.float64)).log_prob(x[:,i]) for i in range(self.N)]))\n",
        "    # p(w|alpha) = conditional distribution                   \n",
        "    prior += torch.sum(torch.stack([(self.d / 2) * torch.log(alpha[i] / \\\n",
        "              (2 * math.pi)) - 0.5 * alpha[i] * torch.sum(w[:,i]**2) \\\n",
        "              for i in range(self.q)]))                                      \n",
        "    # p(alpha) = Gamma(a, b)                             \n",
        "    prior += torch.sum((tdist.gamma.Gamma(self.a_alpha, self.b_alpha)).log_prob(alpha))                                  \n",
        "    # p(mu) = N(mu|0,Beta^-1I)       \n",
        "    prior += multivariate_normal(torch.zeros(self.d, dtype=torch.float64), torch.eye(self.d, dtype=torch.float64)/self.beta).log_prob(mu) \n",
        "    # p(tau) = Gamma(c, d)      \n",
        "    prior += torch.sum((tdist.gamma.Gamma(self.a_tau, self.b_tau)).log_prob(tau))            \n",
        "    \n",
        "    # log likelihood of the conditional distribution \n",
        "    # p(t_n | x_n, W, mu, tau)\n",
        "    w_x = torch.mm(w, x)\n",
        "    list_t = []\n",
        "    for n in range(self.N):\n",
        "      list_t.append(w_x[:,n] + mu)\n",
        "    likelihood = torch.sum(torch.stack([multivariate_normal(torch.stack(list_t).t()[:,n], \\\n",
        "                  torch.eye(self.d, dtype=torch.float64) / tau).log_prob(self.t_n[:,n]) for n in range(self.N)]))   \n",
        "    \n",
        "    # entropy\n",
        "    # q(x) \\\n",
        "    entropy = self.N * 0.5 * torch.logdet(self.sigma_x) \\\n",
        "                + torch.log((self.d / 2) * (1 + torch.log(torch.DoubleTensor([2 * math.pi]))))\n",
        "              \n",
        "    # q(mu)\n",
        "    entropy += 0.5 * torch.logdet(self.sigma_mu) \\\n",
        "                + torch.log((self.d / 2) * torch.log(torch.DoubleTensor([2 * math.pi])))\n",
        "         \n",
        "    # q(W)   \n",
        "    entropy += self.d * 0.5 * torch.logdet(self.sigma_w) \\\n",
        "                + torch.log((self.d / 2) * torch.log(torch.DoubleTensor([2 * math.pi])))  \n",
        "            \n",
        "    # q(alpha)\n",
        "    entropy += self.q * (torch.log(torch.lgamma(torch.DoubleTensor([self.a_alpha_tilde])).exp()) \\\n",
        "                        - (self.a_alpha_tilde - 1) \\\n",
        "                        * torch.digamma(torch.DoubleTensor([self.a_alpha_tilde])) + self.a_alpha_tilde)\n",
        "    for i in range(self.q):\n",
        "      entropy -= torch.log(self.b_alpha_tilde[i])\n",
        "\n",
        "    # q(tau)  \n",
        "    entropy += -1*(torch.DoubleTensor([self.a_tau_tilde - 1]) * torch.digamma(torch.DoubleTensor([self.a_tau_tilde])) \\\n",
        "               - torch.log(torch.DoubleTensor([self.b_tau_tilde])) + torch.DoubleTensor([self.a_tau_tilde]))\n",
        "    # will ignore torch.log(torch.lgamma(torch.Tensor([self.a_tau_tilde])).exp()) since = inf\n",
        "\n",
        "    return prior + likelihood - entropy \n",
        "  \n",
        "  def __reparameterize_n(self, mean, sigma):\n",
        "    eps = torch.randn_like(sigma[0])\n",
        "    return mean + eps*torch.diag(sigma)\n",
        "  \n",
        "  def __reparameterize_g(self, a, b):\n",
        "    \"\"\"\n",
        "    https://www.hongliangjie.com/2012/12/19/how-to-generate-gamma-random-variables/\n",
        "    \"\"\"\n",
        "    if a > 1:\n",
        "      d = a - 1/3\n",
        "      c = 1/math.sqrt(9 * d)\n",
        "      flag = 1\n",
        "      while flag:\n",
        "        Z = torch.randn(1)\n",
        "        if Z > -1/c:\n",
        "          V = (1 + c * Z) ** 3\n",
        "          U = torch.rand(1)\n",
        "          flag = torch.log(U) > (0.5 * Z**2 + d - d * V + d * torch.log(V))\n",
        "      x = d * V / b\n",
        "    else:\n",
        "      x = tdist.gamma.Gamma(a + 1, b).sample() \n",
        "      x = x * torch.rand(1)**(1/a)\n",
        "    return x\n",
        "  \n",
        "  def fit(self, t_n, epochs = 3):\n",
        "    \"\"\"\n",
        "    Fits the data\n",
        "    \n",
        "    Parameters \n",
        "    ----------\n",
        "    t_n : d x N matrix\n",
        "      observed data to be fit\n",
        "      \n",
        "    iterations: int\n",
        "      number of iterations to re-estimate the lower bound\n",
        "    \n",
        "    threshold: float\n",
        "      determines convergence\n",
        "      \n",
        "    \"\"\"\n",
        "    self.t_n = t_n\n",
        "    self.d = self.t_n.shape[0]                     \n",
        "    self.q = self.d - 1\n",
        "    self.N = self.t_n.shape[1]   \n",
        "    \n",
        "    # variational parameters\n",
        "    self.mean_x = torch.randn(self.q, self.N, dtype=torch.float64)\n",
        "    self.sigma_x = torch.eye(self.q, dtype=torch.float64)\n",
        "    self.mean_mu = torch.randn(self.d, 1, dtype=torch.float64)\n",
        "    self.sigma_mu = torch.eye(self.d, dtype=torch.float64)\n",
        "    self.mean_w = torch.randn(self.d, self.q, dtype=torch.float64)\n",
        "    self.sigma_w = torch.eye(self.q, dtype=torch.float64)\n",
        "    self.a_alpha_tilde = torch.Tensor([self.a_alpha + self.d / 2])\n",
        "    self.b_alpha_tilde = torch.abs(torch.randn(self.q, dtype=torch.float64))\n",
        "    self.a_tau_tilde = torch.Tensor([self.a_tau + self.N * self.d / 2])\n",
        "    self.b_tau_tilde = torch.abs(torch.randn(1, dtype=torch.float64))\n",
        "    self.mean_x.requires_grad_()\n",
        "    self.sigma_x.requires_grad_()\n",
        "    self.mean_mu.requires_grad_()\n",
        "    self.sigma_mu.requires_grad_()\n",
        "    self.mean_w.requires_grad_()\n",
        "    self.sigma_w.requires_grad_()\n",
        "    self.a_alpha_tilde.requires_grad_()\n",
        "    self.b_alpha_tilde.requires_grad_()\n",
        "    self.a_tau_tilde.requires_grad_()\n",
        "    self.b_tau_tilde.requires_grad_()\n",
        "    \n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "      lr = 0.01\n",
        "      elbo = self.__get_elbo()\n",
        "      loss = -elbo\n",
        "      print(loss)\n",
        "      loss.backward()\n",
        "      with torch.no_grad():\n",
        "        self.mean_x -= self.mean_x.grad * lr\n",
        "        self.mean_x.grad.zero_()\n",
        "        self.sigma_x -= self.sigma_x.grad * lr\n",
        "        self.sigma_x.grad.zero_()\n",
        "        self.mean_mu -= self.mean_mu.grad * lr\n",
        "        self.mean_mu.grad.zero_()\n",
        "        self.sigma_mu -= self.sigma_mu.grad * lr\n",
        "        self.sigma_mu.grad.zero_()\n",
        "        self.mean_w -= self.mean_w.grad * lr\n",
        "        self.mean_w.grad.zero_()\n",
        "        self.sigma_w -= self.sigma_w.grad * lr\n",
        "        self.sigma_w.grad.zero_()\n",
        "        self.a_alpha_tilde -= self.a_alpha_tilde.grad * lr\n",
        "        self.a_alpha_tilde.grad.zero_()\n",
        "        self.b_alpha_tilde -= self.b_alpha_tilde.grad * lr\n",
        "        self.b_alpha_tilde.grad.zero_()\n",
        "        self.a_tau_tilde -= self.a_tau_tilde.grad * lr\n",
        "        self.a_tau_tilde.grad.zero_()\n",
        "        self.b_tau_tilde -= self.b_tau_tilde.grad * lr\n",
        "        self.b_tau_tilde.grad.zero_()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cjMFyIcNuUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def hinton(matrix, max_weight=None, ax=None):\n",
        "    \"\"\"\n",
        "    Draw Hinton diagram for visualizing a weight matrix.\n",
        "    From https://matplotlib.org/3.1.1/gallery/specialty_plots/hinton_demo.html\n",
        "    \n",
        "    \"\"\"\n",
        "    ax = ax if ax is not None else plt.gca()\n",
        "\n",
        "    if not max_weight:\n",
        "        max_weight = 2 ** np.ceil(np.log(np.abs(matrix).max()) / np.log(2))\n",
        "\n",
        "    ax.patch.set_facecolor('gray')\n",
        "    ax.set_aspect('equal', 'box')\n",
        "    ax.xaxis.set_major_locator(plt.NullLocator())\n",
        "    ax.yaxis.set_major_locator(plt.NullLocator())\n",
        "\n",
        "    for (x, y), w in np.ndenumerate(matrix):\n",
        "        color = 'white' if w > 0 else 'black'\n",
        "        size = np.sqrt(np.abs(w) / max_weight)\n",
        "        rect = plt.Rectangle([x - size / 2, y - size / 2], size, size,\n",
        "                             facecolor=color, edgecolor=color)\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "    ax.autoscale_view()\n",
        "    ax.invert_yaxis()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjFWwu5LNzHb",
        "colab_type": "code",
        "outputId": "7a4a8f05-fefd-4420-eda1-7e27e46cd1e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "\"\"\"\n",
        "We generate 100 data points in d = 10 dimensions from a Gaussian distribution \n",
        "having standard deviations of (5, 4, 3, 2) along four orthogonal directions \n",
        "and a standard deviation of 1 in the remaining five directions\n",
        "\"\"\"\n",
        "m = tdist.multivariate_normal.MultivariateNormal(torch.zeros(10, dtype=torch.float64), torch.diag(torch.DoubleTensor([5,4,3,2,1,1,1,1,1,1])))\n",
        "X = m.sample(sample_shape=torch.Size([100])).t()\n",
        "\n",
        "\"\"\"\n",
        "Hinton diagram of <W> from variational Bayesian PCA \n",
        "\"\"\"\n",
        "test = BayesianPCA()\n",
        "test.fit(X) \n",
        "hinton(test.mean_w.t().detach().numpy())"
      ],
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([7382352.7582], dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "tensor([nan], dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "tensor([nan], dtype=torch.float64, grad_fn=<NegBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAANUAAADuCAYAAACu/ULtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAACdlJREFUeJzt3TFy20gQheHR1kamAyVOnOwlNmfp\nNDqAyrlyls+k0gH2EI4c7IZyjA1csGAUSADEe2Q3+/8ir8lFASLezGDEbt91XdcA6Pxx7RMAbg2h\nAsQIFSBGqAAxQgWIESpAjFABYoQKECNUgNifa9784cOH7v7+3nUuQGjfv3//r+u6T3PvWxWq+/v7\n9vj4eP5ZAYk9Pz9/W/I+ln+AGKECxAgVIEaoADFCBYgRKkCMUAFihAoQW/XL3zUOh0P78ePHyffs\ndrv25csX1yks8vT01D5+/Hj09be3t/b169cLnlFecz/L1mr8PG0z1Vyglr7Hbe4mmHsd75b8rCr8\nPG0zVXVTo/bWUXpq9o8w2+N3aUM1vMEi3lhTI/LWUXpqZo8w2+N3aTcqhjcTN9Z2+/2+7ff7a5/G\nrJeXl2ufwqy0MxW0Xl9fr30KNyPtTLXb7Sb/jNv28PBw7VOYlXamivYMNfb29ja5UbHFbreb3KhA\nLGlDpTJ1849fP4fjdzHRBxL8ZAvV1Kg69Z5ru/VfRF7S3ADVv+fW2ULFqFoPA9RPaTcqgKgIFSBG\nqAAxQgWIESpAjFABYoQKECNUgFj5rylFt6QtwVDE2rJqCFVwa2vFrlVbtqQ/RWs1elTYQkUTkFqW\nVjXTo2KD6k1AHD0qkEPK5Z/jhh0fc+vxHD0qqht+RpEHqJShctyw4/+/UgCiN9HpDT8T1efTX7vy\nutlSR+kmOv31Kq+bUKF0v4/+epXXnXL55zCuWq1QodqLutwbG35Gqs/Hce0pQ+VoqqJ+6HWcY3VR\nNybGUoYqww83wznCw/ZMtWRUZuS+HUs/ywqfuW2mYqTWWNKVavz+a+Dzfpdy+VdJlk0EvGNLHRAj\nVIAYoQLECBUgRqgAMUIFiEm31Nf2U+hFLjcA1pLOVOd+fb5auQFuG8s/QCzVNyrmlpfnLCMdx0Rt\nqUI1t0w8ZxnpOKbLsQEgUvCPddGK3FNCLVWoshmHYOvNfyzgkYJ/rHdEpZ4fhMpofLNHuvmHsjR+\n6WdB5aznOCYbFUjT+KWf7ZSznuOYhAppGr/0BY7KQkfHMVn+GY0LDKPesFGXe2OOjQ7HMQmVkfpm\nPVYFHCmsUw1v+r+vIlWo5krLz7m5HMd0yTCjVNk2PyVVqBw3VYYbFbmwUQGISUN17lIp0hIL2Eq6\n/GMpBbD8A+QIFSBGqAAxQgWIESpATP7L33Oav0QuNwDWkofKUX3rcKxC9ZhKlavYpuzyb239TKXK\nVWyT6rt/NGmJbW72rzLbp5qpMjVpqWhuNq8y26eaqbJx9H5QN5OBHqEycvR+cDSTqdz4pb925XWn\nWv7Bo3LjFwdmKqNhVXHk8pYs59mX6itL8x2zMqEycnxgjmYyUZd7Y1l2DglVMlkCUFmqZ6q5UTny\n0qWCuWVZlY5KqWYqRunYsizP3OQz1bltwi5t7ahZZZTFdvKZKstswqgKl1TPVEAGhAoQI1SAGKEC\nxAgVIEaoADFCBYgRKkAs1deU6FGBDFKFKkuPilPhJ/i3zx6qcYedqB11lL0fToU7SvDhYw/VuPQ5\naim0o/dDFo4eFcPBVDWQOnpUOKRa/uGnw+HQWtN9ednRo2I4eKoGUkePChq/AGL9IKJcmTBTJaTe\n6HA0fumbtPR/Vh5TWdvWX7uyps8equEPt//viBwNVbJw7EY6nnkcx0zZTSnyA+WQ8oc7Duj4Ndy2\nVMu/Uzdr/3oE/B6qtlSh4mZFBuz+AWKEChAjVIAYoQLECBUgRqgAMUIFiBEqQIxQAWKEChBL9TUl\ntXGp/5ToVaaIJ1Wo5kKwNgBLKkijlv8jrlShmrvBbz0AU12aInZnytLsxyVVqLJRNz+ZKnuJ2KAm\nS7Mfl4uF6unpqbWWp2hRwdH8xEXdTMbB0aSltdZeXl7aw8OD7Hjs/gFiF5upKs1QPUfzExflyO/q\nS+KaRZWzVGs8U1mpB5KpdgJRWggMVRxAh1KFajwCTr1+yyI/7+BdqlCpR8C5kPbvAdZIFSq16ssU\neLD7B4gRKkCMUAFihAoQI1SAGKECxAgVIEaoADFCBYiV/kbFkh4VrdWrXMU2pUO1tHAwQoHhVCn9\nUMSy+qpKh8rp2Cx47qw3VzYfpaxefd0ZpQtVlg/t2OwWYdZzqnrdQxcJ1XDpsnWZwoeG6C6y+zdc\nmkRZpuB3h8PhV/OXyNTn6bhuttQBsYss/4a9FSL2VECeUn31eTqu+yKhcnbqGf59JFnOU63qdQ+l\n2/2LtMN3yiU6KY1fjyDL5+OULlRVZVmeofhGxdIlSaWlC7YrPVOxVIFD6ZkKcCBUgBihAsQIFSBG\nqACxcrt/S6t9j4lWYoJ4ys1UW0tEKDHBnHKhAtwsyz8aqqAyS6hcDVXmwkpIEUGqjYq5EEZ63pka\nALaE/tSAwmASS6pQZTIVgC2hP/X/RhpMWptup1aphRqhwq9ZUDXjTdV9KXqTqM+ztfcBQBl6dv/w\na6aLNuONOc6zD7uyIRGhwq96seh1Y47z7CumlZXTLP+QZpPDcZ6O5zxmKpOp0TT6TKAyNepH6aFx\nCalmqmOdeoavR6EeVU9de6Trbo1+GpZQzd38w/etkWWZ4lD52rOxhIobAJXxTAWIESpArFyotj7U\nR9sUQDypdv8UeN6DW7mZCnAjVIAYoQLECBUgRqgAMUIFiFm31KfKqntRyqsznCNysYbqVDXluZWW\nx0JwbgAc54ja0v3y99iNHi0AjuYn6g5NveG5Kmbn8bVXm/HThSoLR/MTdYem3vC8FIPT+BiqAc/R\npMWBjQqk4WjS4kCo8Fupe+Syd0eTFgeWfya73W7ymWqLqYpqxbfm1Uup8bWrQhB5yTdkDdXUjTV8\nTXlM9fG2HLM1zw2Q5Rv2WW5+l7uu6xa/+fPnz93j46PxdIC4np+f/+m67u+59/FMBYgRKkCMUAFi\nhAoQI1SAGKECxAgVIEaoADFCBYgRKkDM8t2/UyXqY9FrY4C1LKFaU+9yzdqYqUraKcp/DR23L1Xp\nx9wMuHbWW1o1G/1fbUcsqUI1N6tFqgg9NgtumfUcfS+glypUmRyb3bbMeo6+Fw7jAaXa8plQQW48\ncCiXz4fDobWmK4TsBwBl8NlSR2uttf1+3/b7/bVP4yYwU6G11trr6+u1T2ER9fOjY1nKTJXIVM+M\niJ2Fxs1oqv2TrqlmqlNNWvrXo5jqfNT//bmy7PJV2pSYkipUWW6q1rixKrMs/9bMGNecXZbOGtWW\nL9jGMlNlmVGYTeDARgUgRqgAMUIFiBEqQIxQAWKEChAjVIAYoQLEVv37VHd3d/+21r75TgcI7a+u\n6z7NvWlVqADMY/kHiBEqQIxQAWKEChAjVIAYoQLECBUgRqgAMUIFiP0PWvOX5ZQKWdcAAAAASUVO\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZKD6zGkdaDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}