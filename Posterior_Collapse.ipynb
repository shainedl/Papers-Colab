{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Posterior_Collapse.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shainedl/Papers-Colab/blob/master/Posterior_Collapse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4Mg5TYiaI7D",
        "colab_type": "code",
        "outputId": "4975208f-91d9-4667-e0d8-4bb6136aaa07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from google.colab import files\n",
        "from collections import defaultdict\n",
        "from itertools import count, chain\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import numpy as np\n",
        "import pdb\n",
        "import re"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU1ksCbwaOjT",
        "colab_type": "code",
        "outputId": "1cb5ef1e-f721-4fdb-9cf1-0bd4b81d8d56",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "uploaded_training = files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-aa7db2bd-9091-4f2c-b7d2-69976f4bd213\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-aa7db2bd-9091-4f2c-b7d2-69976f4bd213\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving sample_train.txt to sample_train.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_3MgQC5aVpG",
        "colab_type": "code",
        "outputId": "0fa8edbe-2c9a-48fc-dfd1-4ca5828e60d9",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "uploaded_val = files.upload()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9cb7256f-57bc-470e-b32c-5850986a31d5\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-9cb7256f-57bc-470e-b32c-5850986a31d5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving sample_dev.txt to sample_dev.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygvyxibDahEP",
        "colab_type": "code",
        "outputId": "10e56403-9034-4959-80c5-483e62b4fc48",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "uploaded_test = files.upload()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b16202db-5bc5-4efc-ad19-9646811234bd\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-b16202db-5bc5-4efc-ad19-9646811234bd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving sample_test.txt to sample_test.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR1DMUXiammS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(file):\n",
        "  \"\"\"\n",
        "  Load training data and output vocabulary dictionaries\n",
        "  \"\"\"\n",
        "  w2i = defaultdict(lambda x=count(0): next(x))\n",
        "  w2i[\"<s>\"] \n",
        "  w2i[\"</s>\"] \n",
        "  w2i[\"<unk>\"] \n",
        "  w2i['<unk>twoDigitNum']\n",
        "  w2i['<unk>fourDigitNum']\n",
        "  w2i['<unk>containsDigitAndAlpha']\n",
        "  w2i['<unk>containsDigitAndDash']\n",
        "  w2i['<unk>containsDigitAndSlash']\n",
        "  w2i['<unk>containsDigitAndComma']\n",
        "  w2i['<unk>containsDigitAndPeriod']\n",
        "  w2i['<unk>othernum']\n",
        "  w2i['<unk>allCaps']\n",
        "  w2i['<unk>initCap']\n",
        "  w2i['<unk>lowercase']\n",
        "  w2i['<unk>time']\n",
        "  data = []\n",
        "  file = file.decode('utf-8')\n",
        "  sentences = file.splitlines()\n",
        "\n",
        "  for sentence in sentences:\n",
        "    tokens = word_tokenize(sentence)\n",
        "    for token in tokens:\n",
        "      w2i[token]\n",
        "    data.append(tokens)\n",
        "\n",
        "  freq_dist = nltk.FreqDist([item for sublist in data for item in sublist])\n",
        "  freq1 = set(list(freq_dist.keys())[-4000:])\n",
        "\n",
        "  w2i = dict(w2i)\n",
        "  for key in list(w2i.keys()):\n",
        "    if key in freq1:\n",
        "      w2i.pop(key)\n",
        "  i2w = {i:w for w,i in w2i.items()}\n",
        "\n",
        "  return data, w2i, i2w, freq_dist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqelmnoMayFH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_data, w2i, i2w, freq_dist = load_data(uploaded_training['sample_train.txt'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuOoLPZEaz_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data_test(file):\n",
        "  \"\"\"\n",
        "  Load test and validation data\n",
        "  \"\"\"\n",
        "  data = []\n",
        "  file = file.decode('utf-8')\n",
        "  sentences = file.splitlines()\n",
        "\n",
        "  for sentence in sentences:\n",
        "    tokens = word_tokenize(sentence)\n",
        "    data.append(tokens)\n",
        "    \n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSAbigBra1Tw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_data = load_data_test(uploaded_val['sample_dev.txt'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRPrdosha2jJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = load_data_test(uploaded_test['sample_test.txt'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaRH-yBsa3eJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_sequence(seq, to_ix):\n",
        "  idxs = [to_ix[\"<s>\"]]\n",
        "  for w in seq:\n",
        "    if w in to_ix:\n",
        "      idxs.append(to_ix[w])\n",
        "    else:\n",
        "      unk = get_word_class(w)\n",
        "      idxs.append(to_ix[unk])\n",
        "  idxs.append(to_ix[\"</s>\"])\n",
        "  return torch.tensor(idxs, dtype=torch.long)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-o0z6Faa40z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_word_class(x):\n",
        "    \"\"\"\n",
        "    Get fword class for a given word.\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : str\n",
        "        word to be replaced\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        corresponding word class\n",
        "    \"\"\"\n",
        "    if re.fullmatch(r'[0-9]{2}', x):\n",
        "        return '<unk>twoDigitNum'\n",
        "    elif re.fullmatch(r'[0-9]{4}', x):\n",
        "        return '<unk>fourDigitNum'\n",
        "    elif re.fullmatch(r'A[0-9\\-]+', x):\n",
        "        return '<unk>containsDigitAndAlpha'\n",
        "    elif re.fullmatch(r'[0-9]+\\-[0-9]+', x):\n",
        "        return '<unk>containsDigitAndDash'\n",
        "    elif re.fullmatch(r'[0-9]+/[0-9]+/[0-9]+', x):\n",
        "        return '<unk>containsDigitAndSlash'\n",
        "    elif re.fullmatch(r'([0-9]+,[0-9]+)+\\.[0-9]+', x):\n",
        "        return '<unk>containsDigitAndComma'\n",
        "    elif re.fullmatch(r'[0-9]+\\.[0-9]+', x):\n",
        "        return '<unk>containsDigitAndPeriod'\n",
        "    elif re.fullmatch(r'[0-9]+', x):\n",
        "        return '<unk>othernum'\n",
        "    elif re.fullmatch(r'[A-Z]+', x):\n",
        "        return '<unk>allCaps'\n",
        "    elif re.fullmatch(r'[A-Z][a-z]+', x):\n",
        "        return '<unk>initCap'\n",
        "    elif re.fullmatch(r'[a-z]+', x):\n",
        "        return '<unk>lowercase'\n",
        "    elif re.match(r'[0-9]+:[0-9]+', x):\n",
        "        return '<unk>time'\n",
        "\n",
        "    return '<unk>'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYbg1MLFa6Z7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_data(batch_size, data, w2i):\n",
        "  \"\"\"\n",
        "  Batches data with sequences of the same length\n",
        "  \"\"\"\n",
        "  sentence_lengths = np.array([len(sentence) for sentence in data])\n",
        "  sorted_idx = np.argsort(sentence_lengths)\n",
        "  sorted_lengths = sentence_lengths[sorted_idx]\n",
        "\n",
        "  len_increase_idx = []\n",
        "  for i in range(1, len(sorted_lengths)):\n",
        "    if sorted_lengths[i] > sorted_lengths[i-1]:\n",
        "      len_increase_idx.append(i)\n",
        "  len_increase_idx.append(len(sorted_lengths))\n",
        "\n",
        "  batch_data = []\n",
        "  curr_idx = 0\n",
        "  for i, idx in enumerate(len_increase_idx):\n",
        "    while curr_idx < idx:\n",
        "      batch_sentences = []\n",
        "      new_idx = min(curr_idx + batch_size, idx)\n",
        "      for i in range(curr_idx, new_idx):\n",
        "        sent_to_vec = prepare_sequence(data[sorted_idx[i]], w2i)\n",
        "        batch_sentences.append(sent_to_vec)\n",
        "      curr_idx = new_idx\n",
        "      batch_sentences = torch.stack(batch_sentences).to(device=cuda)\n",
        "      batch_data.append(batch_sentences)\n",
        "\n",
        "  i = 0\n",
        "  j = len(batch_data)\n",
        "  while i < j:\n",
        "    if i != 0 and len(batch_data[i]) <= 2 and len(batch_data[i][0]) == len(batch_data[i-1][0]):\n",
        "      batch_data.append(torch.cat((batch_data[i], batch_data[i-1])))\n",
        "      batch_data.pop(i)\n",
        "      batch_data.pop(i-1)\n",
        "      i -= 1\n",
        "      j = len(batch_data)\n",
        "    elif len(batch_data[i]) == 1:\n",
        "      batch_data.pop(i)\n",
        "      j = len(batch_data)\n",
        "    else:\n",
        "      i += 1\n",
        "\n",
        "  return batch_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPzqSwHIa8a4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hO_2lWKa9zi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_cuda = True\n",
        "cuda = torch.device('cuda')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKZ1Kuj0a_PW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_training = batch_data(batch_size, training_data, w2i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOF1ngl1bAQj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_val = batch_data(batch_size, val_data, w2i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPUl_RdxbBMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_test = batch_data(batch_size, test_data, w2i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiqQIDEZbDDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_size, hidden_size, latent_size):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.embeddings = nn.Embedding(num_embeddings=vocab_size,\n",
        "                                   embedding_dim=embedding_size)\n",
        "    \n",
        "    self.rnn = nn.LSTM(input_size=embedding_size,\n",
        "                        hidden_size=hidden_size,\n",
        "                        batch_first=True)\n",
        "    \n",
        "    self.fc_mu = nn.Linear(hidden_size, latent_size)\n",
        "    self.fc_var = nn.Linear(hidden_size, latent_size)\n",
        "\n",
        "    self._initialize_parameters([-0.01, 0.01], [-0.1, 0.1]) \n",
        "\n",
        "  def encode(self, x):\n",
        "    \"\"\"\n",
        "    Produces a Gaussian distribution over the possible values of the code z \n",
        "    from which x could have been generated\n",
        "        \n",
        "    Parameters\n",
        "    ----------\n",
        "      x: batch size x sequence length Tensor\n",
        "        observed data\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "      mu: batch size x latent size Tensor\n",
        "        mean of Gaussian distribution\n",
        "        \n",
        "      logvar: batch size x latent size Tensor\n",
        "        log of variance of Gaussian distribution     \n",
        "    \"\"\"\n",
        "    x = self.embeddings(x)\n",
        "    outputs, (hidden, cell) = self.rnn(x)\n",
        "    mu = self.fc_mu(hidden)\n",
        "    logvar = self.fc_var(hidden)\n",
        "    mu = mu.squeeze()\n",
        "    logvar = logvar.squeeze()\n",
        "    return mu, logvar \n",
        "\n",
        "  def _initialize_parameters(self, lstm_init, embed_init):\n",
        "    \"\"\"\n",
        "    Initializes the LSTM parameters and embeddings with uniform distributions\n",
        "    \"\"\"\n",
        "    for param in self.parameters():\n",
        "      nn.init.uniform_(param, a=lstm_init[0], b=lstm_init[1])\n",
        "    nn.init.uniform_(self.embeddings.weight, a=embed_init[0], b=embed_init[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp6lE4_XbEQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_size, hidden_size, latent_size):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.embeddings = nn.Embedding(num_embeddings=vocab_size,\n",
        "                                   embedding_dim=embedding_size)\n",
        "    \n",
        "    self.rnn = nn.LSTM(input_size=embedding_size + latent_size,\n",
        "                        hidden_size=hidden_size,\n",
        "                        batch_first=True)    \n",
        "\n",
        "    self.fc_hid = nn.Linear(latent_size, hidden_size, bias=False)\n",
        "    self.fc_voc = nn.Linear(hidden_size, vocab_size, bias=False)\n",
        "\n",
        "    self.dropout = nn.Dropout()\n",
        "    self._initialize_parameters([-0.01, 0.01], [-0.1, 0.1])\n",
        "\n",
        "  def decode(self, z, inputs):\n",
        "    \"\"\"\n",
        "    Given a code z it produces unscaled output corresponding to the vocabulary\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "      z: batch size x latent size Tensor\n",
        "        latent variables\n",
        "      \n",
        "      inputs: batch size x sequence length Tensor\n",
        "        source sequence\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "      output_logits: batch size x sequence length x vocab size Tensor\n",
        "        unscaled output\n",
        "    \"\"\"\n",
        "    cell = self.fc_hid(z)\n",
        "    cell = cell.unsqueeze(0)\n",
        "    hidden = torch.tanh(cell)\n",
        "\n",
        "    embed = self.embeddings(inputs)\n",
        "    embed = self.dropout(embed)\n",
        "    z = z.expand(embed.size(1), z.size(0), z.size(1))\n",
        "    z = z.transpose(1,0)\n",
        "    embed_lat = torch.cat((embed, z), 2)\n",
        "\n",
        "    outputs, (hidden, cell) = self.rnn(embed_lat, (hidden, cell))\n",
        "    outputs = self.dropout(outputs)\n",
        "    output_logits = self.fc_voc(outputs)\n",
        "    return output_logits\n",
        "\n",
        "  def decode_greedy(self, z, inputs, interpolation=False):\n",
        "    \"\"\"\n",
        "    Given a code z it produces unscaled output corresponding to the vocabulary\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "      z: batch size x latent size Tensor\n",
        "        latent variables\n",
        "      \n",
        "      inputs: batch size x sequence length Tensor\n",
        "        source sequence\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "      output_logits: batch size x sequence length x vocab size Tensor\n",
        "        unscaled output\n",
        "      \n",
        "      batch_decoded: batch size x output sequence length list\n",
        "        decoded output sequence\n",
        "    \"\"\"\n",
        "    cell = self.fc_hid(z)\n",
        "    cell = cell.unsqueeze(0)\n",
        "    hidden = torch.tanh(cell)\n",
        "\n",
        "    batch_size = inputs.size(0)\n",
        "    input_d = inputs[:,0]\n",
        "    output_logit_prev = None\n",
        "    seq_len = inputs.size(1)\n",
        "    batch_decoded = [[] for j in range(batch_size)]\n",
        "\n",
        "    end_mask = torch.ones(batch_size)\n",
        "    counter = 0\n",
        "    while end_mask.sum() != 0 and counter < seq_len:\n",
        "      embed = self.embeddings(input_d)\n",
        "      embed_lat = torch.cat((embed, z), 1)\n",
        "      embed_lat = embed_lat.unsqueeze(1)\n",
        "      outputs, (hidden, cell) = self.rnn(embed_lat, (hidden, cell))  \n",
        "      output_logit = self.fc_voc(outputs)\n",
        "      if output_logit_prev is not None:\n",
        "        output_logits = torch.cat((output_logit_prev, output_logit), dim=1)\n",
        "        output_logit_prev = output_logits\n",
        "      else:\n",
        "        output_logit_prev = output_logit\n",
        "      input_d = torch.argmax(output_logit, dim=2).flatten()\n",
        "\n",
        "      for k in range(batch_size):\n",
        "        if end_mask[k] != 0:\n",
        "          if interpolation and input_d[k].item() == w2i[\"</s>\"] :\n",
        "            end_mask[k] = 0\n",
        "          else:\n",
        "            token = i2w[input_d[k].item()]\n",
        "            batch_decoded[k].append(token)\n",
        "      counter += 1\n",
        "    \n",
        "    return output_logits, batch_decoded\n",
        "\n",
        "  def _initialize_parameters(self, lstm_init, embed_init):\n",
        "    \"\"\"\n",
        "    Initializes the LSTM parameters and embeddings with uniform distributions\n",
        "    \"\"\"\n",
        "    for param in self.parameters():\n",
        "      nn.init.uniform_(param, a=lstm_init[0], b=lstm_init[1])\n",
        "    nn.init.uniform_(self.embeddings.weight, a=embed_init[0], b=embed_init[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FG9GSLxPbFuT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VAE(nn.Module):\n",
        "\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super(VAE, self).__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.re_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "  def forward(self, x, greedy=False):\n",
        "    \"\"\"\n",
        "    Forward pass of the model \n",
        "    \"\"\"\n",
        "    mu, logvar = self.encoder.encode(x)\n",
        "    kl = self.get_kl(mu, logvar)\n",
        "    z = self._reparameterize(mu, logvar)\n",
        "\n",
        "    source = x[:,:-1]\n",
        "    target = x[:, 1:]\n",
        "    if greedy:\n",
        "      output_logits, batch_decoded = self.decoder.decode_greedy(z, source)\n",
        "      re = self.get_reconstruction_error(output_logits, target)\n",
        "      return kl, re, batch_decoded\n",
        "    else: \n",
        "      output_logits = self.decoder.decode(z, source)\n",
        "      re = self.get_reconstruction_error(output_logits, target)\n",
        "      return kl, re\n",
        "\n",
        "  def _reparameterize(self, mu, logvar):\n",
        "    \"\"\"\n",
        "    Reparameterize the random variable z to express as a deterministic variable\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "      mu: batch size x latent size Tensor\n",
        "        mean of Gaussian distribution\n",
        "        \n",
        "      logvar: batch size x latent size Tensor\n",
        "        log of variance of Gaussian distribution     \n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "      z: batch size x latent size Tensor\n",
        "        reparameterization of latent variables\n",
        "    \"\"\"\n",
        "    std = torch.exp(logvar / 2)\n",
        "    eps = torch.randn_like(std)\n",
        "    return mu + std * eps  \n",
        "\n",
        "  def get_kl(self, mu, logvar):\n",
        "    \"\"\"\n",
        "    Returns the KLD between posterior and prior\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "      mu: batch size x latent size Tensor\n",
        "        mean of Gaussian distribution\n",
        "        \n",
        "      logvar: batch size x latent size Tensor\n",
        "        log of variance of Gaussian distribution\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "      kl: batch size x latent size Tensor\n",
        "        kl divergence\n",
        "    \"\"\"\n",
        "    return (mu**2 + logvar.exp() - 1 - logvar) / 2\n",
        "\n",
        "  def get_reconstruction_error(self, output_logits, target):\n",
        "    \"\"\"\n",
        "    Returns the reconstruction error\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "      output_logits: batch size x sequence length x vocab size Tensor\n",
        "        unscaled output\n",
        "      \n",
        "      target: batch size x sequence length Tensor\n",
        "        target sequence\n",
        "    \"\"\"\n",
        "    target = target.contiguous().view(-1)\n",
        "    output_logits = output_logits.view(-1, output_logits.size(2))\n",
        "    return self.re_loss(output_logits, target)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJRMfmJIbHcb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(w2i)\n",
        "embedding_size = 128\n",
        "hidden_size = 512 \n",
        "latent_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBa3KOvZbI7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print_every = round(len(batch_training) / batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SD2w4eBnbNWF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecayLearning:\n",
        "  \"\"\"\n",
        "  Class updated from https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
        "  \"\"\"\n",
        "  def __init__(self, patience=2):\n",
        "    self.patience = patience\n",
        "    self.counter = 0\n",
        "    self.best_score = None\n",
        "    self.update_lr = False\n",
        "\n",
        "  def __call__(self, val_loss):\n",
        "    score = -val_loss\n",
        "\n",
        "    if self.best_score is None:\n",
        "      self.best_score = score\n",
        "    elif score < self.best_score:\n",
        "      self.counter += 1\n",
        "      if self.counter >= self.patience:\n",
        "        self.update_lr = True\n",
        "    else:\n",
        "      self.best_score = score\n",
        "      self.counter = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpZjLw7-hUz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def interpolate(latent_size):\n",
        "  sample1 = torch.randn(1, latent_size, device=cuda)\n",
        "  sample2 = torch.randn(1, latent_size, device=cuda)\n",
        "  for w in range(11):\n",
        "    weight = w * 0.1\n",
        "    sample = weight * sample2 + (1-weight) * sample1\n",
        "    _, batch_decoded = decoder.decode_greedy(sample, torch.zeros(13, dtype=torch.long, device=cuda).unsqueeze(0), True)\n",
        "    print(*batch_decoded[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS9bGdlsb5YH",
        "colab_type": "text"
      },
      "source": [
        "Pre-training on Autoencoder objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saVwTKzDc3eq",
        "colab_type": "text"
      },
      "source": [
        "Pre-training ran for 3 hours"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGrbRCtRbKgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epoch):\n",
        "  \"\"\"\n",
        "  Trains the model\n",
        "  \"\"\"\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  for batch_idx, data in enumerate(batch_training):\n",
        "    optimizer_e.zero_grad()\n",
        "    optimizer_d.zero_grad()\n",
        "    re = model(data)\n",
        "    loss = re\n",
        "    loss.backward()\n",
        "    optimizer_e.step()\n",
        "    optimizer_d.step()\n",
        "\n",
        "    running_loss += loss\n",
        "  \n",
        "    if (epoch == 0 or epoch % 10 == 9) and batch_idx % print_every == print_every-1:    \n",
        "      print('[%d, %5d] Train loss: %.3f' % (epoch + 1, batch_idx + 1, running_loss / print_every))\n",
        "      running_loss = 0.0 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiHxkIzObLqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(epoch, validation=False):\n",
        "  \"\"\"\n",
        "  Run the model on validation or test dataset\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  anneal = 0.1 * epoch if epoch < 10 else 1.0\n",
        "\n",
        "  data = batch_val if validation else batch_test\n",
        "  running_loss = 0.0\n",
        "  for batch_idx, data in enumerate(data):\n",
        "    if validation:  \n",
        "      re = model(data)\n",
        "    else:\n",
        "      re, batch_decoded = model(data, True)\n",
        "    loss = re\n",
        "\n",
        "    running_loss += loss\n",
        "\n",
        "  avg_loss = running_loss / (batch_idx + 1)\n",
        "  if validation and (epoch == 0 or epoch % 10 == 9):\n",
        "    print('[%d, %5d] Validation loss: %.3f' % (epoch + 1, batch_idx + 1, avg_loss))\n",
        "  else:\n",
        "    print('[%d, %5d] Test loss: %.3f' % (epoch + 1, batch_idx + 1, avg_loss))\n",
        "    print(*batch_decoded[0])\n",
        "\n",
        "  return avg_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dxpRu1LbSaA",
        "colab_type": "code",
        "outputId": "430fd20c-33aa-4954-ed8c-9194c6e04c56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "encoder = Encoder(vocab_size, embedding_size, hidden_size, latent_size)\n",
        "decoder = Decoder(vocab_size, embedding_size, hidden_size, latent_size)\n",
        "model = VAE(encoder, decoder)\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "  \n",
        "lr = 0.5\n",
        "optimizer_e = optim.SGD(encoder.parameters(), lr=lr)\n",
        "optimizer_d = optim.SGD(decoder.parameters(), lr=lr)\n",
        "\n",
        "decay_learning = DecayLearning()\n",
        "num_decays = 0\n",
        "early_stop = 5\n",
        "\n",
        "for epoch in range(10):\n",
        "  train(epoch)\n",
        "  val_loss = test(epoch, True)\n",
        "  decay_learning(val_loss)\n",
        "  if decay_learning.update_lr:\n",
        "    lr *= 0.5\n",
        "    num_decays += 1\n",
        "    if num_decays == early_stop + 1:\n",
        "      print(\"Stopping early at epoch\", epoch+1)\n",
        "      break\n",
        "    print(\"Learning rate has been decayed to\", lr, \"at epoch\", epoch+1)\n",
        "    optimizer_e = optim.SGD(encoder.parameters(), lr=lr)\n",
        "    optimizer_d = optim.SGD(decoder.parameters(), lr=lr)\n",
        "    decay_learning = DecayLearning()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  3125] Train loss: 3.947\n",
            "[1,  6250] Train loss: 3.595\n",
            "[1,  9375] Train loss: 3.789\n",
            "[1, 12500] Train loss: 3.809\n",
            "[1,  1248] Validation loss: 4.795\n",
            "Learning rate has been decayed to 0.25 at epoch 4\n",
            "[10,  3125] Train loss: 1.061\n",
            "[10,  6250] Train loss: 1.135\n",
            "[10,  9375] Train loss: 1.453\n",
            "[10, 12500] Train loss: 1.745\n",
            "[10,  1248] Validation loss: 2.527\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV6lmdwQhxKH",
        "colab_type": "code",
        "outputId": "da67386e-9564-4c69-9ce6-957550bd2cd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "# epoch in range(10,20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate has been decayed to 0.125 at epoch 17\n",
            "[20,  3125] Train loss: 0.276\n",
            "[20,  6250] Train loss: 0.382\n",
            "[20,  9375] Train loss: 0.614\n",
            "[20, 12500] Train loss: 0.917\n",
            "[20,  1248] Validation loss: 2.240\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHlKxzZ7pDqn",
        "colab_type": "code",
        "outputId": "3000eacc-df6a-417e-9e30-105968b243e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "# epoch in range(20,30)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate has been decayed to 0.0625 at epoch 23\n",
            "[30,  3125] Train loss: 0.136\n",
            "[30,  6250] Train loss: 0.214\n",
            "[30,  9375] Train loss: 0.391\n",
            "[30, 12500] Train loss: 0.666\n",
            "[30,  1248] Validation loss: 1.204\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fFbie1ywMoz",
        "colab_type": "code",
        "outputId": "da4ec794-681d-4ad6-d1af-b5323f6fcb0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "# epoch in range(30,40)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate has been decayed to 0.03125 at epoch 32\n",
            "[40,  3125] Train loss: 0.096\n",
            "[40,  6250] Train loss: 0.158\n",
            "[40,  9375] Train loss: 0.311\n",
            "[40, 12500] Train loss: 0.554\n",
            "[40,  1248] Validation loss: 0.786\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tafSVIN829PI",
        "colab_type": "code",
        "outputId": "ec59506a-1b7d-47e1-8b9e-0ba83a109ee6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "# epoch in range(40,50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate has been decayed to 0.015625 at epoch 43\n",
            "[50,  3125] Train loss: 0.081\n",
            "[50,  6250] Train loss: 0.137\n",
            "[50,  9375] Train loss: 0.273\n",
            "[50, 12500] Train loss: 0.504\n",
            "[50,  1248] Validation loss: 0.678\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMU44-dfUYzs",
        "colab_type": "code",
        "outputId": "de7ae0fd-64cc-4484-f375-dd0de2e1c65b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# epoch in range(50,60):"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stopping early at epoch 58\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NrS8irLaxYZ",
        "colab_type": "code",
        "outputId": "372b56b5-0b1a-4bc4-dd97-0df85fcc570f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "test(58, False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[59,  1249] Test loss: 1.142\n",
            "A caucasian wearing a green jacket and a hat and yellow shirts . </s>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.1421, device='cuda:0', grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRqTKdLFhWQr",
        "colab_type": "code",
        "outputId": "6655ca35-900a-4ea5-a9a0-54926debc4c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "interpolate(latent_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "friends are celebrating a white and one on another shirt\n",
            "the children were a yellow dress and riding an object\n",
            "the children were wearing yellow and shoes with a helmet .\n",
            "the children have a long , green on and no .\n",
            "the children chews and riding and have on another , outside\n",
            "An animal was doing karate and wearing no and sandals outside .\n",
            "An artist was balancing and boots at a small dog vendor .\n",
            "An artist was balancing and vegetables at another boy and shovel .\n",
            "A mother was selling vegetables and on crafts and vegetables together .\n",
            "A mother does not seen and at something , cooking flowers .\n",
            "A blond man shoots wine while wearing no hat and clothes .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7guNuHe3hp50",
        "colab_type": "code",
        "outputId": "7af06093-3fd0-4a18-b5fd-0bca66e7e5cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "interpolate(latent_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<unk>initCap , people fight\n",
            "<unk>initCap , people in dogs are running\n",
            "<unk>initCap , people in snow are looking\n",
            "Three firemen , a car are running across the car .\n",
            "Three firemen , a car are running across the car .\n",
            "Three firemen are a hot dog looking along the ocean .\n",
            "Three firemen are holding two dogs along along <unk>lowercase .\n",
            "Two firemen are holding a bus back along the sand .\n",
            "Two sheep are having two fishing walking across the .\n",
            "Two sheep are having two fishing going across leaves .\n",
            "Two sheep are having two fishing to the left .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9K29slQhvnl",
        "colab_type": "code",
        "outputId": "d23556fa-3935-454a-a2df-8b66eb66d8e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "interpolate(latent_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "some tourists run\n",
            "some actors has work\n",
            "some actors has work while look\n",
            "some spectators on opposing people are tired .\n",
            "two guys of flowers are posing while both\n",
            "two guys in helmets are posing , waits outside and .\n",
            "two guys in helmets are posing , waits outside and .\n",
            "Three guys in uniform are posing while both outside .\n",
            "Three guys in pink clothes are posing while holding bags .\n",
            "Two guys in pink uniforms are running , outside .\n",
            "Two guys in pink uniforms are running outside and outside .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD6Uqmt3hxes",
        "colab_type": "code",
        "outputId": "44ad237d-ac7c-47fa-f18f-1fbfa360fd14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "interpolate(latent_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This water rides through train tracks , his dog 's wings .\n",
            "a bucket from <unk>lowercase of pigeons , while his feet is life\n",
            "a bucket from <unk>lowercase of people because she was all in her\n",
            "a light boat of some passengers while wearing shorts 's mouth .\n",
            "a lone boat full of several people plays an white cone .\n",
            "a lone lot of some passengers while a brown car sleeps .\n",
            "a lone bicyclist riding a bucking dog and an obstacle stand .\n",
            "a bull , near a grocery store while an angel .\n",
            "a bull , riding a bucking dog outside of flowers .\n",
            "a small soldier riding a unicycle while wearing ear gear .\n",
            "a black cat carrying a flag while wearing ear gear .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgLwDbFH74qd",
        "colab_type": "text"
      },
      "source": [
        "Free Bits Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLIUepxtYkfn",
        "colab_type": "text"
      },
      "source": [
        "Free Bits ran for 2 and a half hours"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_g0gZhVfZmK_",
        "colab_type": "code",
        "outputId": "ae04d811-e2e4-4f0c-d62c-7a284d14c678",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGelVF5YdNWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Helpful Link: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
        "\"\"\"\n",
        "model_save_name = 'pretraining.pt'\n",
        "path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "checkpoint = torch.load(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i98KAajfUX4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epoch, target_rate=(4.0 / float(latent_size))):\n",
        "  \"\"\"\n",
        "  Trains the model\n",
        "  Helpful link for free bits: https://stats.stackexchange.com/questions/267924/explanation-of-the-free-bits-technique-for-variational-autoencoders\n",
        "  \"\"\"\n",
        "  model.train()\n",
        "  anneal = 0.1 * epoch if epoch < 10 else 1.0\n",
        "\n",
        "  running_loss = 0.0\n",
        "  running_kl = 0.0\n",
        "  running_re = 0.0\n",
        "  for batch_idx, data in enumerate(batch_training):\n",
        "    optimizer_e.zero_grad()\n",
        "    optimizer_d.zero_grad()\n",
        "    kl, re = model(data)\n",
        "    kl_mean = kl.mean(dim=0)\n",
        "    kl_mask = (kl_mean > target_rate).float()\n",
        "    fb_mask = (kl_mean <= target_rate).float()\n",
        "    free_b = kl_mask + target_rate\n",
        "    kl = (kl_mean * kl_mask + free_b * fb_mask).sum()\n",
        "    loss = kl * anneal + re\n",
        "    running_kl += kl * anneal\n",
        "    running_re += re\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
        "    optimizer_e.step()\n",
        "    optimizer_d.step()\n",
        "\n",
        "    running_loss += loss\n",
        "  \n",
        "    if (epoch == 0 or epoch % 10 == 9) and  batch_idx % print_every == print_every-1:    \n",
        "      print('[%d, %5d] Train loss: %.3f' % (epoch + 1, batch_idx + 1, running_loss / print_every))\n",
        "      running_loss = 0.0 \n",
        "      print('[%d, %5d] Train KL: %.3f' % (epoch + 1, batch_idx + 1, running_kl / print_every))\n",
        "      print('[%d, %5d] Train RE: %.3f' % (epoch + 1, batch_idx + 1, running_re / print_every))\n",
        "      running_kl = 0.0\n",
        "      running_re = 0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SnHpy3FWv9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(epoch, validation=False, target_rate = (4.0 / float(latent_size))):\n",
        "  \"\"\"\n",
        "  Run the model on validation or test dataset\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  anneal = 0.1 * epoch if epoch < 10 else 1.0\n",
        "\n",
        "  data = batch_val if validation else batch_test\n",
        "  running_loss = 0.0\n",
        "  running_kl = 0.0\n",
        "  running_re = 0.0\n",
        "  for batch_idx, data in enumerate(data):\n",
        "    if validation:  \n",
        "      kl, re = model(data)\n",
        "    else:\n",
        "      kl, re, batch_decoded = model(data, True)\n",
        "    kl_mean = kl.mean(dim=0)\n",
        "    kl_mask = (kl_mean > target_rate).float()\n",
        "    fb_mask = (kl_mean <= target_rate).float()\n",
        "    fb = kl_mask + target_rate\n",
        "    kl = (kl_mean * kl_mask + fb * fb_mask).sum()\n",
        "    loss = kl * anneal + re\n",
        "    running_kl += kl * anneal\n",
        "    running_re += re\n",
        "\n",
        "    running_loss += loss\n",
        "\n",
        "  avg_loss = running_loss / (batch_idx + 1)\n",
        "  avg_kl = running_kl / (batch_idx + 1)\n",
        "  avg_re = running_re / (batch_idx + 1)\n",
        "  if validation and (epoch == 0 or epoch % 10 == 9):\n",
        "    print('[%d, %5d] Validation loss: %.3f' % (epoch + 1, batch_idx + 1, avg_loss))\n",
        "    print('[%d, %5d] Validation KL: %.3f' % (epoch + 1, batch_idx + 1, avg_kl))\n",
        "    print('[%d, %5d] Validation RE: %.3f' % (epoch + 1, batch_idx + 1, avg_re))\n",
        "  elif not validation:\n",
        "    print('[%d, %5d] Test loss: %.3f' % (epoch + 1, batch_idx + 1, avg_loss))\n",
        "    print('[%d, %5d] Test KL: %.3f' % (epoch + 1, batch_idx + 1, avg_kl))\n",
        "    print('[%d, %5d] Test RE: %.3f' % (epoch + 1, batch_idx + 1, avg_re))\n",
        "    print(*batch_decoded[0])\n",
        "\n",
        "  return avg_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8Ivstgze1-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(vocab_size, embedding_size, hidden_size, latent_size)\n",
        "encoder.load_state_dict(checkpoint['encoder'])\n",
        "decoder = Decoder(vocab_size, embedding_size, hidden_size, latent_size)\n",
        "model = VAE(encoder, decoder)\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArRFGjxSa-UT",
        "colab_type": "code",
        "outputId": "6989fa93-34b0-462e-b83d-3d609844eb3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        }
      },
      "source": [
        "lr = 0.5\n",
        "optimizer_e = optim.SGD(encoder.parameters(), lr=lr)\n",
        "optimizer_d = optim.SGD(decoder.parameters(), lr=lr)\n",
        "decay_learning = DecayLearning()\n",
        "num_decays = 0\n",
        "early_stop = 5\n",
        "clip_grad = 5.0\n",
        "\n",
        "for epoch in range(10):\n",
        "  train(epoch)\n",
        "  val_loss = test(epoch, True)\n",
        "  decay_learning(val_loss)\n",
        "  if decay_learning.update_lr:\n",
        "    lr *= 0.5\n",
        "    num_decays += 1\n",
        "    if num_decays == early_stop + 1:\n",
        "      print(\"Stopping early at epoch\", epoch+1)\n",
        "      break\n",
        "    print(\"Learning rate has been decayed to\", lr, \"at epoch\", epoch+1)\n",
        "    optimizer_e = optim.SGD(encoder.parameters(), lr=lr)\n",
        "    optimizer_d = optim.SGD(decoder.parameters(), lr=lr)\n",
        "    decay_learning = DecayLearning()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  3125] Train loss: 2.330\n",
            "[1,  3125] Train KL: 0.000\n",
            "[1,  3125] Train RE: 2.330\n",
            "[1,  6250] Train loss: 1.797\n",
            "[1,  6250] Train KL: 0.000\n",
            "[1,  6250] Train RE: 1.797\n",
            "[1,  9375] Train loss: 2.044\n",
            "[1,  9375] Train KL: 0.000\n",
            "[1,  9375] Train RE: 2.044\n",
            "[1, 12500] Train loss: 2.248\n",
            "[1, 12500] Train KL: 0.000\n",
            "[1, 12500] Train RE: 2.248\n",
            "[1,  1248] Validation loss: 2.836\n",
            "[1,  1248] Validation KL: 0.000\n",
            "[1,  1248] Validation RE: 2.836\n",
            "Learning rate has been decayed to 0.25 at epoch 3\n",
            "Learning rate has been decayed to 0.125 at epoch 6\n",
            "Learning rate has been decayed to 0.0625 at epoch 9\n",
            "[10,  3125] Train loss: 6.338\n",
            "[10,  3125] Train KL: 3.630\n",
            "[10,  3125] Train RE: 2.708\n",
            "[10,  6250] Train loss: 6.323\n",
            "[10,  6250] Train KL: 3.621\n",
            "[10,  6250] Train RE: 2.702\n",
            "[10,  9375] Train loss: 6.618\n",
            "[10,  9375] Train KL: 3.618\n",
            "[10,  9375] Train RE: 3.000\n",
            "[10, 12500] Train loss: 6.758\n",
            "[10, 12500] Train KL: 3.613\n",
            "[10, 12500] Train RE: 3.144\n",
            "[10,  1248] Validation loss: 7.505\n",
            "[10,  1248] Validation KL: 3.644\n",
            "[10,  1248] Validation RE: 3.861\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXSa3ugS9G9_",
        "colab_type": "code",
        "outputId": "80de3880-8b29-4261-dc59-1f74c834c44d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "source": [
        "# epoch in range(10, 20)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate has been decayed to 0.03125 at epoch 12\n",
            "Learning rate has been decayed to 0.015625 at epoch 15\n",
            "[20,  3125] Train loss: 6.790\n",
            "[20,  3125] Train KL: 4.023\n",
            "[20,  3125] Train RE: 2.767\n",
            "[20,  6250] Train loss: 6.744\n",
            "[20,  6250] Train KL: 4.015\n",
            "[20,  6250] Train RE: 2.728\n",
            "[20,  9375] Train loss: 6.998\n",
            "[20,  9375] Train KL: 4.013\n",
            "[20,  9375] Train RE: 2.985\n",
            "[20, 12500] Train loss: 7.123\n",
            "[20, 12500] Train KL: 4.010\n",
            "[20, 12500] Train RE: 3.113\n",
            "[20,  1248] Validation loss: 7.674\n",
            "[20,  1248] Validation KL: 4.032\n",
            "[20,  1248] Validation RE: 3.643\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMZASnAQJZy5",
        "colab_type": "code",
        "outputId": "1d8c54b2-3842-45a3-c4db-a58168e8c3ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "# epoch in range(20, 30)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[30,  3125] Train loss: 6.761\n",
            "[30,  3125] Train KL: 4.021\n",
            "[30,  3125] Train RE: 2.740\n",
            "[30,  6250] Train loss: 6.705\n",
            "[30,  6250] Train KL: 4.014\n",
            "[30,  6250] Train RE: 2.691\n",
            "[30,  9375] Train loss: 6.983\n",
            "[30,  9375] Train KL: 4.012\n",
            "[30,  9375] Train RE: 2.971\n",
            "[30, 12500] Train loss: 7.126\n",
            "[30, 12500] Train KL: 4.009\n",
            "[30, 12500] Train RE: 3.117\n",
            "[30,  1248] Validation loss: 7.578\n",
            "[30,  1248] Validation KL: 4.031\n",
            "[30,  1248] Validation RE: 3.547\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG52hKkUL-uv",
        "colab_type": "code",
        "outputId": "d0efca5e-ac76-4189-8dc4-efbe2656f2f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# epoch in range(30, 40)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stopping early at epoch 32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNMSN27ENPZP",
        "colab_type": "code",
        "outputId": "ca7bf221-241c-4b20-adcd-6c1cc22c1dca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "test(31, False)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[32,  1249] Test loss: 11.532\n",
            "[32,  1249] Test KL: 4.030\n",
            "[32,  1249] Test RE: 7.503\n",
            "An old man in a blue shirt , holding a white shirt and smiles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(11.5324, device='cuda:0', grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHI2e6BCNiuw",
        "colab_type": "code",
        "outputId": "a1c761f2-b2d6-4908-f930-96f1d24af598",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "interpolate(latent_size)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "People are playing with a ball in a field of a large crowd\n",
            "Two children are playing with a toy in a blue and white shirt\n",
            "Two girls are playing with a toy in a blue and white shirt\n",
            "Two children are playing with a toy in a field of a tree\n",
            "Two children are standing in front of a large building , and one\n",
            "The children are standing in front of a large building , and smiling\n",
            "The children are standing in front of a large building , and one\n",
            "The children are standing in front of a large building , and one\n",
            "The children are standing in front of a large building , with a\n",
            "The boy is standing in front of a large building with a hose\n",
            "The boy is standing in front of a large building with a hose\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2bD3VS1UVJ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "5331b31f-1272-438d-9f9e-98ca699445be"
      },
      "source": [
        "interpolate(latent_size)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A woman in a white shirt is holding a baby in her hand\n",
            "A young girl in a pink dress is playing with a soccer ball\n",
            "A young girl in a pink shirt is playing with a soccer ball\n",
            "A young girl in a pink shirt is playing with a soccer ball\n",
            "A young girl in a red shirt is playing with a soccer ball\n",
            "A young boy in a red shirt is playing with a soccer ball\n",
            "A group of people are playing a game of a crowd of people\n",
            "A group of people are playing with a red ball in the background\n",
            "The two men are wearing a red shirt , and white , and\n",
            "The two men are wearing a red shirt , and white , and\n",
            "The two dogs are playing in the snow , wearing a red shirt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejh5KCP5U6Oi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "fcdd983c-e73d-48cf-e631-57d702c334b6"
      },
      "source": [
        "interpolate(latent_size)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A woman in a blue shirt is walking down a street with flowers\n",
            "A woman in a blue shirt is walking down a street with flowers\n",
            "A woman in a blue shirt is walking down a street , talking\n",
            "A woman in a blue shirt is walking down a street , talking\n",
            "A woman in a blue shirt is walking down a street , talking\n",
            "A woman in a blue shirt is walking down a street , talking\n",
            "A woman in a blue shirt is walking down a street , talking\n",
            "A man in a blue shirt is walking down a street , talking\n",
            "A man is sitting on a bench , looking at a table .\n",
            "A man is sitting on a bench , looking at a table .\n",
            "A man is sitting on a bench , looking at a table .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RPoJql3WeOu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "02ba13f2-6388-4c4c-8eb9-0c280659c26e"
      },
      "source": [
        "interpolate(latent_size)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A man is standing in a chair , holding a large white bag\n",
            "A man is standing in a chair , holding a large white bag\n",
            "A man in a blue shirt is standing in a large tree area\n",
            "A man in a blue shirt is standing in a large city area\n",
            "A man in a blue shirt is standing in a large city area\n",
            "A man in a blue shirt is standing in a large city area\n",
            "A man in a blue shirt is standing in a large city area\n",
            "A man in a blue shirt is standing in a large city area\n",
            "A man in a blue shirt is standing in a large city area\n",
            "A man in a blue shirt is standing in front of a building\n",
            "A man in a blue shirt is standing in front of a building\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bS4rxRPXXUQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "2b89f50f-8366-4aec-829a-9369a647e021"
      },
      "source": [
        "interpolate(latent_size)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A man is playing a guitar with a man in a blue shirt\n",
            "A man is playing a guitar with a man in a blue shirt\n",
            "A man is playing a guitar with a man in a blue shirt\n",
            "A man is playing a guitar with a man in a blue shirt\n",
            "A man is standing in front of a crowd of people , talking\n",
            "The man is in a kitchen , holding a <unk>lowercase , is holding\n",
            "The woman is in a kitchen , holding a <unk>lowercase , is holding\n",
            "The woman is in a kitchen , holding a red shirt\n",
            "The woman is in a kitchen , holding a red shirt\n",
            "The woman is in a kitchen , reading a book in the background\n",
            "The woman is in a kitchen with a man in a white shirt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXpN3zx9krjD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "# Training\n",
        "f = open('./snli_1.0/snli_1.0_train.txt')\n",
        "contents = f.read()\n",
        "file_as_list = contents.splitlines()\n",
        "sentences = []\n",
        "for example in file_as_list[1:]:\n",
        "    sentence1 = example.split('\\t')[5]\n",
        "    sentence2 = example.split('\\t')[6]\n",
        "    sent1_len = len(sentence1.split())\n",
        "    sent2_len = len(sentence2.split())\n",
        "    if sent1_len <= 12: sentences.append(sentence1)\n",
        "    if sent2_len <= 12: sentences.append(sentence2)\n",
        "\n",
        "sample = random.sample(sentences, 50000)\n",
        "\n",
        "f = open('./snli_1.0/sample_train.txt', \"w\")\n",
        "for sentence in sample:\n",
        "    f.write(sentence)\n",
        "    f.write(\"\\n\")\n",
        "f.close()\n",
        "\n",
        "# Development\n",
        "f = open('./snli_1.0/snli_1.0_dev.txt')\n",
        "contents = f.read()\n",
        "file_as_list = contents.splitlines()\n",
        "sentences = []\n",
        "for example in file_as_list[1:]:\n",
        "    sentence1 = example.split('\\t')[5]\n",
        "    sentence2 = example.split('\\t')[6]\n",
        "    sent1_len = len(sentence1.split())\n",
        "    sent2_len = len(sentence2.split())\n",
        "    if sent1_len <= 12: sentences.append(sentence1)\n",
        "    if sent2_len <= 12: sentences.append(sentence2)\n",
        "\n",
        "sample = random.sample(sentences, 5000)\n",
        "\n",
        "f = open('./snli_1.0/sample_dev.txt', \"w\")\n",
        "for sentence in sample:\n",
        "    f.write(sentence)\n",
        "    f.write(\"\\n\")\n",
        "f.close()\n",
        "\n",
        "# Test\n",
        "f = open('./snli_1.0/snli_1.0_test.txt')\n",
        "contents = f.read()\n",
        "file_as_list = contents.splitlines()\n",
        "sentences = []\n",
        "for example in file_as_list[1:]:\n",
        "    sentence1 = example.split('\\t')[5]\n",
        "    sentence2 = example.split('\\t')[6]\n",
        "    sent1_len = len(sentence1.split())\n",
        "    sent2_len = len(sentence2.split())\n",
        "    if sent1_len <= 12: sentences.append(sentence1)\n",
        "    if sent2_len <= 12: sentences.append(sentence2)\n",
        "\n",
        "sample = random.sample(sentences, 5000)\n",
        "\n",
        "f = open('./snli_1.0/sample_test.txt', \"w\")\n",
        "for sentence in sample:\n",
        "    f.write(sentence)\n",
        "    f.write(\"\\n\")\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX-c-c28_wBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_save_name = 'pretraining.pt'\n",
        "path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "torch.save({\n",
        "    'model':model.state_dict(), \n",
        "    'encoder':encoder.state_dict(), \n",
        "    'decoder':decoder.state_dict(), \n",
        "    'optimizer_e':optimizer_e.state_dict(), \n",
        "    'optimizer_d':optimizer_d.state_dict()\n",
        "    }, \n",
        "    path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZGPHtjYN4ba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_save_name = 'free_bits.pt'\n",
        "path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "torch.save({\n",
        "    'model':model.state_dict(), \n",
        "    'encoder':encoder.state_dict(), \n",
        "    'decoder':decoder.state_dict(), \n",
        "    'optimizer_e':optimizer_e.state_dict(), \n",
        "    'optimizer_d':optimizer_d.state_dict()\n",
        "    }, \n",
        "    path)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}