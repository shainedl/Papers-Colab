{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Posterior_Collapse.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shainedl/Papers-Colab/blob/master/Posterior_Collapse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4Mg5TYiaI7D",
        "colab_type": "code",
        "outputId": "9686abc7-6af7-44b5-8965-58a45c256db4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from google.colab import files\n",
        "from collections import defaultdict\n",
        "from itertools import count, chain\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import numpy as np\n",
        "import pdb\n",
        "import re"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU1ksCbwaOjT",
        "colab_type": "code",
        "outputId": "0a52eb04-168d-4812-d8ae-380efb1426ca",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "uploaded_training = files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3548c337-063a-4051-8188-3ff2a998e5a0\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-3548c337-063a-4051-8188-3ff2a998e5a0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving sample_train.txt to sample_train.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_3MgQC5aVpG",
        "colab_type": "code",
        "outputId": "c9655a4f-4593-4412-a509-8e30b238d43f",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "uploaded_val = files.upload()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bbdc44f5-8ca5-43ea-9396-54a39af451a0\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-bbdc44f5-8ca5-43ea-9396-54a39af451a0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving sample_dev.txt to sample_dev.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygvyxibDahEP",
        "colab_type": "code",
        "outputId": "ad746b12-d1e8-416b-bba1-fc93ed0d2e35",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "uploaded_test = files.upload()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-13836852-b3a7-493c-acd8-a804005ff21c\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-13836852-b3a7-493c-acd8-a804005ff21c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving sample_test.txt to sample_test.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR1DMUXiammS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(file):\n",
        "  \"\"\"\n",
        "  Load training data and output vocabulary dictionaries\n",
        "  \"\"\"\n",
        "  w2i = defaultdict(lambda x=count(0): next(x))\n",
        "  w2i[\"<s>\"] \n",
        "  w2i[\"</s>\"] \n",
        "  w2i[\"<unk>\"] \n",
        "  w2i['<unk>twoDigitNum']\n",
        "  w2i['<unk>fourDigitNum']\n",
        "  w2i['<unk>containsDigitAndAlpha']\n",
        "  w2i['<unk>containsDigitAndDash']\n",
        "  w2i['<unk>containsDigitAndSlash']\n",
        "  w2i['<unk>containsDigitAndComma']\n",
        "  w2i['<unk>containsDigitAndPeriod']\n",
        "  w2i['<unk>othernum']\n",
        "  w2i['<unk>allCaps']\n",
        "  w2i['<unk>initCap']\n",
        "  w2i['<unk>lowercase']\n",
        "  w2i['<unk>time']\n",
        "  data = []\n",
        "  file = file.decode('utf-8')\n",
        "  sentences = file.splitlines()\n",
        "\n",
        "  for sentence in sentences:\n",
        "    tokens = word_tokenize(sentence)\n",
        "    for token in tokens:\n",
        "      w2i[token]\n",
        "    data.append(tokens)\n",
        "\n",
        "  freq_dist = nltk.FreqDist([item for sublist in data for item in sublist])\n",
        "  freq1 = set(list(freq_dist.keys())[-4000:])\n",
        "\n",
        "  w2i = dict(w2i)\n",
        "  for key in list(w2i.keys()):\n",
        "    if key in freq1:\n",
        "      w2i.pop(key)\n",
        "  i2w = {i:w for w,i in w2i.items()}\n",
        "\n",
        "  return data, w2i, i2w, freq_dist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqelmnoMayFH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_data, w2i, i2w, freq_dist = load_data(uploaded_training['sample_train.txt'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuOoLPZEaz_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data_test(file):\n",
        "  \"\"\"\n",
        "  Load test and validation data\n",
        "  \"\"\"\n",
        "  data = []\n",
        "  file = file.decode('utf-8')\n",
        "  sentences = file.splitlines()\n",
        "\n",
        "  for sentence in sentences:\n",
        "    tokens = word_tokenize(sentence)\n",
        "    data.append(tokens)\n",
        "    \n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSAbigBra1Tw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_data = load_data_test(uploaded_val['sample_dev.txt'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRPrdosha2jJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = load_data_test(uploaded_test['sample_test.txt'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaRH-yBsa3eJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_sequence(seq, to_ix):\n",
        "  idxs = [to_ix[\"<s>\"]]\n",
        "  for w in seq:\n",
        "    if w in to_ix:\n",
        "      idxs.append(to_ix[w])\n",
        "    else:\n",
        "      unk = get_word_class(w)\n",
        "      idxs.append(to_ix[unk])\n",
        "  idxs.append(to_ix[\"</s>\"])\n",
        "  return torch.tensor(idxs, dtype=torch.long)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-o0z6Faa40z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_word_class(x):\n",
        "    \"\"\"\n",
        "    Get fword class for a given word.\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : str\n",
        "        word to be replaced\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        corresponding word class\n",
        "    \"\"\"\n",
        "    if re.fullmatch(r'[0-9]{2}', x):\n",
        "        return '<unk>twoDigitNum'\n",
        "    elif re.fullmatch(r'[0-9]{4}', x):\n",
        "        return '<unk>fourDigitNum'\n",
        "    elif re.fullmatch(r'A[0-9\\-]+', x):\n",
        "        return '<unk>containsDigitAndAlpha'\n",
        "    elif re.fullmatch(r'[0-9]+\\-[0-9]+', x):\n",
        "        return '<unk>containsDigitAndDash'\n",
        "    elif re.fullmatch(r'[0-9]+/[0-9]+/[0-9]+', x):\n",
        "        return '<unk>containsDigitAndSlash'\n",
        "    elif re.fullmatch(r'([0-9]+,[0-9]+)+\\.[0-9]+', x):\n",
        "        return '<unk>containsDigitAndComma'\n",
        "    elif re.fullmatch(r'[0-9]+\\.[0-9]+', x):\n",
        "        return '<unk>containsDigitAndPeriod'\n",
        "    elif re.fullmatch(r'[0-9]+', x):\n",
        "        return '<unk>othernum'\n",
        "    elif re.fullmatch(r'[A-Z]+', x):\n",
        "        return '<unk>allCaps'\n",
        "    elif re.fullmatch(r'[A-Z][a-z]+', x):\n",
        "        return '<unk>initCap'\n",
        "    elif re.fullmatch(r'[a-z]+', x):\n",
        "        return '<unk>lowercase'\n",
        "    elif re.match(r'[0-9]+:[0-9]+', x):\n",
        "        return '<unk>time'\n",
        "\n",
        "    return '<unk>'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYbg1MLFa6Z7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_data(batch_size, data, w2i):\n",
        "  \"\"\"\n",
        "  Batches data with sequences of the same length\n",
        "  \"\"\"\n",
        "  sentence_lengths = np.array([len(sentence) for sentence in data])\n",
        "  sorted_idx = np.argsort(sentence_lengths)\n",
        "  sorted_lengths = sentence_lengths[sorted_idx]\n",
        "\n",
        "  len_increase_idx = []\n",
        "  for i in range(1, len(sorted_lengths)):\n",
        "    if sorted_lengths[i] > sorted_lengths[i-1]:\n",
        "      len_increase_idx.append(i)\n",
        "  len_increase_idx.append(len(sorted_lengths))\n",
        "\n",
        "  batch_data = []\n",
        "  curr_idx = 0\n",
        "  for i, idx in enumerate(len_increase_idx):\n",
        "    while curr_idx < idx:\n",
        "      batch_sentences = []\n",
        "      new_idx = min(curr_idx + batch_size, idx)\n",
        "      for i in range(curr_idx, new_idx):\n",
        "        sent_to_vec = prepare_sequence(data[sorted_idx[i]], w2i)\n",
        "        batch_sentences.append(sent_to_vec)\n",
        "      curr_idx = new_idx\n",
        "      batch_sentences = torch.stack(batch_sentences).to(device=cuda)\n",
        "      batch_data.append(batch_sentences)\n",
        "\n",
        "  i = 0\n",
        "  j = len(batch_data)\n",
        "  while i < j:\n",
        "    if i != 0 and len(batch_data[i]) <= 2 and len(batch_data[i][0]) == len(batch_data[i-1][0]):\n",
        "      batch_data.append(torch.cat((batch_data[i], batch_data[i-1])))\n",
        "      batch_data.pop(i)\n",
        "      batch_data.pop(i-1)\n",
        "      i -= 1\n",
        "      j = len(batch_data)\n",
        "    elif len(batch_data[i]) == 1:\n",
        "      batch_data.pop(i)\n",
        "      j = len(batch_data)\n",
        "    else:\n",
        "      i += 1\n",
        "\n",
        "  return batch_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPzqSwHIa8a4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hO_2lWKa9zi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_cuda = True\n",
        "cuda = torch.device('cuda')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKZ1Kuj0a_PW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_training = batch_data(batch_size, training_data, w2i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOF1ngl1bAQj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_val = batch_data(batch_size, val_data, w2i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPUl_RdxbBMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_test = batch_data(batch_size, test_data, w2i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiqQIDEZbDDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_size, hidden_size, latent_size):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.embeddings = nn.Embedding(num_embeddings=vocab_size,\n",
        "                                   embedding_dim=embedding_size)\n",
        "    \n",
        "    self.rnn = nn.LSTM(input_size=embedding_size,\n",
        "                        hidden_size=hidden_size,\n",
        "                        batch_first=True)\n",
        "    \n",
        "    self.fc_mu = nn.Linear(hidden_size, latent_size)\n",
        "    self.fc_var = nn.Linear(hidden_size, latent_size)\n",
        "\n",
        "    self._initialize_parameters([-0.01, 0.01], [-0.1, 0.1]) \n",
        "\n",
        "  def encode(self, x):\n",
        "    \"\"\"\n",
        "    Produces a Gaussian distribution over the possible values of the code z \n",
        "    from which x could have been generated\n",
        "        \n",
        "    Parameters\n",
        "    ----------\n",
        "      x: batch size x sequence length Tensor\n",
        "        observed data\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "      mu: batch size x latent size Tensor\n",
        "        mean of Gaussian distribution\n",
        "        \n",
        "      logvar: batch size x latent size Tensor\n",
        "        log of variance of Gaussian distribution     \n",
        "    \"\"\"\n",
        "    x = self.embeddings(x)\n",
        "    outputs, (hidden, cell) = self.rnn(x)\n",
        "    mu = self.fc_mu(hidden)\n",
        "    logvar = self.fc_var(hidden)\n",
        "    mu = mu.squeeze()\n",
        "    logvar = logvar.squeeze()\n",
        "    return mu, logvar \n",
        "\n",
        "  def _initialize_parameters(self, lstm_init, embed_init):\n",
        "    \"\"\"\n",
        "    Initializes the LSTM parameters and embeddings with uniform distributions\n",
        "    \"\"\"\n",
        "    for param in self.parameters():\n",
        "      nn.init.uniform_(param, a=lstm_init[0], b=lstm_init[1])\n",
        "    nn.init.uniform_(self.embeddings.weight, a=embed_init[0], b=embed_init[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp6lE4_XbEQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_size, hidden_size, latent_size):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.embeddings = nn.Embedding(num_embeddings=vocab_size,\n",
        "                                   embedding_dim=embedding_size)\n",
        "    \n",
        "    self.rnn = nn.LSTM(input_size=embedding_size + latent_size,\n",
        "                        hidden_size=hidden_size,\n",
        "                        batch_first=True)    \n",
        "\n",
        "    self.fc_hid = nn.Linear(latent_size, hidden_size, bias=False)\n",
        "    self.fc_voc = nn.Linear(hidden_size, vocab_size, bias=False)\n",
        "\n",
        "    self.dropout = nn.Dropout()\n",
        "    self._initialize_parameters([-0.01, 0.01], [-0.1, 0.1])\n",
        "\n",
        "  def decode(self, z, inputs):\n",
        "    \"\"\"\n",
        "    Given a code z it produces unscaled output corresponding to the vocabulary\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "      z: batch size x latent size Tensor\n",
        "        latent variables\n",
        "      \n",
        "      inputs: batch size x sequence length Tensor\n",
        "        source sequence\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "      output_logits: batch size x sequence length x vocab size Tensor\n",
        "        unscaled output\n",
        "    \"\"\"\n",
        "    cell = self.fc_hid(z)\n",
        "    cell = cell.unsqueeze(0)\n",
        "    hidden = torch.tanh(cell)\n",
        "\n",
        "    embed = self.embeddings(inputs)\n",
        "    embed = self.dropout(embed)\n",
        "    z = z.expand(embed.size(1), z.size(0), z.size(1))\n",
        "    z = z.transpose(1,0)\n",
        "    embed_lat = torch.cat((embed, z), 2)\n",
        "\n",
        "    outputs, (hidden, cell) = self.rnn(embed_lat, (hidden, cell))\n",
        "    outputs = self.dropout(outputs)\n",
        "    output_logits = self.fc_voc(outputs)\n",
        "    return output_logits\n",
        "\n",
        "  def decode_greedy(self, z, inputs, interpolation=False):\n",
        "    \"\"\"\n",
        "    Given a code z it produces unscaled output corresponding to the vocabulary\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "      z: batch size x latent size Tensor\n",
        "        latent variables\n",
        "      \n",
        "      inputs: batch size x sequence length Tensor\n",
        "        source sequence\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "      output_logits: batch size x sequence length x vocab size Tensor\n",
        "        unscaled output\n",
        "      \n",
        "      batch_decoded: batch size x output sequence length list\n",
        "        decoded output sequence\n",
        "    \"\"\"\n",
        "    cell = self.fc_hid(z)\n",
        "    cell = cell.unsqueeze(0)\n",
        "    hidden = torch.tanh(cell)\n",
        "\n",
        "    batch_size = inputs.size(0)\n",
        "    input_d = inputs[:,0]\n",
        "    output_logit_prev = None\n",
        "    seq_len = inputs.size(1)\n",
        "    batch_decoded = [[] for j in range(batch_size)]\n",
        "\n",
        "    end_mask = torch.ones(batch_size)\n",
        "    counter = 0\n",
        "    while end_mask.sum() != 0 and counter < seq_len:\n",
        "      embed = self.embeddings(input_d)\n",
        "      embed_lat = torch.cat((embed, z), 1)\n",
        "      embed_lat = embed_lat.unsqueeze(1)\n",
        "      outputs, (hidden, cell) = self.rnn(embed_lat, (hidden, cell))  \n",
        "      output_logit = self.fc_voc(outputs)\n",
        "      if output_logit_prev is not None:\n",
        "        output_logits = torch.cat((output_logit_prev, output_logit), dim=1)\n",
        "        output_logit_prev = output_logits\n",
        "      else:\n",
        "        output_logit_prev = output_logit\n",
        "      input_d = torch.argmax(output_logit, dim=2).flatten()\n",
        "\n",
        "      for k in range(batch_size):\n",
        "        if end_mask[k] != 0:\n",
        "          if interpolation and input_d[k].item() == w2i[\"</s>\"] :\n",
        "            end_mask[k] = 0\n",
        "          else:\n",
        "            token = i2w[input_d[k].item()]\n",
        "            batch_decoded[k].append(token)\n",
        "      counter += 1\n",
        "    \n",
        "    return output_logits, batch_decoded\n",
        "\n",
        "  def _initialize_parameters(self, lstm_init, embed_init):\n",
        "    \"\"\"\n",
        "    Initializes the LSTM parameters and embeddings with uniform distributions\n",
        "    \"\"\"\n",
        "    for param in self.parameters():\n",
        "      nn.init.uniform_(param, a=lstm_init[0], b=lstm_init[1])\n",
        "    nn.init.uniform_(self.embeddings.weight, a=embed_init[0], b=embed_init[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FG9GSLxPbFuT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VAE(nn.Module):\n",
        "\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super(VAE, self).__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.re_loss = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "  def forward(self, x, greedy=False):\n",
        "    \"\"\"\n",
        "    Forward pass of the model \n",
        "    \"\"\"\n",
        "    mu, logvar = self.encoder.encode(x)\n",
        "    kl = self.get_kl(mu, logvar)\n",
        "    z = self._reparameterize(mu, logvar)\n",
        "\n",
        "    source = x[:,:-1]\n",
        "    target = x[:, 1:]\n",
        "    if greedy:\n",
        "      output_logits, batch_decoded = self.decoder.decode_greedy(z, source)\n",
        "      re = self.get_reconstruction_error(output_logits, target)\n",
        "      return kl, re, batch_decoded\n",
        "    else: \n",
        "      output_logits = self.decoder.decode(z, source)\n",
        "      re = self.get_reconstruction_error(output_logits, target)\n",
        "      return kl, re\n",
        "\n",
        "  def _reparameterize(self, mu, logvar):\n",
        "    \"\"\"\n",
        "    Reparameterize the random variable z to express as a deterministic variable\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "      mu: batch size x latent size Tensor\n",
        "        mean of Gaussian distribution\n",
        "        \n",
        "      logvar: batch size x latent size Tensor\n",
        "        log of variance of Gaussian distribution     \n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "      z: batch size x latent size Tensor\n",
        "        reparameterization of latent variables\n",
        "    \"\"\"\n",
        "    std = torch.exp(logvar / 2)\n",
        "    eps = torch.randn_like(std)\n",
        "    return mu + std * eps  \n",
        "\n",
        "  def get_kl(self, mu, logvar):\n",
        "    \"\"\"\n",
        "    Returns the KLD between posterior and prior\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "      mu: batch size x latent size Tensor\n",
        "        mean of Gaussian distribution\n",
        "        \n",
        "      logvar: batch size x latent size Tensor\n",
        "        log of variance of Gaussian distribution\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "      kl: batch size x latent size Tensor\n",
        "        kl divergence\n",
        "    \"\"\"\n",
        "    return (mu**2 + logvar.exp() - 1 - logvar) / 2\n",
        "\n",
        "  def get_reconstruction_error(self, output_logits, target):\n",
        "    \"\"\"\n",
        "    Returns the reconstruction error\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "      output_logits: batch size x sequence length x vocab size Tensor\n",
        "        unscaled output\n",
        "      \n",
        "      target: batch size x sequence length Tensor\n",
        "        target sequence\n",
        "    \"\"\"\n",
        "    target = target.contiguous().view(-1)\n",
        "    output_logits = output_logits.view(-1, output_logits.size(2))\n",
        "    return self.re_loss(output_logits, target)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJRMfmJIbHcb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(w2i)\n",
        "embedding_size = 128\n",
        "hidden_size = 512 \n",
        "latent_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBa3KOvZbI7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print_every = round(len(batch_training) / batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SD2w4eBnbNWF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecayLearning:\n",
        "  \"\"\"\n",
        "  Class updated from https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
        "  \"\"\"\n",
        "  def __init__(self, patience=2):\n",
        "    self.patience = patience\n",
        "    self.counter = 0\n",
        "    self.best_score = None\n",
        "    self.update_lr = False\n",
        "\n",
        "  def __call__(self, val_loss):\n",
        "    score = -val_loss\n",
        "\n",
        "    if self.best_score is None:\n",
        "      self.best_score = score\n",
        "    elif score < self.best_score:\n",
        "      self.counter += 1\n",
        "      if self.counter >= self.patience:\n",
        "        self.update_lr = True\n",
        "    else:\n",
        "      self.best_score = score\n",
        "      self.counter = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpZjLw7-hUz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def interpolate(latent_size, sample1, sample2):\n",
        "  \n",
        "  for w in range(11):\n",
        "    weight = w * 0.1\n",
        "    sample = weight * sample2 + (1-weight) * sample1\n",
        "    _, batch_decoded = decoder.decode_greedy(sample, torch.zeros(13, dtype=torch.long, device=cuda).unsqueeze(0), True)\n",
        "    print(*batch_decoded[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS9bGdlsb5YH",
        "colab_type": "text"
      },
      "source": [
        "Pre-training on Autoencoder objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saVwTKzDc3eq",
        "colab_type": "text"
      },
      "source": [
        "Pre-training ran for 3 hours"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGrbRCtRbKgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epoch):\n",
        "  \"\"\"\n",
        "  Trains the model\n",
        "  \"\"\"\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  for batch_idx, data in enumerate(batch_training):\n",
        "    optimizer_e.zero_grad()\n",
        "    optimizer_d.zero_grad()\n",
        "    re = model(data)\n",
        "    loss = re\n",
        "    loss.backward()\n",
        "    optimizer_e.step()\n",
        "    optimizer_d.step()\n",
        "\n",
        "    running_loss += loss\n",
        "  \n",
        "    if (epoch == 0 or epoch % 10 == 9) and batch_idx % print_every == print_every-1:    \n",
        "      print('[%d, %5d] Train loss: %.3f' % (epoch + 1, batch_idx + 1, running_loss / print_every))\n",
        "      running_loss = 0.0 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiHxkIzObLqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(epoch, validation=False):\n",
        "  \"\"\"\n",
        "  Run the model on validation or test dataset\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  anneal = 0.1 * epoch if epoch < 10 else 1.0\n",
        "\n",
        "  data = batch_val if validation else batch_test\n",
        "  running_loss = 0.0\n",
        "  for batch_idx, data in enumerate(data):\n",
        "    if validation:  \n",
        "      re = model(data)\n",
        "    else:\n",
        "      re, batch_decoded = model(data, True)\n",
        "    loss = re\n",
        "\n",
        "    running_loss += loss\n",
        "\n",
        "  avg_loss = running_loss / (batch_idx + 1)\n",
        "  if validation and (epoch == 0 or epoch % 10 == 9):\n",
        "    print('[%d, %5d] Validation loss: %.3f' % (epoch + 1, batch_idx + 1, avg_loss))\n",
        "  else:\n",
        "    print('[%d, %5d] Test loss: %.3f' % (epoch + 1, batch_idx + 1, avg_loss))\n",
        "    print(*batch_decoded[0])\n",
        "\n",
        "  return avg_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dxpRu1LbSaA",
        "colab_type": "code",
        "outputId": "430fd20c-33aa-4954-ed8c-9194c6e04c56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "encoder = Encoder(vocab_size, embedding_size, hidden_size, latent_size)\n",
        "decoder = Decoder(vocab_size, embedding_size, hidden_size, latent_size)\n",
        "model = VAE(encoder, decoder)\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "  \n",
        "lr = 0.5\n",
        "optimizer_e = optim.SGD(encoder.parameters(), lr=lr)\n",
        "optimizer_d = optim.SGD(decoder.parameters(), lr=lr)\n",
        "\n",
        "decay_learning = DecayLearning()\n",
        "num_decays = 0\n",
        "early_stop = 5\n",
        "\n",
        "for epoch in range(10):\n",
        "  train(epoch)\n",
        "  val_loss = test(epoch, True)\n",
        "  decay_learning(val_loss)\n",
        "  if decay_learning.update_lr:\n",
        "    lr *= 0.5\n",
        "    num_decays += 1\n",
        "    if num_decays == early_stop + 1:\n",
        "      print(\"Stopping early at epoch\", epoch+1)\n",
        "      break\n",
        "    print(\"Learning rate has been decayed to\", lr, \"at epoch\", epoch+1)\n",
        "    optimizer_e = optim.SGD(encoder.parameters(), lr=lr)\n",
        "    optimizer_d = optim.SGD(decoder.parameters(), lr=lr)\n",
        "    decay_learning = DecayLearning()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  3125] Train loss: 3.947\n",
            "[1,  6250] Train loss: 3.595\n",
            "[1,  9375] Train loss: 3.789\n",
            "[1, 12500] Train loss: 3.809\n",
            "[1,  1248] Validation loss: 4.795\n",
            "Learning rate has been decayed to 0.25 at epoch 4\n",
            "[10,  3125] Train loss: 1.061\n",
            "[10,  6250] Train loss: 1.135\n",
            "[10,  9375] Train loss: 1.453\n",
            "[10, 12500] Train loss: 1.745\n",
            "[10,  1248] Validation loss: 2.527\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV6lmdwQhxKH",
        "colab_type": "code",
        "outputId": "da67386e-9564-4c69-9ce6-957550bd2cd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "# epoch in range(10,20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate has been decayed to 0.125 at epoch 17\n",
            "[20,  3125] Train loss: 0.276\n",
            "[20,  6250] Train loss: 0.382\n",
            "[20,  9375] Train loss: 0.614\n",
            "[20, 12500] Train loss: 0.917\n",
            "[20,  1248] Validation loss: 2.240\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHlKxzZ7pDqn",
        "colab_type": "code",
        "outputId": "3000eacc-df6a-417e-9e30-105968b243e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "# epoch in range(20,30)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate has been decayed to 0.0625 at epoch 23\n",
            "[30,  3125] Train loss: 0.136\n",
            "[30,  6250] Train loss: 0.214\n",
            "[30,  9375] Train loss: 0.391\n",
            "[30, 12500] Train loss: 0.666\n",
            "[30,  1248] Validation loss: 1.204\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fFbie1ywMoz",
        "colab_type": "code",
        "outputId": "da4ec794-681d-4ad6-d1af-b5323f6fcb0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "# epoch in range(30,40)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate has been decayed to 0.03125 at epoch 32\n",
            "[40,  3125] Train loss: 0.096\n",
            "[40,  6250] Train loss: 0.158\n",
            "[40,  9375] Train loss: 0.311\n",
            "[40, 12500] Train loss: 0.554\n",
            "[40,  1248] Validation loss: 0.786\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tafSVIN829PI",
        "colab_type": "code",
        "outputId": "ec59506a-1b7d-47e1-8b9e-0ba83a109ee6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "# epoch in range(40,50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate has been decayed to 0.015625 at epoch 43\n",
            "[50,  3125] Train loss: 0.081\n",
            "[50,  6250] Train loss: 0.137\n",
            "[50,  9375] Train loss: 0.273\n",
            "[50, 12500] Train loss: 0.504\n",
            "[50,  1248] Validation loss: 0.678\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMU44-dfUYzs",
        "colab_type": "code",
        "outputId": "de7ae0fd-64cc-4484-f375-dd0de2e1c65b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# epoch in range(50,60):"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stopping early at epoch 58\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NrS8irLaxYZ",
        "colab_type": "code",
        "outputId": "372b56b5-0b1a-4bc4-dd97-0df85fcc570f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "test(58, False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[59,  1249] Test loss: 1.142\n",
            "A caucasian wearing a green jacket and a hat and yellow shirts . </s>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.1421, device='cuda:0', grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRqTKdLFhWQr",
        "colab_type": "code",
        "outputId": "6655ca35-900a-4ea5-a9a0-54926debc4c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "interpolate(latent_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "friends are celebrating a white and one on another shirt\n",
            "the children were a yellow dress and riding an object\n",
            "the children were wearing yellow and shoes with a helmet .\n",
            "the children have a long , green on and no .\n",
            "the children chews and riding and have on another , outside\n",
            "An animal was doing karate and wearing no and sandals outside .\n",
            "An artist was balancing and boots at a small dog vendor .\n",
            "An artist was balancing and vegetables at another boy and shovel .\n",
            "A mother was selling vegetables and on crafts and vegetables together .\n",
            "A mother does not seen and at something , cooking flowers .\n",
            "A blond man shoots wine while wearing no hat and clothes .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7guNuHe3hp50",
        "colab_type": "code",
        "outputId": "7af06093-3fd0-4a18-b5fd-0bca66e7e5cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "interpolate(latent_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<unk>initCap , people fight\n",
            "<unk>initCap , people in dogs are running\n",
            "<unk>initCap , people in snow are looking\n",
            "Three firemen , a car are running across the car .\n",
            "Three firemen , a car are running across the car .\n",
            "Three firemen are a hot dog looking along the ocean .\n",
            "Three firemen are holding two dogs along along <unk>lowercase .\n",
            "Two firemen are holding a bus back along the sand .\n",
            "Two sheep are having two fishing walking across the .\n",
            "Two sheep are having two fishing going across leaves .\n",
            "Two sheep are having two fishing to the left .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9K29slQhvnl",
        "colab_type": "code",
        "outputId": "d23556fa-3935-454a-a2df-8b66eb66d8e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "interpolate(latent_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "some tourists run\n",
            "some actors has work\n",
            "some actors has work while look\n",
            "some spectators on opposing people are tired .\n",
            "two guys of flowers are posing while both\n",
            "two guys in helmets are posing , waits outside and .\n",
            "two guys in helmets are posing , waits outside and .\n",
            "Three guys in uniform are posing while both outside .\n",
            "Three guys in pink clothes are posing while holding bags .\n",
            "Two guys in pink uniforms are running , outside .\n",
            "Two guys in pink uniforms are running outside and outside .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD6Uqmt3hxes",
        "colab_type": "code",
        "outputId": "44ad237d-ac7c-47fa-f18f-1fbfa360fd14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "interpolate(latent_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This water rides through train tracks , his dog 's wings .\n",
            "a bucket from <unk>lowercase of pigeons , while his feet is life\n",
            "a bucket from <unk>lowercase of people because she was all in her\n",
            "a light boat of some passengers while wearing shorts 's mouth .\n",
            "a lone boat full of several people plays an white cone .\n",
            "a lone lot of some passengers while a brown car sleeps .\n",
            "a lone bicyclist riding a bucking dog and an obstacle stand .\n",
            "a bull , near a grocery store while an angel .\n",
            "a bull , riding a bucking dog outside of flowers .\n",
            "a small soldier riding a unicycle while wearing ear gear .\n",
            "a black cat carrying a flag while wearing ear gear .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgLwDbFH74qd",
        "colab_type": "text"
      },
      "source": [
        "Free Bits Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLIUepxtYkfn",
        "colab_type": "text"
      },
      "source": [
        "Free Bits ran for 1 and a half hours"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_g0gZhVfZmK_",
        "colab_type": "code",
        "outputId": "aef69d91-8023-4c92-ddbf-d49dfb705b8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGelVF5YdNWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Helpful Link: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
        "\"\"\"\n",
        "model_save_name = 'pretraining.pt'\n",
        "path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "checkpoint = torch.load(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i98KAajfUX4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epoch, target_rate=(4.0 / float(latent_size))):\n",
        "  \"\"\"\n",
        "  Trains the model\n",
        "  Helpful link for free bits: https://stats.stackexchange.com/questions/267924/explanation-of-the-free-bits-technique-for-variational-autoencoders\n",
        "  \"\"\"\n",
        "  model.train()\n",
        "  anneal = 0.1 * epoch if epoch < 10 else 1.0\n",
        "\n",
        "  running_loss = running_kl = running_re = 0.0\n",
        "  num_seq = 0\n",
        "  for batch_idx, data in enumerate(batch_training):\n",
        "    batch_size, seq_len = data.size()\n",
        "    num_seq += batch_size\n",
        "    optimizer_e.zero_grad()\n",
        "    optimizer_d.zero_grad()\n",
        "    kl, re = model(data)\n",
        "    re_mean = re.view(batch_size, -1).mean(dim=0)\n",
        "    re_loss = re_mean.sum()\n",
        "    kl_mean = kl.mean(dim=0)\n",
        "    kl_mask = (kl_mean > target_rate).float()\n",
        "    fb_mask = (kl_mean <= target_rate).float()\n",
        "    free_b = kl_mask + target_rate\n",
        "    kl_fb = (kl_mean * kl_mask + free_b * fb_mask).sum()\n",
        "    loss = kl_fb * anneal + re_loss\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
        "    optimizer_e.step()\n",
        "    optimizer_d.step()\n",
        "\n",
        "    running_kl += kl.sum()\n",
        "    running_re += re.sum()\n",
        "    running_loss += kl.sum() + re.sum()\n",
        "  \n",
        "    if (epoch == 0 or epoch % 10 == 9) and  batch_idx % print_every == print_every-1:    \n",
        "      print('[%d, %5d] Train loss: %.3f' % (epoch + 1, batch_idx + 1, running_loss / num_seq))\n",
        "      print('[%d, %5d] Train KL: %.3f' % (epoch + 1, batch_idx + 1, running_kl / num_seq))\n",
        "      print('[%d, %5d] Train RE: %.3f' % (epoch + 1, batch_idx + 1, running_re / num_seq))\n",
        "      running_loss = running_kl = running_re = 0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SnHpy3FWv9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(epoch, validation=False, target_rate = (4.0 / float(latent_size))):\n",
        "  \"\"\"\n",
        "  Run the model on validation or test dataset\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  anneal = 0.1 * epoch if epoch < 10 else 1.0\n",
        "\n",
        "  data = batch_val if validation else batch_test\n",
        "  running_loss = running_kl = running_re = 0.0\n",
        "  num_words = num_seq = 0\n",
        "  for batch_idx, data in enumerate(data):\n",
        "    if validation:  \n",
        "      kl, re = model(data)\n",
        "    else:\n",
        "      kl, re, batch_decoded = model(data, True)\n",
        "    batch_size, seq_len = data.size()\n",
        "    num_words += batch_size * (seq_len - 1)\n",
        "    num_seq += batch_size\n",
        "    running_kl += kl.sum()\n",
        "    running_re += re.sum()\n",
        "\n",
        "  avg_loss = (running_kl + running_re) / num_seq\n",
        "  avg_kl = running_kl / num_seq\n",
        "  avg_re = running_re / num_seq\n",
        "  if validation and (epoch == 0 or epoch % 10 == 9):\n",
        "    print('[%d, %5d] Validation loss: %.3f' % (epoch + 1, batch_idx + 1, avg_loss))\n",
        "    print('[%d, %5d] Validation KL: %.3f' % (epoch + 1, batch_idx + 1, avg_kl))\n",
        "    print('[%d, %5d] Validation RE: %.3f' % (epoch + 1, batch_idx + 1, avg_re))\n",
        "  elif not validation:\n",
        "    print('[%d, %5d] Test loss: %.3f' % (epoch + 1, batch_idx + 1, avg_loss))\n",
        "    print('[%d, %5d] Test KL: %.3f' % (epoch + 1, batch_idx + 1, avg_kl))\n",
        "    print('[%d, %5d] Test RE: %.3f' % (epoch + 1, batch_idx + 1, avg_re))\n",
        "    nll = (running_kl + running_re) / num_seq\n",
        "    pdb.set_trace()\n",
        "    ppl = torch.exp((running_kl + running_re) / num_words)\n",
        "    print('[%d, %5d] Test NLL: %.3f' % (epoch + 1, batch_idx + 1, nll))\n",
        "    print('[%d, %5d] Test PPL: %.3f' % (epoch + 1, batch_idx + 1, ppl))\n",
        "    print(*batch_decoded[0])\n",
        "\n",
        "  return avg_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8Ivstgze1-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(vocab_size, embedding_size, hidden_size, latent_size)\n",
        "encoder.load_state_dict(checkpoint['encoder'])\n",
        "decoder = Decoder(vocab_size, embedding_size, hidden_size, latent_size)\n",
        "model = VAE(encoder, decoder)\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAfxKMwurYID",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "outputId": "ef0aa7f8-b3b8-4482-c242-f4bc0dbdaf56"
      },
      "source": [
        "lr = 0.5\n",
        "optimizer_e = optim.SGD(encoder.parameters(), lr=lr)\n",
        "optimizer_d = optim.SGD(decoder.parameters(), lr=lr)\n",
        "decay_learning = DecayLearning()\n",
        "num_decays = 0\n",
        "early_stop = 5\n",
        "clip_grad = 5.0\n",
        "\n",
        "for epoch in range(10):\n",
        "  train(epoch)\n",
        "  val_loss = test(epoch, True)\n",
        "  decay_learning(val_loss)\n",
        "  if decay_learning.update_lr:\n",
        "    lr *= 0.5\n",
        "    num_decays += 1\n",
        "    if num_decays == early_stop + 1:\n",
        "      print(\"Stopping early at epoch\", epoch+1)\n",
        "      break\n",
        "    print(\"Learning rate has been decayed to\", lr, \"at epoch\", epoch+1)\n",
        "    optimizer_e = optim.SGD(encoder.parameters(), lr=lr)\n",
        "    optimizer_d = optim.SGD(decoder.parameters(), lr=lr)\n",
        "    decay_learning = DecayLearning()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  3125] Train loss: 127.056\n",
            "[1,  3125] Train KL: 119.090\n",
            "[1,  3125] Train RE: 7.966\n",
            "[1,  6250] Train loss: 63.078\n",
            "[1,  6250] Train KL: 56.953\n",
            "[1,  6250] Train RE: 6.125\n",
            "[1,  9375] Train loss: 43.746\n",
            "[1,  9375] Train KL: 36.464\n",
            "[1,  9375] Train RE: 7.282\n",
            "[1, 12500] Train loss: 35.303\n",
            "[1, 12500] Train KL: 27.303\n",
            "[1, 12500] Train RE: 8.000\n",
            "[1,  1248] Validation loss: 146.207\n",
            "[1,  1248] Validation KL: 120.718\n",
            "[1,  1248] Validation RE: 25.489\n",
            "[10,  3125] Train loss: 24.723\n",
            "[10,  3125] Train KL: 5.652\n",
            "[10,  3125] Train RE: 19.071\n",
            "[10,  6250] Train loss: 15.919\n",
            "[10,  6250] Train KL: 3.192\n",
            "[10,  6250] Train RE: 12.727\n",
            "[10,  9375] Train loss: 13.754\n",
            "[10,  9375] Train KL: 2.340\n",
            "[10,  9375] Train RE: 11.414\n",
            "[10, 12500] Train loss: 12.884\n",
            "[10, 12500] Train KL: 1.865\n",
            "[10, 12500] Train RE: 11.019\n",
            "[10,  1248] Validation loss: 45.523\n",
            "[10,  1248] Validation KL: 8.090\n",
            "[10,  1248] Validation RE: 37.434\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk0xcKxAycmC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "665d8c22-58bb-44b9-9a2e-447c9317734d"
      },
      "source": [
        "# epoch in range(10,20)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate has been decayed to 0.25 at epoch 13\n",
            "[20,  3125] Train loss: 21.244\n",
            "[20,  3125] Train KL: 4.252\n",
            "[20,  3125] Train RE: 16.992\n",
            "[20,  6250] Train loss: 13.835\n",
            "[20,  6250] Train KL: 2.380\n",
            "[20,  6250] Train RE: 11.454\n",
            "[20,  9375] Train loss: 12.016\n",
            "[20,  9375] Train KL: 1.683\n",
            "[20,  9375] Train RE: 10.333\n",
            "[20, 12500] Train loss: 11.349\n",
            "[20, 12500] Train KL: 1.302\n",
            "[20, 12500] Train RE: 10.046\n",
            "[20,  1248] Validation loss: 41.028\n",
            "[20,  1248] Validation KL: 5.381\n",
            "[20,  1248] Validation RE: 35.647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUfqU4Wg2G8O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "fb02df61-023b-4a9e-f1e3-67bc7b7914f9"
      },
      "source": [
        "# epoch in range(20,30):"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate has been decayed to 0.125 at epoch 22\n",
            "Learning rate has been decayed to 0.0625 at epoch 25\n",
            "[30,  3125] Train loss: 19.931\n",
            "[30,  3125] Train KL: 4.241\n",
            "[30,  3125] Train RE: 15.689\n",
            "[30,  6250] Train loss: 12.676\n",
            "[30,  6250] Train KL: 2.328\n",
            "[30,  6250] Train RE: 10.348\n",
            "[30,  9375] Train loss: 11.080\n",
            "[30,  9375] Train KL: 1.650\n",
            "[30,  9375] Train RE: 9.430\n",
            "[30, 12500] Train loss: 10.416\n",
            "[30, 12500] Train KL: 1.295\n",
            "[30, 12500] Train RE: 9.120\n",
            "[30,  1248] Validation loss: 41.414\n",
            "[30,  1248] Validation KL: 5.076\n",
            "[30,  1248] Validation RE: 36.338\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j094nqcW6uxS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "bf096a86-53ef-4fbc-8745-40ffcf5e17ee"
      },
      "source": [
        "# epoch in range(30,40):"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate has been decayed to 0.03125 at epoch 31\n",
            "Learning rate has been decayed to 0.015625 at epoch 37\n",
            "[40,  3125] Train loss: 20.112\n",
            "[40,  3125] Train KL: 4.295\n",
            "[40,  3125] Train RE: 15.816\n",
            "[40,  6250] Train loss: 12.550\n",
            "[40,  6250] Train KL: 2.330\n",
            "[40,  6250] Train RE: 10.221\n",
            "[40,  9375] Train loss: 10.884\n",
            "[40,  9375] Train KL: 1.651\n",
            "[40,  9375] Train RE: 9.233\n",
            "[40, 12500] Train loss: 10.272\n",
            "[40, 12500] Train KL: 1.297\n",
            "[40, 12500] Train RE: 8.975\n",
            "[40,  1248] Validation loss: 37.370\n",
            "[40,  1248] Validation KL: 4.883\n",
            "[40,  1248] Validation RE: 32.487\n",
            "Stopping early at epoch 40\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AL7Kg1Gi9-41",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "643d454f-a78f-412d-91d7-2c3d731ba5b4"
      },
      "source": [
        "test(39, False)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[40,  1249] Test loss: 76.540\n",
            "[40,  1249] Test KL: 4.834\n",
            "[40,  1249] Test RE: 71.705\n",
            "[40,  1249] Test NLL: 76.540\n",
            "[40,  1249] Test PPL: 3068.368\n",
            "The two men are wearing a red shirt , and white pants , smiling\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(76.5395, device='cuda:0', grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGTvrgNb_1k2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample1 = torch.randn(1, latent_size, device=cuda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXqjfi-KArNI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample2 = torch.randn(1, latent_size, device=cuda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vL1C1uTp_-io",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "05f1c551-fb13-4d72-b112-55397cf5f3cc"
      },
      "source": [
        "interpolate(latent_size, sample1, sample2)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "An old man stands in front of a crowd of people .\n",
            "An old man stands in front of a crowd of people .\n",
            "An old man stands in front of a crowd of people .\n",
            "An old man stands in front of a crowd of people .\n",
            "The man is standing in front of a crowd of people .\n",
            "The man is standing in front of a crowd of people .\n",
            "The man is standing in front of a building , wearing a suit\n",
            "The man is not wearing a red shirt , and black pants ,\n",
            "The man is wearing a red shirt , and a black hat smiles\n",
            "The man is in the kitchen , wearing a hat , and jeans\n",
            "The man is in the kitchen , while others look on the sidewalk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6gKH1ihBjiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample1 = torch.randn(1, latent_size, device=cuda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLdZtfdEBxIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample2 = torch.randn(1, latent_size, device=cuda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poRcbAaBAHsu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "77349219-bc4e-48fc-c7ca-278bb1839e77"
      },
      "source": [
        "interpolate(latent_size, sample1, sample2)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A man in a blue shirt is in front of a building .\n",
            "A man in a blue shirt is in front of a building .\n",
            "A man in a blue shirt is walking down the street .\n",
            "A man in a blue shirt is walking down a street corner .\n",
            "A man in a blue shirt is walking down a street corner .\n",
            "A man in a black shirt is walking down a street corner .\n",
            "A man in a black shirt is walking down a street corner .\n",
            "A man with a beard is walking down a street , holding a\n",
            "A man wearing a black shirt , standing in front of a building\n",
            "A man is playing guitar , and singing into a microphone .\n",
            "A man is playing guitar , and singing into a microphone .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tj4Bg5nUCUI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample1 = torch.randn(1, latent_size, device=cuda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtFK4SliCkmF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample2 = torch.randn(1, latent_size, device=cuda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAQBaLTlClhM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "53994d32-e5ff-4770-dd30-a142555aabc5"
      },
      "source": [
        "interpolate(latent_size, sample1, sample2)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A man is about to get ready to go to work .\n",
            "A man is about to get ready to go to work .\n",
            "A man is having a conversation , while sitting on a bench .\n",
            "A man is having a conversation on a sunny day 's shoulder .\n",
            "A man is having fun in a classroom , is running on the\n",
            "A man is sitting on a bench , while others watch him .\n",
            "A man is sitting on a bench , while others watch him .\n",
            "A man is sitting on the couch , while others watch him .\n",
            "A man is sitting on the couch , while others watch him .\n",
            "A man is sitting on the couch , while others watch him .\n",
            "A man sits on the couch , while another man watches him .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXpN3zx9krjD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "# Training\n",
        "f = open('./snli_1.0/snli_1.0_train.txt')\n",
        "contents = f.read()\n",
        "file_as_list = contents.splitlines()\n",
        "sentences = []\n",
        "for example in file_as_list[1:]:\n",
        "    sentence1 = example.split('\\t')[5]\n",
        "    sentence2 = example.split('\\t')[6]\n",
        "    sent1_len = len(sentence1.split())\n",
        "    sent2_len = len(sentence2.split())\n",
        "    if sent1_len <= 12: sentences.append(sentence1)\n",
        "    if sent2_len <= 12: sentences.append(sentence2)\n",
        "\n",
        "sample = random.sample(sentences, 50000)\n",
        "\n",
        "f = open('./snli_1.0/sample_train.txt', \"w\")\n",
        "for sentence in sample:\n",
        "    f.write(sentence)\n",
        "    f.write(\"\\n\")\n",
        "f.close()\n",
        "\n",
        "# Development\n",
        "f = open('./snli_1.0/snli_1.0_dev.txt')\n",
        "contents = f.read()\n",
        "file_as_list = contents.splitlines()\n",
        "sentences = []\n",
        "for example in file_as_list[1:]:\n",
        "    sentence1 = example.split('\\t')[5]\n",
        "    sentence2 = example.split('\\t')[6]\n",
        "    sent1_len = len(sentence1.split())\n",
        "    sent2_len = len(sentence2.split())\n",
        "    if sent1_len <= 12: sentences.append(sentence1)\n",
        "    if sent2_len <= 12: sentences.append(sentence2)\n",
        "\n",
        "sample = random.sample(sentences, 5000)\n",
        "\n",
        "f = open('./snli_1.0/sample_dev.txt', \"w\")\n",
        "for sentence in sample:\n",
        "    f.write(sentence)\n",
        "    f.write(\"\\n\")\n",
        "f.close()\n",
        "\n",
        "# Test\n",
        "f = open('./snli_1.0/snli_1.0_test.txt')\n",
        "contents = f.read()\n",
        "file_as_list = contents.splitlines()\n",
        "sentences = []\n",
        "for example in file_as_list[1:]:\n",
        "    sentence1 = example.split('\\t')[5]\n",
        "    sentence2 = example.split('\\t')[6]\n",
        "    sent1_len = len(sentence1.split())\n",
        "    sent2_len = len(sentence2.split())\n",
        "    if sent1_len <= 12: sentences.append(sentence1)\n",
        "    if sent2_len <= 12: sentences.append(sentence2)\n",
        "\n",
        "sample = random.sample(sentences, 5000)\n",
        "\n",
        "f = open('./snli_1.0/sample_test.txt', \"w\")\n",
        "for sentence in sample:\n",
        "    f.write(sentence)\n",
        "    f.write(\"\\n\")\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX-c-c28_wBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_save_name = 'pretraining.pt'\n",
        "path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "torch.save({\n",
        "    'model':model.state_dict(), \n",
        "    'encoder':encoder.state_dict(), \n",
        "    'decoder':decoder.state_dict(), \n",
        "    'optimizer_e':optimizer_e.state_dict(), \n",
        "    'optimizer_d':optimizer_d.state_dict()\n",
        "    }, \n",
        "    path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZGPHtjYN4ba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_save_name = 'fb.pt'\n",
        "path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "torch.save({\n",
        "    'model':model.state_dict(), \n",
        "    'encoder':encoder.state_dict(), \n",
        "    'decoder':decoder.state_dict(), \n",
        "    'optimizer_e':optimizer_e.state_dict(), \n",
        "    'optimizer_d':optimizer_d.state_dict()\n",
        "    }, \n",
        "    path)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}