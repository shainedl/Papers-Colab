{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Variational_Principal_Components.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shainedl/Papers-Colab/blob/master/Variational_Principal_Components.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khdsw1Chol0V",
        "colab_type": "text"
      },
      "source": [
        "Based on *Variational Principal Components* (Bishop '99)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDsLw_WLUCcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import scipy.special as sp\n",
        "from scipy.stats import multivariate_normal \n",
        "from scipy.stats import gamma \n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCND_ufcSiGM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BayesianPCA():\n",
        "  \n",
        "  def __init__(self, a_alpha=10e-3, b_alpha=10e-3, a_tau=10e-3, b_tau=10e-3, beta=10e-3):\n",
        "    \n",
        "    # hyperparameters\n",
        "    self.a_alpha = a_alpha\n",
        "    self.b_alpha = b_alpha\n",
        "    self.a_tau = a_tau\n",
        "    self.b_tau = b_tau\n",
        "    self.beta = beta \n",
        "     \n",
        "  def __reestimate(self):\n",
        "    \"\"\"\n",
        "    Cycle through the groups of variables in turn to re-estimate each distribution \n",
        "    \"\"\"\n",
        "    \n",
        "    # observation parameter\n",
        "    self.tau = self.a_tau_tilde / self.b_tau_tilde\n",
        "\n",
        "    # latent variables\n",
        "    self.sigma_x = np.linalg.inv(np.identity(self.q) + self.tau *\n",
        "                   (np.trace(self.sigma_w) + np.dot(self.mean_w.T, self.mean_w)))\n",
        "    self.mean_x = self.tau * np.dot(np.dot(self.sigma_x, self.mean_w.T),(self.t_n - self.mean_mu))\n",
        "    \n",
        "    # observation parameter                                \n",
        "    self.sigma_mu = np.identity(self.d) / (self.beta + self.N * self.tau)\n",
        "    w_x = np.dot(self.mean_w, self.mean_x)\n",
        "    sum = 0\n",
        "    for n in range(self.N):\n",
        "      sum += np.subtract(self.t_n[:,n], w_x[:,n])\n",
        "    self.mean_mu = (self.tau * np.dot(self.sigma_mu, sum)).reshape(-1,1)\n",
        "    \n",
        "    # hyperparameter controlling the columns of W\n",
        "    self.alpha = self.a_alpha_tilde / self.b_alpha_tilde\n",
        "                                     \n",
        "    # weight                                 \n",
        "    self.sigma_w = np.linalg.inv(np.diag(self.alpha) + self.tau * \n",
        "                   (self.N * self.sigma_x + np.dot(self.mean_x, self.mean_x.T)))\n",
        "    self.mean_w = (self.tau * np.dot(self.mean_x, (np.subtract(self.t_n.T, self.mean_mu.T)))).T\n",
        "    \n",
        "    # alpha's gamma distribution parameter                            \n",
        "    self.b_alpha_tilde = self.b_alpha + 0.5 * (np.trace(self.sigma_w) + np.diag(np.square(self.mean_w)))                                                     \n",
        "    # tau's gamma distribution parameter     \n",
        "    self.b_tau_tilde = np.asarray([self.b_tau + 0.5 * np.sum(np.square(self.t_n))  + \\\n",
        "                       0.5 * self.N * (np.trace(self.sigma_mu) + np.dot(self.mean_mu.flatten(), self.mean_mu.flatten())) + \\\n",
        "                       0.5 * np.trace(np.dot(np.trace(self.sigma_w) + \\\n",
        "                       np.dot(self.mean_w.T, self.mean_w), self.N * self.sigma_x + \\\n",
        "                       np.dot(self.mean_x, self.mean_x.T))) + \\\n",
        "                       np.sum(np.dot(np.dot(self.mean_mu.T, self.mean_w), self.mean_x)) - \\\n",
        "                       np.sum(np.dot(np.dot(self.t_n.T, self.mean_w), self.mean_x)) - \\\n",
        "                       np.sum(np.dot(self.t_n.T, self.mean_mu))])       \n",
        "    \n",
        "  def __get_elbo(self):\n",
        "    \"\"\"\n",
        "    Computes the rigorous lower bound on the true log marginal likelihood \n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "      float\n",
        "        the lower bound = prior + likelihood - entropy \n",
        "    \n",
        "    \"\"\"                             \n",
        "    # random sample\n",
        "    x = np.asarray([np.random.multivariate_normal(self.mean_x[:,n], self.sigma_x) for n in range(self.N)]).T\n",
        "    mu = np.random.multivariate_normal(self.mean_mu.flatten(), self.sigma_mu)\n",
        "    w = np.asarray([np.random.multivariate_normal(self.mean_w[i], self.sigma_w) for i in range(self.d)])\n",
        "    alpha = np.random.gamma(self.a_alpha_tilde, 1 / self.b_alpha_tilde)     \n",
        "    tau = np.random.gamma(self.a_tau_tilde, 1 / self.b_tau_tilde)                             \n",
        "                \n",
        "      \n",
        "    # priors\n",
        "    # p(x) = N(x|0,I_q)\n",
        "    prior = np.sum(np.asarray([multivariate_normal.logpdf(x[:,i], np.zeros(self.q), np.identity(self.q)) for i in range(self.N)]))\n",
        "      \n",
        "    # p(w|alpha) = conditional distribution                   \n",
        "    prior += np.sum(np.asarray([(self.d / 2) * np.log(alpha[i] / (2 * np.pi)) - 0.5 * alpha[i] * np.sum(np.power(w[:,i],2)) for i in range(self.q)]))                \n",
        "                                 \n",
        "    # p(alpha) = Gamma(a, b)                             \n",
        "    prior += np.sum(gamma.logpdf(alpha, self.a_alpha, scale=1/self.b_alpha))                            \n",
        "                                 \n",
        "    # p(mu) = N(mu|0,Beta^-1I)       \n",
        "    prior += multivariate_normal.logpdf(mu, np.zeros(self.d), np.identity(self.d)/self.beta)\n",
        "                    \n",
        "    # p(tau) = Gamma(c, d)      \n",
        "    prior += np.sum(gamma.logpdf(tau, self.a_tau, scale=1/self.b_tau))\n",
        "        \n",
        "                    \n",
        "    # log likelihood of the conditional distribution \n",
        "    # p(t_n | x_n, W, mu, tau)\n",
        "    w_x = np.dot(w, x)\n",
        "    list_t = []\n",
        "    for n in range(self.N):\n",
        "      list_t.append(w_x[:,n] + mu)\n",
        "    likelihood = np.sum(np.asarray([multivariate_normal.logpdf(self.t_n[:,n], np.asarray(list_t).T[:,n], np.identity(self.d) / tau) for n in range(self.N)]))                 \n",
        "           \n",
        "      \n",
        "    # entropy\n",
        "    # q(x) \n",
        "    entropy = self.N * (0.5 * np.log(np.linalg.det(self.sigma_x) + (self.d / 2) * (1 + np.log(2 * np.pi))))   \n",
        "                       \n",
        "    # q(mu)\n",
        "    entropy += 0.5 * np.log(np.linalg.det(self.sigma_mu) + (self.d / 2) * (1 + np.log(2 * np.pi)))\n",
        "                            \n",
        "    # q(W)          \n",
        "    entropy += self.d * (0.5 * np.log(np.linalg.det(self.sigma_w) + (self.d / 2) * (1 + np.log(2 * np.pi))))  \n",
        "                         \n",
        "    # q(alpha)\n",
        "    entropy += self.q * (np.log(sp.gamma(self.a_alpha_tilde)) - (self.a_alpha_tilde - 1) \\\n",
        "                        * sp.digamma(self.a_alpha_tilde) + self.a_alpha_tilde)\n",
        "    for i in range(self.q):\n",
        "      entropy -= np.log(self.b_alpha_tilde[i])\n",
        "                         \n",
        "    # q(tau)   \n",
        "    entropy += - (self.a_tau_tilde - 1) * sp.digamma(self.a_tau_tilde) - \\\n",
        "               np.log(self.b_tau_tilde) + self.a_tau_tilde  \n",
        "    # will ignore np.log(sp.gamma(self.a_tau_tilde)) since = inf\n",
        "    \n",
        "    return prior + likelihood - entropy   \n",
        "                         \n",
        "  def fit(self, t_n, iterations = 5000, threshold = 1.0):\n",
        "    \"\"\"\n",
        "    Fits the data\n",
        "    \n",
        "    Parameters \n",
        "    ----------\n",
        "    t_n : d x N matrix\n",
        "      observed data to be fit\n",
        "      \n",
        "    iterations: int\n",
        "      number of iterations to re-estimate the lower bound\n",
        "    \n",
        "    threshold: float\n",
        "      determines convergence\n",
        "      \n",
        "    \"\"\"\n",
        "    self.t_n = t_n\n",
        "    self.d = self.t_n.shape[0]                     \n",
        "    self.q = self.d - 1\n",
        "    self.N = self.t_n.shape[1]   \n",
        "    \n",
        "    # variational parameters\n",
        "    self.mean_x = np.random.randn(self.q, self.N)\n",
        "    self.sigma_x = np.identity(self.q)\n",
        "    self.mean_mu = np.random.randn(self.d, 1)\n",
        "    self.sigma_mu = np.identity(self.d)\n",
        "    self.mean_w = np.random.randn(self.d, self.q)\n",
        "    self.sigma_w = np.identity(self.q)\n",
        "    self.a_alpha_tilde = self.a_alpha + self.d / 2\n",
        "    self.b_alpha_tilde = np.abs(np.random.randn(self.q))  \n",
        "    self.a_tau_tilde = self.a_tau + self.N * self.d / 2\n",
        "    self.b_tau_tilde = np.abs(np.random.randn(1))\n",
        "                     \n",
        "    self.elbos = [self.__get_elbo()]  \n",
        "    for i in range(iterations):\n",
        "      self.__reestimate()\n",
        "      self.elbos.append(self.__get_elbo())\n",
        "      if np.abs(self.elbos[-2] - self.elbos[-1]) <= threshold:\n",
        "        print('ELBO converged.')\n",
        "        print(\"Iterations: \", i+1)                   \n",
        "        print(\"ELBO: \", int(self.elbos[-1])) \n",
        "        break\n",
        "              \n",
        "      if (i+1) % 100 == 0:\n",
        "        print(\"Iterations: \", i+1)                   \n",
        "        print(\"ELBO: \", int(self.elbos[-1])) \n",
        "        \n",
        "        if i == iterations:\n",
        "          print('Ended without convergence.')\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNmSIgRA7U_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hinton(matrix, max_weight=None, ax=None):\n",
        "    \"\"\"\n",
        "    Draw Hinton diagram for visualizing a weight matrix.\n",
        "    From https://matplotlib.org/3.1.1/gallery/specialty_plots/hinton_demo.html\n",
        "    \n",
        "    \"\"\"\n",
        "    ax = ax if ax is not None else plt.gca()\n",
        "\n",
        "    if not max_weight:\n",
        "        max_weight = 2 ** np.ceil(np.log(np.abs(matrix).max()) / np.log(2))\n",
        "\n",
        "    ax.patch.set_facecolor('gray')\n",
        "    ax.set_aspect('equal', 'box')\n",
        "    ax.xaxis.set_major_locator(plt.NullLocator())\n",
        "    ax.yaxis.set_major_locator(plt.NullLocator())\n",
        "\n",
        "    for (x, y), w in np.ndenumerate(matrix):\n",
        "        color = 'white' if w > 0 else 'black'\n",
        "        size = np.sqrt(np.abs(w) / max_weight)\n",
        "        rect = plt.Rectangle([x - size / 2, y - size / 2], size, size,\n",
        "                             facecolor=color, edgecolor=color)\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "    ax.autoscale_view()\n",
        "    ax.invert_yaxis()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yH_htTIfjNV3",
        "colab_type": "code",
        "outputId": "bc4ee7d3-bfcb-414a-dfa8-0ba132657319",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\"\"\"\n",
        "We generate 100 data points in d = 10 dimensions from a Gaussian distribution \n",
        "having standard deviations of (5, 4, 3, 2) along four orthogonal directions \n",
        "and a standard deviation of 1 in the remaining five directions\n",
        "\"\"\"\n",
        "X = np.random.multivariate_normal(np.zeros(10), np.diag([5,4,3,2,1,1,1,1,1,1]), 100).T\n",
        "\n",
        "\"\"\"\n",
        "Hinton diagram of <W> from variational Bayesian PCA \n",
        "\"\"\"\n",
        "test = BayesianPCA()\n",
        "test.fit(X) \n",
        "hinton(test.mean_w)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iterations:  100\n",
            "ELBO:  -878\n",
            "Iterations:  200\n",
            "ELBO:  -1163\n",
            "Iterations:  300\n",
            "ELBO:  -738\n",
            "Iterations:  400\n",
            "ELBO:  -2012\n",
            "Iterations:  500\n",
            "ELBO:  -1208\n",
            "Iterations:  600\n",
            "ELBO:  -648\n",
            "Iterations:  700\n",
            "ELBO:  -942\n",
            "Iterations:  800\n",
            "ELBO:  -1037\n",
            "Iterations:  900\n",
            "ELBO:  -4823\n",
            "Iterations:  1000\n",
            "ELBO:  -1911\n",
            "Iterations:  1100\n",
            "ELBO:  -2305\n",
            "Iterations:  1200\n",
            "ELBO:  -734\n",
            "Iterations:  1300\n",
            "ELBO:  -1833\n",
            "Iterations:  1400\n",
            "ELBO:  -1418\n",
            "Iterations:  1500\n",
            "ELBO:  -1024\n",
            "Iterations:  1600\n",
            "ELBO:  -969\n",
            "Iterations:  1700\n",
            "ELBO:  -1300\n",
            "Iterations:  1800\n",
            "ELBO:  -754\n",
            "Iterations:  1900\n",
            "ELBO:  -4045\n",
            "Iterations:  2000\n",
            "ELBO:  -5765\n",
            "Iterations:  2100\n",
            "ELBO:  -1720\n",
            "Iterations:  2200\n",
            "ELBO:  -1398\n",
            "Iterations:  2300\n",
            "ELBO:  -892\n",
            "Iterations:  2400\n",
            "ELBO:  -886\n",
            "Iterations:  2500\n",
            "ELBO:  -924\n",
            "Iterations:  2600\n",
            "ELBO:  -25666\n",
            "Iterations:  2700\n",
            "ELBO:  -1331\n",
            "Iterations:  2800\n",
            "ELBO:  -699\n",
            "Iterations:  2900\n",
            "ELBO:  -1410\n",
            "Iterations:  3000\n",
            "ELBO:  -1738\n",
            "Iterations:  3100\n",
            "ELBO:  -1309\n",
            "ELBO converged.\n",
            "Iterations:  3199\n",
            "ELBO:  -1037\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQwAAADuCAYAAADMdzmuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1lNtGlttAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o42ZgAIKqDLo",
        "colab_type": "text"
      },
      "source": [
        "Other helpful links:\n",
        "\n",
        "https://en.wikipedia.org/wiki/Variance\n",
        " - For expection of $X^{T}X$ or $||X||^2$\n",
        " \n",
        "https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf\n",
        "\n",
        "https://www.programcreek.com/python/example/57185/numpy.random.multivariate_normal\n",
        "- For generating random samples"
      ]
    }
  ]
}