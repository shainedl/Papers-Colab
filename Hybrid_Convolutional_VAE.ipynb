{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hybrid_Convolutional_VAE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shainedl/Papers-Colab/blob/master/Hybrid_Convolutional_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRm2MachEdba",
        "colab_type": "text"
      },
      "source": [
        "Based on *A Hybrid Convolutional Variational Autoencoder for Text Generation* (Semeniuta et al, Univweaitat zu Lubeck)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pja0xCLkENyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from google.colab import files\n",
        "from collections import defaultdict\n",
        "from itertools import count\n",
        "import numpy as np\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVrASc8MGSBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VAE(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, padding_idx, z_dim, rnn_units):\n",
        "    \"\"\"\n",
        "    Helpful Links\n",
        "    -------------\n",
        "    To define the layers of the CNN: https://www.analyticsvidhya.com/blog/2019/10/building-image-classification-models-cnn-pytorch/\n",
        "    Deconvolution layers: https://datascience.stackexchange.com/questions/6107/what-are-deconvolutional-layers\n",
        "    Batch normalization: https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c\n",
        "    \"\"\"\n",
        "    super(VAE, self).__init__()\n",
        "    \n",
        "    self.embeddings = nn.Embedding(vocab_size, embedding_dim,\n",
        "                                   padding_idx=padding_idx)\n",
        "    \n",
        "    self.cnn_layers1 = nn.Sequential(\n",
        "        # Defining 1st 1D convolution layer\n",
        "        nn.Conv1d(embedding_dim, 128, kernel_size=3, stride=2),\n",
        "        nn.BatchNorm1d(128),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        # Defining 2nd 1D convolution layer\n",
        "        nn.Conv1d(128, 256, kernel_size=3, stride=2),\n",
        "        nn.BatchNorm1d(256),\n",
        "        nn.ReLU(),  \n",
        "        \n",
        "        # Defining 3rd 1D convolution layer\n",
        "        nn.Conv1d(256, 512, kernel_size=3, stride=2),\n",
        "        nn.BatchNorm1d(512),\n",
        "        nn.ReLU(),    \n",
        "\n",
        "        # Defining 4th 1D convolution layer\n",
        "        nn.Conv1d(512, 512, kernel_size=3, stride=2),\n",
        "        nn.BatchNorm1d(512),\n",
        "        nn.ReLU(),    \n",
        "\n",
        "        # Defining 5th 1D convolution layer\n",
        "        nn.Conv1d(512, 512, kernel_size=3, stride=2),\n",
        "        nn.BatchNorm1d(512),\n",
        "        nn.ReLU(),           \n",
        "\n",
        "        nn.Flatten()  \n",
        "    )\n",
        "\n",
        "    self.cnn_layers2 = nn.Sequential(\n",
        "        # Defining 1st 1D deconvolution layer\n",
        "        nn.ConvTranspose1d(z_dim, 512, kernel_size=3, stride=2),\n",
        "        nn.BatchNorm1d(512),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        # Defining 2nd 1D deconvolution layer\n",
        "        nn.ConvTranspose1d(512, 512, kernel_size=3, stride=2),\n",
        "        nn.BatchNorm1d(512),\n",
        "        nn.ReLU(),    \n",
        "\n",
        "        # Defining 3rd 1D deconvolution layer\n",
        "        nn.ConvTranspose1d(512, 256, kernel_size=3, stride=2),\n",
        "        nn.BatchNorm1d(256),\n",
        "        nn.ReLU(), \n",
        "\n",
        "        # Defining 4th 1D deconvolution layer        \n",
        "        nn.ConvTranspose1d(256, 128, kernel_size=3, stride=2),\n",
        "        nn.BatchNorm1d(128),\n",
        "        nn.ReLU(),   \n",
        "\n",
        "        # Defining 5th 1D deconvolution layer\n",
        "        nn.ConvTranspose1d(128, embedding_dim, kernel_size=3, stride=2),\n",
        "        nn.BatchNorm1d(embedding_dim),\n",
        "        nn.ReLU()\n",
        "\n",
        "    )\n",
        "\n",
        "    self.fc11 = nn.Linear(512 * 3, z_dim)\n",
        "    self.fc12 = nn.Linear(512 * 3, z_dim)\n",
        "\n",
        "    self.rnn = nn.LSTM(input_size=embedding_dim,\n",
        "                       hidden_size=rnn_units,\n",
        "                       batch_first=True)\n",
        "    \n",
        "    self.fc2 = nn.Linear(rnn_units, vocab_size)\n",
        "  \n",
        "  def encode(self, x):\n",
        "    cnn_e = self.cnn_layers1(x)\n",
        "    mu = self.fc11(cnn_e) \n",
        "    logvar = self.fc12(cnn_e) \n",
        "\n",
        "    return mu, logvar\n",
        "  \n",
        "  def decode(self, z, inputs):\n",
        "    cnn_d = self.cnn_layers2(z)\n",
        "    rnn_input = torch.cat((cnn_d, inputs), dim=2)\n",
        "    rnn_input = nn.utils.rnn.pack_padded_sequence(rnn_input, X_lengths, batch_first=True)\n",
        "    rnn_output, final_state = self.rnn(rnn_input)\n",
        "    rnn_output, _ = nn.utils.rnn.pad_packed_sequence(rnn_output, batch_first=True)\n",
        "\n",
        "    fc_output = self.fc2(rnn_output)\n",
        "    decoder_output = nn.functional.log_softmax(fc_output, dim = -1)\n",
        "    return decoder_output\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Forward pass of the model \n",
        "    \"\"\"\n",
        "    inputs = self.embeddings(x)\n",
        "    inputs = torch.transpose(inputs, 1, 2)\n",
        "    \n",
        "    mu, logvar = self.encode(inputs)\n",
        "\n",
        "    z = self.__reparameterize(mu, logvar)\n",
        "    z = z.unsqueeze(2)\n",
        "  \n",
        "    return self.decode(z, inputs), mu, logvar    \n",
        "  \n",
        "  def __reparameterize(self, mu, logvar):\n",
        "    std = torch.exp(logvar / 2)\n",
        "    eps = torch.randn_like(std)\n",
        "    \n",
        "    return mu + std * eps    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prAO5FjM2GKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(txt):\n",
        "  w2i = defaultdict(lambda x=count(0): next(x))\n",
        "  data = []\n",
        "  tweets = txt.splitlines()\n",
        "  tknzr = TweetTokenizer()\n",
        "\n",
        "  for tweet in tweets:\n",
        "    tweet = tknzr.tokenize(tweet)\n",
        "    tokens = [twitter_word_classes(tok) for tok in tweet]\n",
        "    characters = []\n",
        "    for i,tok in enumerate(tokens):\n",
        "      if tok not in ('@userid', 'url'):\n",
        "        for char in tok:\n",
        "          w2i[char]\n",
        "          characters.append(char)\n",
        "      else:\n",
        "        characters.append(tok)\n",
        "      if i != len(tokens) - 1:\n",
        "        characters.append(\" \")\n",
        "    data.append(characters)\n",
        "  w2i[\" \"]\n",
        "  w2i[\"@userid\"] \n",
        "  w2i[\"url\"] \n",
        "  w2i[\"_UNK_\"]  \n",
        "  w2i[\"_PAD_\"] \n",
        "  padding_idx = w2i[\"_PAD_\"]\n",
        "  w2i[\"_START_\"] \n",
        "  w2i[\"_STOP_\"] \n",
        "  w2i = dict(w2i)\n",
        "  i2w = {i:w for w,i in w2i.items()}\n",
        "\n",
        "  return data, w2i, i2w, padding_idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcUXuLLX68v2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def twitter_word_classes(token):\n",
        "  \"\"\" \n",
        "  Converts Twitter specific classes\n",
        "  \"\"\"\n",
        "  wc = ''\n",
        "  if token[0] == '@' and len(token) > 1:\n",
        "      wc = '@userid'\n",
        "  elif token[0:7] == 'http://':\n",
        "      wc = 'url'\n",
        "  else:\n",
        "      wc = token\n",
        "\n",
        "  return wc\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7p0V7L2t22LU",
        "colab_type": "code",
        "outputId": "134c87c1-1f61-4b77-b9ff-24008268e411",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fbd3b357-c6c5-4709-bc9b-1036c842fc2e\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-fbd3b357-c6c5-4709-bc9b-1036c842fc2e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving hybrid_cvae.txt to hybrid_cvae.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZY_op4J3g-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data, w2i, i2w, padding_idx = load_data(uploaded['hybrid_cvae.txt'])\n",
        "vocab_size = len(w2i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FffQ16g0QyRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 215\n",
        "z_dim = 512\n",
        "rnn_units = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wanp7ANDE8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epoch):\n",
        "  \n",
        "  model.train()\n",
        "\n",
        "  y, mu, logvar = model(batch_input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5h0A-OkSSHl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = VAE(vocab_size, embedding_dim, padding_idx, z_dim, rnn_units)\n",
        "train(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIuMgKf_v6q0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_sequence(seq, to_ix):\n",
        "  idxs = [to_ix[w] for w in seq]\n",
        "  return torch.tensor(idxs, dtype=torch.long)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vmsYt5oxWm3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Helpful Links\n",
        "-------------\n",
        "Batching: https://cs230-stanford.github.io/pytorch-nlp.html\n",
        "Pack sequence: https://towardsdatascience.com/taming-lstms-variable-sized-mini-batches-and-why-pytorch-is-good-for-your-health-61d35642972e\n",
        "\"\"\"\n",
        "batch_size = 128\n",
        "\n",
        "batch_input = []\n",
        "for i in range(batch_size):\n",
        "  batch_sentences.append(prepare_sequence(data[i], w2i))\n",
        "\n",
        "# compute length of longest sentence in batch\n",
        "batch_max_len = max([len(s) for s in batch_sentences])\n",
        "\n",
        "batch_input = w2i[\"_PAD_\"]*np.ones((len(batch_sentences), batch_max_len+1))\n",
        "batch_target = w2i[\"_PAD_\"]*np.ones((len(batch_sentences), batch_max_len+1))\n",
        "\n",
        "X_lengths = []\n",
        "\n",
        "# copy the data to the numpy array\n",
        "for j in range(len(batch_sentences)):\n",
        "  cur_len = len(batch_sentences[j]) + 1\n",
        "  X_lengths.append(cur_len)\n",
        "  batch_input[j][0] = w2i[\"_START_\"]\n",
        "  batch_input[j][1:cur_len] = batch_sentences[j]\n",
        "  batch_target[j][:cur_len-1] = batch_sentences[j]\n",
        "  batch_target[j][cur_len-1] = w2i[\"_STOP_\"]\n",
        "\n",
        "X_lengths.sort(reverse=True)\n",
        "\n",
        "# since all data are indices, we convert them to torch LongTensors\n",
        "batch_input = torch.LongTensor(batch_input)\n",
        "batch_target = torch.LongTensor(batch_target)\n",
        "\n",
        "# convert Tensors to Variables\n",
        "batch_input = Variable(batch_input)\n",
        "batch_target = Variable(batch_target)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}